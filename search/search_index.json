{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Solution Cookbook for IBM Multicloud Manager This cookbook will provide explicit, prescriptive guidance for every aspect of managing Kubernetes clusters in multicloud environment with IBM Multicloud Manager. Unless otherwise noted in specific sections, this cookbook is written in support of IBM Multicloud Manager version 3.1.2 . As additional versions are released, this cookbook will be updated to support additional versions as possible. Chapter 1 - Scenarios for Kubernetes multi-cluster management When will you consider multi-cluster? What should be considered when managing multiple Kubernetes clusters? Go Chapter 2 - Quick Start Quick start guide to get the product installed. Deploy a sample application to validate the installation. Go Chapter 3 - Create and Deploy Applications with MCM Understand the application concept in multi-cluster environment. Walk through the application creation and deployment process. Go Chapter 4 - DevOps in Multi-cluster Environment Integration with CI/CD tools. Application rollout to multiple clusters. Go Chapter 5 - Create Multi-cluster Compliance Policies What policy/compliance client cares: HIPAA, Resource usage. How to best apply compliance. Go Chapter 6 - Monitoring and Event Management with MCM Monitoring and event management features delivered with MCM. Go Chapter 7 - Manage Red Hat OpenShift Clusters How to manage an IBM Cloud Private on OpenShift cluster with MCM. Go Chapter 8 - Manage AWS EKS Clusters How to manage a hosted Kubernetes cluster on AWS EKS. Go","title":"Home"},{"location":"#solution-cookbook-for-ibm-multicloud-manager","text":"This cookbook will provide explicit, prescriptive guidance for every aspect of managing Kubernetes clusters in multicloud environment with IBM Multicloud Manager. Unless otherwise noted in specific sections, this cookbook is written in support of IBM Multicloud Manager version 3.1.2 . As additional versions are released, this cookbook will be updated to support additional versions as possible. Chapter 1 - Scenarios for Kubernetes multi-cluster management When will you consider multi-cluster? What should be considered when managing multiple Kubernetes clusters? Go Chapter 2 - Quick Start Quick start guide to get the product installed. Deploy a sample application to validate the installation. Go Chapter 3 - Create and Deploy Applications with MCM Understand the application concept in multi-cluster environment. Walk through the application creation and deployment process. Go Chapter 4 - DevOps in Multi-cluster Environment Integration with CI/CD tools. Application rollout to multiple clusters. Go Chapter 5 - Create Multi-cluster Compliance Policies What policy/compliance client cares: HIPAA, Resource usage. How to best apply compliance. Go Chapter 6 - Monitoring and Event Management with MCM Monitoring and event management features delivered with MCM. Go Chapter 7 - Manage Red Hat OpenShift Clusters How to manage an IBM Cloud Private on OpenShift cluster with MCM. Go Chapter 8 - Manage AWS EKS Clusters How to manage a hosted Kubernetes cluster on AWS EKS. Go","title":"Solution Cookbook for IBM Multicloud Manager"},{"location":"mcm-applications/","text":"Create and Deploy Applications with MCM Author: Fabio Gomez (fabiogomez@us.ibm.com) In this document, we will go over the process of creating and deploying an MCM Application on multiple clusters using the Helm CLI against the MCM Hub Cluster. Pre-Requisites 2 x IBM Cloud Private Clusters with MCM Installed. I recommend you go through the Installing MCM section of the Quick Start chapter. Kubectl (Kubernetes CLI) Follow the instructions here to install it on your platform. ICP Helm (Kubernetes package manager) Follow the instructions here to install it on your platform. IBM Cloud Private CLI Follow the instructions here to install it on your platform. Creating Multicloud Manager Applications As explained in the Core Multicloud Manager Resources to Learn section of the Quick Start chapter, an Application is a way to relate multiple workloads (Helm Charts) as a single unit. By implementing the custom MCM YAML resources, MCM can monitor your applications, automate their deployment to multiple clusters, and extract the topology of all its components, amongst other features. Though not immediately useful for developers, these capabilities makes the lives of operators easier by having a single place where they can deploy and monitor applications on multiple clusters. With the MCM Hub Cluster dashboard you can deploy a regular Helm Chart to multiple IBM Cloud Private (ICP) clusters, but that's the only benefit you get out of the box. To fully leverage MCM's deployment automation, monitoring, topology and its other capabilities, you will have to create custom MCM YAML resources that get deployed with your charts. Luckily, no changes need to be made to the original Helm Charts or their application code. Guestbook Helm Charts Introduced In this document, we are going to cover the basic Application concepts we learned in Core Multicloud Manager Resources to Learn , but we will be using the Guestbook MCM Application code as an example to deploy 3 separate charts (explained later) on multiple clusters. The actual Guestbook application code comes from the Kubernetes Community, and it's located publicly on the kubernetes/examples repository. The application is broken up in the following 3 services: Redis Master : A Redis Master instance. Redis Slave : A Redis Slave instance, which replicates the contents of the Redis Master. Frontend : An Angular.js application that uses PHP script to communicate with Redis Master and Slave. In the application, you can submit Guestbook names, which get persisted in Redos. It sends new names to the Redis Master, which then get replicated to the Redis Slave. It then retrieves the names from the Redis Slave and prints them on the screen. The above 3 services were converted into the following 3 Helm Charts for the purpose of this demo. You can find the source code for the Helm Charts here: Redis Master : gbrm . Redis Slave : gbrs . Frontend : gbf . Lastly, in order to deploy the above charts through MCM, we are also going to need a Helm chart that deploys all of the MCM resources. Those resources do the actual deployment, monitoring, and topology setup for the 3 Helm Charts mentioned above. In the next sections, you are going to explore parts of the source code that make the gbapp Helm Chart (the chart with the MCM Resources) to understand the process of creating an Multicloud Manager Application . NOTE : To checkout the entire source code of the gbapp Helm Chart, checkout the link below: demos/guestbook/gbapp Application Resource First things first, in order for MCM to recognize your workloads as an MCM application, you have to create an Application resource. The Application resource lets you specify criteria to associate Kubernetes resources to your application so that MCM can manage and monitor them. Let's look at the gbapp Application resource from the Helm Chart below: apiVersion: app.k8s.io/v1beta1 kind: Application metadata: name: {{ template guestbookapplication.fullname . }} labels: app: {{ template guestbookapplication.name . }} chart: {{ .Chart.Name }}-{{ .Chart.Version | replace + _ }} release: {{ .Release.Name }} heritage: {{ .Release.Service }} name: {{ template guestbookapplication.fullname . }} spec: selector: matchExpressions: - key: app operator: In values: - gbapp - gbf - gbrm - gbrs componentKinds: - group: core kind: Pods Where: apiVersion : Contains the Community SIG API version for Applications. kind : Specifies that this is an Application resource type. metadata : Contains the resource name and some labels. spec.selector.matchExpressions : Contains the selectors that MCM will use to associate other Kubernetes resources to the application. In this case, we are associated all resources with app label that contains any of the values listed in spec.selector.matchExpressions[0].values , which in this case are the names of the charts themselves. If you go to the source code of any of the Guestbook Helm Charts and explore its resources (deployments, services, etc) you will notice that they contain an app label that contains the name of the Helm chart. spec.selector.componentKinds : Contains a list of the kinds of resources to be associated with the application. In this case we are only associating components of kind Pods . However, you can associate resources of type Deployment , StatefulSet , Service , etc. You can assign whatever name you like to the group field. Later on, when we deploy the gbapp Helm Chart, you will see for yourself how the Guestbook resources are associated with the Application . Deployable Resource Now that we know how to create an Application that knows how to associate resources to itself, it's time to learn how an Application can deploy those resources for itself. The way to do that is by creating a Deployable object, where you can specify what Helm chart to deploy and what namespace to deploy the chart into once the PlacementPolicy (explained in later sections) decides what clusters to deploy the Helm Charts into. Let's look at the gbapp-frontend Deployable resource from the Helm Chart below: apiVersion: mcm.ibm.com/v1alpha1 kind: Deployable metadata: name: {{ template guestbookapplication.fullname . }}-frontend labels: app: {{ template guestbookapplication.name . }} chart: {{ .Chart.Name }}-{{ .Chart.Version | replace + _ }} release: {{ .Release.Name }} heritage: {{ .Release.Service }} name: {{ template guestbookapplication.fullname . }}-frontend servicekind: ApplicationService spec: deployer: kind: helm helm: chartURL: {{ .Values.chartRepoURL }}/gbf-0.1.0.tgz namespace: {{ .Values.appInClusterNamespace }} Where: apiVersion : Contains the MCM API version for Deployables. kind : Specifies that this is an Deployable resource type. metadata : Contains the resource name and some labels. spec.deployer.kind : Specifies that is is a deployer that will deploy a Helm chart. spec.deployer.helm.chartURL : Specifies the URL with the Helm Chart location. In this case the Chart location is on this document's GitHub Repository. spec.deployer.helm.namespace : Specifies the cluster namespace to deploy the Helm chart to. Relationship Resource The more Deployable objects you create for an Application , the harder it becomes to determine your application's topology, specially for newcomers in your team. MCM provides application topology features via Weave Scope out of the box. However, to explicitly specify dependencies between certain components (Deployment, Application, etc), you will need to create an ApplicationRelationship resource. Let's look at the gbapp-frontend-app ApplicationRelationship resource from the Helm Chart below: apiVersion: mcm.ibm.com/v1alpha1 kind: ApplicationRelationship metadata: name: {{ template guestbookapplication.fullname . }}-app-frontend labels: app: {{ template guestbookapplication.name . }} chart: {{ .Chart.Name }}-{{ .Chart.Version | replace + _ }} release: {{ .Release.Name }} heritage: {{ .Release.Service }} spec: type: contains source: kind: Application name: {{ template guestbookapplication.fullname . }} destination: kind: Deployable name: {{ template guestbookapplication.fullname . }}-frontend Where: apiVersion : Contains the MCM API version for ApplicationRelationship. kind : Specifies that this is an ApplicationRelationship resource type. metadata : Contains the resource name and some labels. spec.source.kind : specifies the type of Resource for the source object, which is Application in this case. spec.source.name : specifies the name of the source object, which is the guestbook-gbapp Application release and chart name combined. spec.destination.kind : specifies the type of Resource for the destination object, which is Deployable in this case. spec.destination.name : specifies the name of the destination object, which is the name for the gbabb-frontend Deployable. Placement Policy Resource Normally, to deploy an application to multiple clusters, you to run the following steps on each cluster: Login to get the kubernetes context. Run the helm install command. This does not seem like a lot of work for 2 or 3 clusters. But the more clusters you have to work with, the more credentials you have to remember and manage. Also, if you are using CI/CD pipelines to deploy your workloads, the more clusters you have, the more pipelines you have to write and embed those credentials to, which can lead to human errors. If those credentials were to change, you would have to update those credentials wherever they are being used, which can get hard to track. By leveraging MCM, access to your clusters is managed from a central location. MCM expands on this capability with Placement Policies by allowing you to deploy workloads through it by specifying cluster labels, which are much easier to remember and manage. By adopting MCM into your CI/CD pipelines, you only have to remember the credentials for the MCM Hub cluster. To deploy workloads to multiple clusters, you only have to provide the number of application replicas and the cluster labels that match the clusters that you would like to deploy them to. Let's look at the gbapp-frontend PlacementPolicy resource from the Helm Chart below: apiVersion: mcm.ibm.com/v1alpha1 kind: PlacementPolicy metadata: name: {{ template guestbookapplication.fullname . }}-frontend labels: app: {{ template guestbookapplication.name . }} chart: {{ .Chart.Name }}-{{ .Chart.Version | replace + _ }} release: {{ .Release.Name }} heritage: {{ .Release.Service }} name: {{ template guestbookapplication.fullname . }}-frontend servicekind: CacheService spec: clusterReplicas: {{ .Values.replicaCount }} clusterLabels: matchLabels: {{ toYaml .Values.targetCluster.labelSelector.matchLabels | indent 6 }} resourceHint: {{ toYaml .Values.targetCluster.resourceSelector | indent 4 }} compliances: {{ .Values.targetCluster.compliances }} Where: apiVersion : Contains the MCM API version for PlacementPolicy. kind : Specifies that this is an PlacementPolicy resource type. metadata : Contains the resource name and some labels. spec.clusterReplicas : contains the number of replicas (application instances) to deploy. spec.clusterLabels.matchLabels : Contains the list of labels and values that clusters must have for MCM to deploy applications to them. spec.resourceHint : If more than one cluster matches the labels and values provided above, you can specify to MCM a resource specific criteria to select the clusters. For example, you can ask MCM to select the clsuter with the most available CPU cores. spec.compliances : Contains a list of Compliances (outside of the scope of this chapter) resources to attach this policy to. To learn about Compliance resources, checkout the Create Multi-cluster Compliance Policies Chapter . Placement Binding Resource Just like how a Kubernetes Role requires RoleBinding to attach it to a specific ServiceAccount , a PlacementPolicy requires another resource that binds its rules to specific resources. This MCM resources is the PlacementBinding , which can bind the PlacementPolicy rules to specific Deployables. Let's look at the gbapp-frontend PlacementPolicy resource from the Helm Chart below: apiVersion: mcm.ibm.com/v1alpha1 kind: PlacementBinding metadata: name: {{ template guestbookapplication.fullname . }}-frontend labels: app: {{ template guestbookapplication.name . }} chart: {{ .Chart.Name }}-{{ .Chart.Version | replace + _ }} release: {{ .Release.Name }} heritage: {{ .Release.Service }} name: {{ template guestbookapplication.fullname . }}-frontend servicekind: CacheService placementRef: apiGroup: mcm.ibm.com kind: PlacementPolicy name: {{ template guestbookapplication.fullname . }}-frontend subjects: - apiGroup: mcm.ibm.com kind: Deployable name: {{ template guestbookapplication.fullname . }}-frontend Where: apiVersion : Contains the MCM API version for PlacementBinding. kind : Specifies that this is an PlacementBinding resource type. metadata : Contains the resource name and some labels. placementRef.name : Specifies the PlacementPolicy name to bind. subjects[0].name : Specifies the Deployable name to bind the PlacementPolicy to. MCM Application Helm Chart The above YAML resources mostly cover supporting the frontend Helm Chart. To see how we put together an MCM Application Helm Chart that deploys all of the 3 Guestbook Helm Charts, checkout the source code at the link below: demos/guestbook/gbapp Deployment MCM Application Now that you are aware of how to create MCM Applications, it is time to deploy the gbapp MCM Application Helm Chart, which deploys the Guestbook Frontend ( gbf ), Redis Master ( gbrm ), and Redis Slave ( gbrs ) charts. Before you start with the examples, you will need to clone the code and log into the MCM Hub Cluster with the commands below: # Clone the Repository git clone https://github.com/ibm-cloud-architecture/kubernetes-multicloud-management.git Also, open a new browser tab and go to the Applications page on the MCM Dashboard, as shown below: You should then be greeted by an empty Applications page, which we will be populating later: Now you are ready to start deploying the MCM Application! 0. Create Image Policies on Both Clusters Since ICP version 3.1, you are required to create Image Policies that allow you to pull Docker images from specific Docker registries ( gcr.io in our case). To do so, let's run the following commands on EACH ICP CLUSTER : # Login to the ICP Cluster cloudctl login -a https://ICP_MASTER_IP:8443 -n default --skip-ssl-validation # Go to Helm Charts directory cd demos/guestbook # Create the Image Policy in the ICP Cluster kubectl apply -f demos/guestbook/guestbook-cluster-image-policy.yaml Don't forget to run the above commands on EACH ICP Cluster so that there are no issues when deploying the gbapp application. 1. Deploy to Dev Cluster The process of deploying an MCM Application is the same as deploying a Helm Chart. To deploy the gbapp MCM Application Helm chart, run the commands below: # Log into MCM HUB Cluster cloudctl login -a https://HUB_CLUSTER_MASTER_IP:8443 -n default --skip-ssl-validation # If not already there, go to the Helm Charts directory cd demos/guestbook # Deploy MCM Application Helm Chart helm upgrade --install guestbook --set replicaCount=1 --set targetCluster.labelSelector.matchLabels.environment=Dev ./gbapp --tls Where: replicaCount : is a field that indicates the number of application instances we are deploying, which is 1. targetCluster.labelSelector.matchLabels.environment : means we are using the environment field with a value of Dev , which tells the MCM Controller to should find a cluster that has that value for that field. Checkout the demos/guestbook/gbapp/values.yaml to learn the different fields we can use as cluster selectors. Assuming that everything went well, you should now see the guestbook-gbapp MCM Application show up in the Applications page as shown below: The above is all it takes to deploy an MCM Application. To deploy to multiple clusters (done in later sections) all you have to do is change the fields, values, and number of replicas in the helm upgrade command and MCM takes care of the rest. Before exploring the multiple cluster scenario, let's first get familiar with examining MCM Application details on the Applications page. 2. Exploring the Application Details Page If you click on the guestbook-gbapp entry, you will be taken to that application's overview page, which contains multiple sections that show you the custom MCM Resources that were deployed. The first section you will encounter is the Application Details , which contains the high level MCM Application details: If you scroll down, you see a section for the Application Deployments , which show the deployment status foe each of the 3 Helm Charts. If there was an error on the charts deployment, you will see the deployment status reflected under the Status column and the failure details under the Reason column. If you scroll down further, you will encounter the Placement Policies section, which contains the placement policies used to deploy the Helm charts into the se-stg-31 cluster. If you look under the Decisions column, you will notice that, after finding a cluster that matches the labels we provided earlier, it decided to use the se-dev-312-ubuntu cluster (for you it's going to be se-dev-31 or whatever name you assigned to the cluster). Down below is the Placement Bindings section, which should show the Placement Policies and their respected Subjects (Deployables) to be bound to. After that you will find the Deployable Objects section, which contain a list of all of the Helm Charts, along with the Chart URL and the namespace to deploy them into. Next you will find the Application Relationships section, which contains a list of all the relationships between the Application and the Deployable objects. Last but not least, if you click on the Diagram tab, you will see a visual representation of the Application and all of the resources we inspected above, including the Helm Charts resources (Pods, Deployments, and Services). This diagram is available thanks to the Weave Scope component in MCM, which takes the Application Relationship objects and its own algorithm to deduce and reduce the application topology on the screen. If you click on any of the components you will open a small view with either a short description or the YAML resource file that was used to create the resource. Now that you have learned how to navigate the Application Details page, let's proceed with redeploying the application into another cluster, followed by multiple clusters. 3. Redeploy to Staging Cluster To redeploy the application to the se-stg-31 cluster, all we have to do is change the value of the environment field to Staging , and MCM will take care of uninstalling the application from the se-dev-31 cluster and then redeploying it to the se-stg-31 cluster. To do so, run the command below: # If not already there, go to the Helm Charts directory cd demos/guestbook # Deploy MCM Application Helm Chart helm upgrade --install guestbook --set replicaCount=1 --set targetCluster.labelSelector.matchLabels.environment=Staging ./gbapp --tls That's all it takes. With the same cloudctl session, we were able to move the application from one cluster to another by simply changing the value of a label. This is very powerful for operations folks or anyone responsible for deploying an application to multiple environments as it is very user intuitive while leveraging the capabilities of the MCM platform. I encourage you to refresh the Applications page and explore how the resources mentioned earlier changed. The only change was the cluster name that the resources were deployed into. 4. Redeploy to Both Clusters Now let's go ahead and deploy the application to both clusters simultaneously! To do so, we will have to use a different label of which value is shared by both clusters. Both se-dev-31 and se-stg-31 clusters actually share the value of case for the label owner , so we will use that for the deployment. The last thing that remains is to increase the value of replicaCount from 1 to 2 in order for MCM to know that it needs to install the application on two clusters that match label values. To install the application on both clusters, run the command below: # If not already there, go to the Helm Charts directory cd demos/guestbook # Deploy MCM Application Helm Chart helm upgrade --install guestbook --set replicaCount=2 --set targetCluster.labelSelector.matchLabels.owner=case ./gbapp --tls That's really it. Once again, by using the same cloudctl session to the HUB Cluster, we were able to redeploy the application to both clusters by simply using a label with a value that both clusters share and increasing the replicaCount value to 2 . Imagine how easy it would be to write a CI/CD pipeline to promote an application from one environment to another and run tests! NOTE : More on CI/CD on the DevOps with MCM Tutorial Chapter . Anyways, feel free to refresh and checkout the MCM Application Details page again to see what changed. Most resources look the same with the main difference being the Application Deployments section below, which contains details of all the Helm deployments across both clusters instead of just one. Another thing that changed significantly is the Diagram tab below, which shows a topology diagram that includes the resources of both clusters. 5. Health View Dashboard When deploying an MCM Application, MCM creates a dashboard in Grafana that lets you visualize CPU, memory, filesystem, and bandwidth usage. The data is shows is pulled from the Federated Prometheus instance that gets deployed with the MCM Controller chart. To access the Health View, first go back to the Applications page as shown below: Now click the Launch Health View link under the Dashboard column and it should open a Grafana dashboard in a new page that looks similar to the following: Feel free to scroll around to see the resource consumption for each of the Helm Chart deployments. 5. Cleanup To uninstall the MCM Application from both clusters, run the command below: helm delete guestbook --purge --tls Conclusion We hope this chapter opened your mind and made you realize how easy it is to use MCM to deploy an application to multiple clusters from a central location and with a simple cluster selection criteria. Usually, when companies company grows, the number of application environments grow as well and the complexity of deploying and managing applications on those environments grows as well. Now that you know how to use MCM to deploy, manage, and monitor your applications on multiple clusters, you should checkout the DevOps with MCM Tutorial Chapter to learn how to use MCM in an automated CI/CD pipeline to promote applications between environments and run tests.","title":"Create and Deploy Applications with MCM"},{"location":"mcm-applications/#create-and-deploy-applications-with-mcm","text":"Author: Fabio Gomez (fabiogomez@us.ibm.com) In this document, we will go over the process of creating and deploying an MCM Application on multiple clusters using the Helm CLI against the MCM Hub Cluster.","title":"Create and Deploy Applications with MCM"},{"location":"mcm-applications/#pre-requisites","text":"2 x IBM Cloud Private Clusters with MCM Installed. I recommend you go through the Installing MCM section of the Quick Start chapter. Kubectl (Kubernetes CLI) Follow the instructions here to install it on your platform. ICP Helm (Kubernetes package manager) Follow the instructions here to install it on your platform. IBM Cloud Private CLI Follow the instructions here to install it on your platform.","title":"Pre-Requisites"},{"location":"mcm-applications/#creating-multicloud-manager-applications","text":"As explained in the Core Multicloud Manager Resources to Learn section of the Quick Start chapter, an Application is a way to relate multiple workloads (Helm Charts) as a single unit. By implementing the custom MCM YAML resources, MCM can monitor your applications, automate their deployment to multiple clusters, and extract the topology of all its components, amongst other features. Though not immediately useful for developers, these capabilities makes the lives of operators easier by having a single place where they can deploy and monitor applications on multiple clusters. With the MCM Hub Cluster dashboard you can deploy a regular Helm Chart to multiple IBM Cloud Private (ICP) clusters, but that's the only benefit you get out of the box. To fully leverage MCM's deployment automation, monitoring, topology and its other capabilities, you will have to create custom MCM YAML resources that get deployed with your charts. Luckily, no changes need to be made to the original Helm Charts or their application code.","title":"Creating Multicloud Manager Applications"},{"location":"mcm-applications/#guestbook-helm-charts-introduced","text":"In this document, we are going to cover the basic Application concepts we learned in Core Multicloud Manager Resources to Learn , but we will be using the Guestbook MCM Application code as an example to deploy 3 separate charts (explained later) on multiple clusters. The actual Guestbook application code comes from the Kubernetes Community, and it's located publicly on the kubernetes/examples repository. The application is broken up in the following 3 services: Redis Master : A Redis Master instance. Redis Slave : A Redis Slave instance, which replicates the contents of the Redis Master. Frontend : An Angular.js application that uses PHP script to communicate with Redis Master and Slave. In the application, you can submit Guestbook names, which get persisted in Redos. It sends new names to the Redis Master, which then get replicated to the Redis Slave. It then retrieves the names from the Redis Slave and prints them on the screen. The above 3 services were converted into the following 3 Helm Charts for the purpose of this demo. You can find the source code for the Helm Charts here: Redis Master : gbrm . Redis Slave : gbrs . Frontend : gbf . Lastly, in order to deploy the above charts through MCM, we are also going to need a Helm chart that deploys all of the MCM resources. Those resources do the actual deployment, monitoring, and topology setup for the 3 Helm Charts mentioned above. In the next sections, you are going to explore parts of the source code that make the gbapp Helm Chart (the chart with the MCM Resources) to understand the process of creating an Multicloud Manager Application . NOTE : To checkout the entire source code of the gbapp Helm Chart, checkout the link below: demos/guestbook/gbapp","title":"Guestbook Helm Charts Introduced"},{"location":"mcm-applications/#application-resource","text":"First things first, in order for MCM to recognize your workloads as an MCM application, you have to create an Application resource. The Application resource lets you specify criteria to associate Kubernetes resources to your application so that MCM can manage and monitor them. Let's look at the gbapp Application resource from the Helm Chart below: apiVersion: app.k8s.io/v1beta1 kind: Application metadata: name: {{ template guestbookapplication.fullname . }} labels: app: {{ template guestbookapplication.name . }} chart: {{ .Chart.Name }}-{{ .Chart.Version | replace + _ }} release: {{ .Release.Name }} heritage: {{ .Release.Service }} name: {{ template guestbookapplication.fullname . }} spec: selector: matchExpressions: - key: app operator: In values: - gbapp - gbf - gbrm - gbrs componentKinds: - group: core kind: Pods Where: apiVersion : Contains the Community SIG API version for Applications. kind : Specifies that this is an Application resource type. metadata : Contains the resource name and some labels. spec.selector.matchExpressions : Contains the selectors that MCM will use to associate other Kubernetes resources to the application. In this case, we are associated all resources with app label that contains any of the values listed in spec.selector.matchExpressions[0].values , which in this case are the names of the charts themselves. If you go to the source code of any of the Guestbook Helm Charts and explore its resources (deployments, services, etc) you will notice that they contain an app label that contains the name of the Helm chart. spec.selector.componentKinds : Contains a list of the kinds of resources to be associated with the application. In this case we are only associating components of kind Pods . However, you can associate resources of type Deployment , StatefulSet , Service , etc. You can assign whatever name you like to the group field. Later on, when we deploy the gbapp Helm Chart, you will see for yourself how the Guestbook resources are associated with the Application .","title":"Application Resource"},{"location":"mcm-applications/#deployable-resource","text":"Now that we know how to create an Application that knows how to associate resources to itself, it's time to learn how an Application can deploy those resources for itself. The way to do that is by creating a Deployable object, where you can specify what Helm chart to deploy and what namespace to deploy the chart into once the PlacementPolicy (explained in later sections) decides what clusters to deploy the Helm Charts into. Let's look at the gbapp-frontend Deployable resource from the Helm Chart below: apiVersion: mcm.ibm.com/v1alpha1 kind: Deployable metadata: name: {{ template guestbookapplication.fullname . }}-frontend labels: app: {{ template guestbookapplication.name . }} chart: {{ .Chart.Name }}-{{ .Chart.Version | replace + _ }} release: {{ .Release.Name }} heritage: {{ .Release.Service }} name: {{ template guestbookapplication.fullname . }}-frontend servicekind: ApplicationService spec: deployer: kind: helm helm: chartURL: {{ .Values.chartRepoURL }}/gbf-0.1.0.tgz namespace: {{ .Values.appInClusterNamespace }} Where: apiVersion : Contains the MCM API version for Deployables. kind : Specifies that this is an Deployable resource type. metadata : Contains the resource name and some labels. spec.deployer.kind : Specifies that is is a deployer that will deploy a Helm chart. spec.deployer.helm.chartURL : Specifies the URL with the Helm Chart location. In this case the Chart location is on this document's GitHub Repository. spec.deployer.helm.namespace : Specifies the cluster namespace to deploy the Helm chart to.","title":"Deployable Resource"},{"location":"mcm-applications/#relationship-resource","text":"The more Deployable objects you create for an Application , the harder it becomes to determine your application's topology, specially for newcomers in your team. MCM provides application topology features via Weave Scope out of the box. However, to explicitly specify dependencies between certain components (Deployment, Application, etc), you will need to create an ApplicationRelationship resource. Let's look at the gbapp-frontend-app ApplicationRelationship resource from the Helm Chart below: apiVersion: mcm.ibm.com/v1alpha1 kind: ApplicationRelationship metadata: name: {{ template guestbookapplication.fullname . }}-app-frontend labels: app: {{ template guestbookapplication.name . }} chart: {{ .Chart.Name }}-{{ .Chart.Version | replace + _ }} release: {{ .Release.Name }} heritage: {{ .Release.Service }} spec: type: contains source: kind: Application name: {{ template guestbookapplication.fullname . }} destination: kind: Deployable name: {{ template guestbookapplication.fullname . }}-frontend Where: apiVersion : Contains the MCM API version for ApplicationRelationship. kind : Specifies that this is an ApplicationRelationship resource type. metadata : Contains the resource name and some labels. spec.source.kind : specifies the type of Resource for the source object, which is Application in this case. spec.source.name : specifies the name of the source object, which is the guestbook-gbapp Application release and chart name combined. spec.destination.kind : specifies the type of Resource for the destination object, which is Deployable in this case. spec.destination.name : specifies the name of the destination object, which is the name for the gbabb-frontend Deployable.","title":"Relationship Resource"},{"location":"mcm-applications/#placement-policy-resource","text":"Normally, to deploy an application to multiple clusters, you to run the following steps on each cluster: Login to get the kubernetes context. Run the helm install command. This does not seem like a lot of work for 2 or 3 clusters. But the more clusters you have to work with, the more credentials you have to remember and manage. Also, if you are using CI/CD pipelines to deploy your workloads, the more clusters you have, the more pipelines you have to write and embed those credentials to, which can lead to human errors. If those credentials were to change, you would have to update those credentials wherever they are being used, which can get hard to track. By leveraging MCM, access to your clusters is managed from a central location. MCM expands on this capability with Placement Policies by allowing you to deploy workloads through it by specifying cluster labels, which are much easier to remember and manage. By adopting MCM into your CI/CD pipelines, you only have to remember the credentials for the MCM Hub cluster. To deploy workloads to multiple clusters, you only have to provide the number of application replicas and the cluster labels that match the clusters that you would like to deploy them to. Let's look at the gbapp-frontend PlacementPolicy resource from the Helm Chart below: apiVersion: mcm.ibm.com/v1alpha1 kind: PlacementPolicy metadata: name: {{ template guestbookapplication.fullname . }}-frontend labels: app: {{ template guestbookapplication.name . }} chart: {{ .Chart.Name }}-{{ .Chart.Version | replace + _ }} release: {{ .Release.Name }} heritage: {{ .Release.Service }} name: {{ template guestbookapplication.fullname . }}-frontend servicekind: CacheService spec: clusterReplicas: {{ .Values.replicaCount }} clusterLabels: matchLabels: {{ toYaml .Values.targetCluster.labelSelector.matchLabels | indent 6 }} resourceHint: {{ toYaml .Values.targetCluster.resourceSelector | indent 4 }} compliances: {{ .Values.targetCluster.compliances }} Where: apiVersion : Contains the MCM API version for PlacementPolicy. kind : Specifies that this is an PlacementPolicy resource type. metadata : Contains the resource name and some labels. spec.clusterReplicas : contains the number of replicas (application instances) to deploy. spec.clusterLabels.matchLabels : Contains the list of labels and values that clusters must have for MCM to deploy applications to them. spec.resourceHint : If more than one cluster matches the labels and values provided above, you can specify to MCM a resource specific criteria to select the clusters. For example, you can ask MCM to select the clsuter with the most available CPU cores. spec.compliances : Contains a list of Compliances (outside of the scope of this chapter) resources to attach this policy to. To learn about Compliance resources, checkout the Create Multi-cluster Compliance Policies Chapter .","title":"Placement Policy Resource"},{"location":"mcm-applications/#placement-binding-resource","text":"Just like how a Kubernetes Role requires RoleBinding to attach it to a specific ServiceAccount , a PlacementPolicy requires another resource that binds its rules to specific resources. This MCM resources is the PlacementBinding , which can bind the PlacementPolicy rules to specific Deployables. Let's look at the gbapp-frontend PlacementPolicy resource from the Helm Chart below: apiVersion: mcm.ibm.com/v1alpha1 kind: PlacementBinding metadata: name: {{ template guestbookapplication.fullname . }}-frontend labels: app: {{ template guestbookapplication.name . }} chart: {{ .Chart.Name }}-{{ .Chart.Version | replace + _ }} release: {{ .Release.Name }} heritage: {{ .Release.Service }} name: {{ template guestbookapplication.fullname . }}-frontend servicekind: CacheService placementRef: apiGroup: mcm.ibm.com kind: PlacementPolicy name: {{ template guestbookapplication.fullname . }}-frontend subjects: - apiGroup: mcm.ibm.com kind: Deployable name: {{ template guestbookapplication.fullname . }}-frontend Where: apiVersion : Contains the MCM API version for PlacementBinding. kind : Specifies that this is an PlacementBinding resource type. metadata : Contains the resource name and some labels. placementRef.name : Specifies the PlacementPolicy name to bind. subjects[0].name : Specifies the Deployable name to bind the PlacementPolicy to.","title":"Placement Binding Resource"},{"location":"mcm-applications/#mcm-application-helm-chart","text":"The above YAML resources mostly cover supporting the frontend Helm Chart. To see how we put together an MCM Application Helm Chart that deploys all of the 3 Guestbook Helm Charts, checkout the source code at the link below: demos/guestbook/gbapp","title":"MCM Application Helm Chart"},{"location":"mcm-applications/#deployment-mcm-application","text":"Now that you are aware of how to create MCM Applications, it is time to deploy the gbapp MCM Application Helm Chart, which deploys the Guestbook Frontend ( gbf ), Redis Master ( gbrm ), and Redis Slave ( gbrs ) charts. Before you start with the examples, you will need to clone the code and log into the MCM Hub Cluster with the commands below: # Clone the Repository git clone https://github.com/ibm-cloud-architecture/kubernetes-multicloud-management.git Also, open a new browser tab and go to the Applications page on the MCM Dashboard, as shown below: You should then be greeted by an empty Applications page, which we will be populating later: Now you are ready to start deploying the MCM Application!","title":"Deployment MCM Application"},{"location":"mcm-applications/#0-create-image-policies-on-both-clusters","text":"Since ICP version 3.1, you are required to create Image Policies that allow you to pull Docker images from specific Docker registries ( gcr.io in our case). To do so, let's run the following commands on EACH ICP CLUSTER : # Login to the ICP Cluster cloudctl login -a https://ICP_MASTER_IP:8443 -n default --skip-ssl-validation # Go to Helm Charts directory cd demos/guestbook # Create the Image Policy in the ICP Cluster kubectl apply -f demos/guestbook/guestbook-cluster-image-policy.yaml Don't forget to run the above commands on EACH ICP Cluster so that there are no issues when deploying the gbapp application.","title":"0. Create Image Policies on Both Clusters"},{"location":"mcm-applications/#1-deploy-to-dev-cluster","text":"The process of deploying an MCM Application is the same as deploying a Helm Chart. To deploy the gbapp MCM Application Helm chart, run the commands below: # Log into MCM HUB Cluster cloudctl login -a https://HUB_CLUSTER_MASTER_IP:8443 -n default --skip-ssl-validation # If not already there, go to the Helm Charts directory cd demos/guestbook # Deploy MCM Application Helm Chart helm upgrade --install guestbook --set replicaCount=1 --set targetCluster.labelSelector.matchLabels.environment=Dev ./gbapp --tls Where: replicaCount : is a field that indicates the number of application instances we are deploying, which is 1. targetCluster.labelSelector.matchLabels.environment : means we are using the environment field with a value of Dev , which tells the MCM Controller to should find a cluster that has that value for that field. Checkout the demos/guestbook/gbapp/values.yaml to learn the different fields we can use as cluster selectors. Assuming that everything went well, you should now see the guestbook-gbapp MCM Application show up in the Applications page as shown below: The above is all it takes to deploy an MCM Application. To deploy to multiple clusters (done in later sections) all you have to do is change the fields, values, and number of replicas in the helm upgrade command and MCM takes care of the rest. Before exploring the multiple cluster scenario, let's first get familiar with examining MCM Application details on the Applications page.","title":"1. Deploy to Dev Cluster"},{"location":"mcm-applications/#2-exploring-the-application-details-page","text":"If you click on the guestbook-gbapp entry, you will be taken to that application's overview page, which contains multiple sections that show you the custom MCM Resources that were deployed. The first section you will encounter is the Application Details , which contains the high level MCM Application details: If you scroll down, you see a section for the Application Deployments , which show the deployment status foe each of the 3 Helm Charts. If there was an error on the charts deployment, you will see the deployment status reflected under the Status column and the failure details under the Reason column. If you scroll down further, you will encounter the Placement Policies section, which contains the placement policies used to deploy the Helm charts into the se-stg-31 cluster. If you look under the Decisions column, you will notice that, after finding a cluster that matches the labels we provided earlier, it decided to use the se-dev-312-ubuntu cluster (for you it's going to be se-dev-31 or whatever name you assigned to the cluster). Down below is the Placement Bindings section, which should show the Placement Policies and their respected Subjects (Deployables) to be bound to. After that you will find the Deployable Objects section, which contain a list of all of the Helm Charts, along with the Chart URL and the namespace to deploy them into. Next you will find the Application Relationships section, which contains a list of all the relationships between the Application and the Deployable objects. Last but not least, if you click on the Diagram tab, you will see a visual representation of the Application and all of the resources we inspected above, including the Helm Charts resources (Pods, Deployments, and Services). This diagram is available thanks to the Weave Scope component in MCM, which takes the Application Relationship objects and its own algorithm to deduce and reduce the application topology on the screen. If you click on any of the components you will open a small view with either a short description or the YAML resource file that was used to create the resource. Now that you have learned how to navigate the Application Details page, let's proceed with redeploying the application into another cluster, followed by multiple clusters.","title":"2. Exploring the Application Details Page"},{"location":"mcm-applications/#3-redeploy-to-staging-cluster","text":"To redeploy the application to the se-stg-31 cluster, all we have to do is change the value of the environment field to Staging , and MCM will take care of uninstalling the application from the se-dev-31 cluster and then redeploying it to the se-stg-31 cluster. To do so, run the command below: # If not already there, go to the Helm Charts directory cd demos/guestbook # Deploy MCM Application Helm Chart helm upgrade --install guestbook --set replicaCount=1 --set targetCluster.labelSelector.matchLabels.environment=Staging ./gbapp --tls That's all it takes. With the same cloudctl session, we were able to move the application from one cluster to another by simply changing the value of a label. This is very powerful for operations folks or anyone responsible for deploying an application to multiple environments as it is very user intuitive while leveraging the capabilities of the MCM platform. I encourage you to refresh the Applications page and explore how the resources mentioned earlier changed. The only change was the cluster name that the resources were deployed into.","title":"3. Redeploy to Staging Cluster"},{"location":"mcm-applications/#4-redeploy-to-both-clusters","text":"Now let's go ahead and deploy the application to both clusters simultaneously! To do so, we will have to use a different label of which value is shared by both clusters. Both se-dev-31 and se-stg-31 clusters actually share the value of case for the label owner , so we will use that for the deployment. The last thing that remains is to increase the value of replicaCount from 1 to 2 in order for MCM to know that it needs to install the application on two clusters that match label values. To install the application on both clusters, run the command below: # If not already there, go to the Helm Charts directory cd demos/guestbook # Deploy MCM Application Helm Chart helm upgrade --install guestbook --set replicaCount=2 --set targetCluster.labelSelector.matchLabels.owner=case ./gbapp --tls That's really it. Once again, by using the same cloudctl session to the HUB Cluster, we were able to redeploy the application to both clusters by simply using a label with a value that both clusters share and increasing the replicaCount value to 2 . Imagine how easy it would be to write a CI/CD pipeline to promote an application from one environment to another and run tests! NOTE : More on CI/CD on the DevOps with MCM Tutorial Chapter . Anyways, feel free to refresh and checkout the MCM Application Details page again to see what changed. Most resources look the same with the main difference being the Application Deployments section below, which contains details of all the Helm deployments across both clusters instead of just one. Another thing that changed significantly is the Diagram tab below, which shows a topology diagram that includes the resources of both clusters.","title":"4. Redeploy to Both Clusters"},{"location":"mcm-applications/#5-health-view-dashboard","text":"When deploying an MCM Application, MCM creates a dashboard in Grafana that lets you visualize CPU, memory, filesystem, and bandwidth usage. The data is shows is pulled from the Federated Prometheus instance that gets deployed with the MCM Controller chart. To access the Health View, first go back to the Applications page as shown below: Now click the Launch Health View link under the Dashboard column and it should open a Grafana dashboard in a new page that looks similar to the following: Feel free to scroll around to see the resource consumption for each of the Helm Chart deployments.","title":"5. Health View Dashboard"},{"location":"mcm-applications/#5-cleanup","text":"To uninstall the MCM Application from both clusters, run the command below: helm delete guestbook --purge --tls","title":"5. Cleanup"},{"location":"mcm-applications/#conclusion","text":"We hope this chapter opened your mind and made you realize how easy it is to use MCM to deploy an application to multiple clusters from a central location and with a simple cluster selection criteria. Usually, when companies company grows, the number of application environments grow as well and the complexity of deploying and managing applications on those environments grows as well. Now that you know how to use MCM to deploy, manage, and monitor your applications on multiple clusters, you should checkout the DevOps with MCM Tutorial Chapter to learn how to use MCM in an automated CI/CD pipeline to promote applications between environments and run tests.","title":"Conclusion"},{"location":"mcm-compliance/","text":"MCM Compliance Manager Author: Fabio Gomez (fabiogomez@us.ibm.com) Introduction In the previous chapter we spoke of the complexities of managing applications across multiple clusters and how MCM takes that complexity away with its placement policies. We also covered a DevOps demo where you can get first hand experience on how easy it is to deploy an application to multiple clusters by leveraging MCM's placement policies. What was not mentioned in the above scenario was that, outside of setting up MCM Hub Cluster and Klusterlets properly, to get the Guestbook app to work we had to create a Cluster Image Policy in both clusters to allow them to download Docker images from Docker Hub and other Docker registries. This manual step may be trivial when you have just 2 clusters. However, the more granular your configuration requirements become and the more environments/clusters you have, it gets more difficult, time-consuming, and error-prone to manage cluster configuration. Configuration management tools, such as Chef Ansible , have existed for a while and have become paramount for managing infrastructure configuration at scale. However, at the time of writing, there is no major configuration management solution for Kubernetes native resources such as Deployments , Quotas , Roles , Role Bindings , etc. MCM attempts to solve this with its MCM Compliance Manager feature, which provides a desired state-based management approach for enforcing custom compliance policies (explained in later sections) on clusters that are managed by MCM. Such policies allow you to, for example, enforce the existence of any Kubernetes resource across all of your Kubernetes clusters. The Cluster Image Policy scenario mentioned above is a clear use case for MCM Compliance Manager. The MCM Compliance Manager policies also allow you to simply inform you of the existence or non-existence of a Kubernetes resource across your clusters without the need of enforcement, thus showing compliance status of individual clusters. The Basics There are 5 concepts that you need to understand to make sense of the Compliance Manager's features. These concepts are: Template Policy Compliance Placement Policy Placement Binding In the following sections we will be explaining the concepts above. However, for this to make sense, we will apply the concepts to a single use case with the following criteria: 2 IBM Cloud Private clusters that are managed by MCM. Each cluster is required to have a Role called operator-role . The operator-role MUST have the following access to Pod resources. get list watch The operator-role must NOT have the following access to Secrets resources: get list watch delete create update patch If the operator-role does not exist, MCM will enforce it into existence. The enforcement applies only to clusters with the following labels and values: vendor : ICP Now that we understand the criteria of the use case above, we can start explaining the custom resources that MCM requires to enforce it. Template A Template defines a list of policyRules that determine whether a particular Kubernetes Resource is compliant or not. These rules are basically a list of attributes that the given Kubernetes Resource must and/or must not have in order to be compliant. To better understand a Template , let's look an actual YAML file for a Template of kind RoleTemplate : apiVersion: roletemplate.mcm.ibm.com/v1alpha1 kind: RoleTemplate metadata: namespace: # will be inferred name: operator-role complianceType: musthave # at this level, it means the role must exist with the rules that it musthave below rules: - complianceType: mustnothave # at this level, it means if the role exists the rule is a mustnothave policyRule: apiGroups: [ core ] resources: [ secrets ] verbs: [ get , list , watch , delete , create , update , patch ] - complianceType: musthave # at this level, it means if the role exists the rule is a musthave policyRule: apiGroups: [ core ] resources: [ pods ] verbs: [ get , list , watch ] Here is a breakdown of the fields above: apiVersion : The path where the custom API is defined. kind : Defines the template kind, which is RoleTemplate in this case. metadata : namespace : The Kubernetes namespace for the template. name : The name of the template. complianceType : This field determines whether the resource ( Role in this case) that matches the given rules must exist or not. musthave : Means that the resource with the matching rules must exist. mustnothave : Means that the resource with the matching rules must NOT exist. Note that the above fields are specific to Template s in general. The following fields are specific to RoleTemplate : rules : List of rules that will determine the compliance of a resource ( Role in this case). complianceType : musthave or mustnothave . Similar to a Template's higher level complianceType , this field determines whether the rule inside the template must exist or not. policyRule : This is the actual compliance rule, which is defined by the fields below: apiGroups : Array of API Groups to apply this rule against. resources : Array of Resources from API Groups mentioned above to apply this rule against. verbs : Array of verbs that apply to the API Groups and Resources mentioned above. The contents of the templates will vary based on the target Kubernetes resource. But just know that the contents of a template is what defines the rules that Compliance Manager will be checking for in order to meet compliance. A Template is the most important concept to grasp correctly as everything that follows revolves around it. But don't worry, it's all downhill from here. Policy Now that we understand what a Template is, it will be much easier to explain what a Policy is. A Policy is responsible for the following: Take a list of Template s and enforce their rules in a cluster. Determine which namespaces to enforce the rules into. Determine what remediation action to take when compliance is not met. To better understand a Policy , let's look an actual YAML file for a Policy object: apiVersion: policy.mcm.ibm.com/v1alpha1 kind: Policy metadata: name: policy-all-icp description: operator-role policy spec: remediationAction: enforce # enforce or inform complianceType: musthave # used as default, when missing in a particular sub-template namespaces: include: [ default ] exclude: [ kube* ] role-templates: - apiVersion: roletemplate.mcm.ibm.com/v1alpha1 kind: RoleTemplate metadata: namespace: # will be inferred name: operator-role complianceType: musthave # at this level, it means the role must exist with the rules that it musthave below rules: - complianceType: mustnothave # at this level, it means if the role exists the rule is a mustnothave policyRule: apiGroups: [ core ] resources: [ secrets ] verbs: [ get , list , watch , delete , create , update , patch ] - complianceType: musthave # at this level, it means if the role exists the rule is a musthave policyRule: apiGroups: [ core ] resources: [ pods ] verbs: [ get , list , watch ] Here is a breakdown of the fields above: apiVersion : The path where the custom API is defined. kind : Policy in this case. spec : remediationAction : this is where you specify what you would like the Compliance Manager to do when there is a non-compliance for the given Policy. Here are the options. enforce : If non-compliance is found, enforce policy by creating/deleting the resources in role-templates . inform : If non-compliance is found, don't enforce policy but inform the MCM Admin of the non-compliance via the Dashboard. namespaces : These are the namespaces where the policies will be enforced in. include : Array or expression of namespaces to INCLUDE in the policy checks. exclude : Array or expression of namespaces to EXCLUDE in the policy checks. role-templates : List of Templates ( RoleTemplates in this case), which contain the rules to enforce. NOTE : You could also use object-templates field (not shown above) to create templates for Kubernetes resources of any kind. Note that, we skipped role-templates as we already broke down and explained the contents of a Template. The only comment to add is that the templates' namespace will be inherited from the Policy's namespaces field. Compliance We now understand that a Template defines the rules to enforce and a Policy defines how and where (namespaces) the rules are enforced in a cluster. However, a Policy must still be a applied to each cluster. Applying the policies manually may be fine for 1 or 2 clusters. But once again, the more abundant and more granular the policies become and the more clusters you have to manage, the more cumbersome managing the policies become. Luckily, the Compliance Manager can manage this for you with a Compliance object. A Compliance object takes a list of Policy objects (and their respective Template s) and determines what clusters to apply them to based on cluster selectors. A Compliance , though not apparent by its name alone, is the final piece that ties the policies into the actual infrastructure (clusters) and, therefore, fully implement Compliance. To better understand a Compliance , let's look an actual YAML file for a Compliance object, which contains 2 Policy objects: apiVersion: compliance.mcm.ibm.com/v1alpha1 kind: Compliance metadata: name: compliance-all-icp namespace: mcm description: operator-role compliance spec: runtime-rules: - apiVersion: policy.mcm.ibm.com/v1alpha1 kind: Policy metadata: name: policy-all-icp description: operator-role policy spec: remediationAction: enforce # enforce or inform complianceType: musthave # used as default, when missing in a particular sub-template namespaces: include: [ default ] exclude: [ kube* ] role-templates: - apiVersion: roletemplate.mcm.ibm.com/v1alpha1 kind: RoleTemplate metadata: namespace: # will be inferred name: operator-role complianceType: musthave # at this level, it means the role must exist with the rules that it musthave below rules: - complianceType: mustnothave # at this level, it means if the role exists the rule is a mustnothave policyRule: apiGroups: [ core ] resources: [ secrets ] verbs: [ get , list , watch , delete , create , update , patch ] - complianceType: musthave # at this level, it means if the role exists the rule is a musthave policyRule: apiGroups: [ core ] resources: [ pods ] verbs: [ get , list , watch ] The above may look overwhelming, but outside a few fields specific to the Compliance object, there is nothing we haven't already seen above. Here is a breakdown of the fields above: apiVersion : The path where the custom API is defined. kind : Compliance in this case. metadata : The common Kubernetes metadata object. There are a couple of fields that are of interest for Compliance Manager. namespace : This is the name of the namespace that was created specifically for MCM to put Compliance objects during MCM installation. description : Description of the Compliance object, which will be displayed in the MCM Dashboard. spec : runtime-rules : This is a list of Policy objects that will be applied to the clusters based on the PlacementPolicy (explained in the next section). Once again, the Compliance object is responsible for applying the specified Policy objects to the matching clusters and reporting status back to the Compliance Manager dashboard in MCM. Placement Policy Now that you understand the functionality of a Compliance object, it's time to decide what clusters to apply this Compliance to. For that, we need to create the following Custom Resources: A Placement Policy resource, where, via cluster labels, you define the rules for selecting the clusters to apply the Compliance object to. A Placement Binding resource, where you bind a specific Placement Policy object to a specific Compliance object, which is the final step that tells MCM to start applying the Compliance object to the selected clusters. We will go over Placement Binding in more detail in the next section. To better understand a PlacementPolicy , let's look an actual YAML file for a Placement Policy object: apiVersion: mcm.ibm.com/v1alpha1 kind: PlacementPolicy metadata: name: placement-policy-all-icp namespace: mcm spec: clusterLabels: matchLabels: vendor: ICP Here is a breakdown of the fields above: apiVersion : The path where the custom API is defined. kind : PlacementPolicy in this case. spec : clusterLabels : matchLabels : Here is where we define the labels to select the clusters where the Compliance object will be applied to. In this case we used the label vendor with a value of ICP , which means to apply the Compliance object to all ICP clusters registered with MCM Controller. You can use a combination of the following filters to select clusters: matchNames : List of cluster names to match against. matchLabels : List of cluster labels to match against. These are the labels which fields you for a cluster during Klusterlet installation. matchExpressions : List of expressions to match against. matchConditions : List of cluster conditions to match against, usually involving cluster health status. Placement Binding Now that we have both a Compliance object (with all its rules and templates) and a Placement Policy object, a Placement Binding resource is required to bind them together and start the process of applying the Compliance policies to the clusters based on the Placement Policy criteria. To better understand a PlacementBinding , let's look an actual YAML file for a Placement Binding object: apiVersion: mcm.ibm.com/v1alpha1 kind: PlacementBinding metadata: name: placement-binding-all-icp namespace: mcm placementRef: name: placement-policy-all-icp subjects: - name: compliance-all-icp kind: Compliance Here is a breakdown of the fields above: apiVersion : The path where the custom API is defined. kind : PlacementBinding in this case. placementRef : name : The name of the Placement Policy to bind. subjects : The list of objects to bind the Placement Policy with. name : The name of the object to bind the Placement Policy with. kind : The type of object (Compliance in our case) that it's going to be bound with the Placement Policy. We have now introduced and explained all of the resources that are required to create and enforce Compliance Policies on Kubernetes Clusters. Now let's move on to a quick tutorial. Compliance Manager Tutorial Now that we know the basic concepts and understand on a high level the components that power the Compliance Manager, let's go ahead and apply the above Compliance, Placement Policy, and Placement Binding objects to our clusters and see what that looks like on the Compliance Manager dashboard. Pre-requisites This tutorial will assume you have the following pre-requisites: 2 IBM Cloud Private 3.1 Clusters loaded with the MCM 3.1.2 PPA loaded Cluster 1 Setup : Install MCM Controller Chart Create a separate namespace called mcm , which will be used to store the Compliance objects. Install MCM Klusterlet Chart and enter the following values: Name : se-stg-31 Namespace : mcm-se-stg Labels : cloud : IBM datacenter : dallas environment : Staging owner : case region : US vendor : ICP Cluster 2 Setup : Install MCM Klusterlet Chart and enter the following values: Name : se-dev-31 Namespace : mcm-se-dev Labels : cloud : IBM datacenter : austin environment : Dev owner : case region : US vendor : ICP 1. Apply the Compliance Now that we have installed MCM in both clusters, let's create the Compliance , Placement Policy , and Placement Binding objects located at demos/compliance . Here is a high level breakdown of the contents of the Compliance object: A Policy that enforces the creation of the policy's RoleTemplate if the cluster is found non-compliant. This means that even if the cluster was found compliant and, for some reason, a cluster admin deletes the role, the Compliance Manager will recreate it. Let's proceed with the Compliance object creation by running the following commands: # Login against MCM Hub Cluster cloudctl login -a https://${MCM_HUB_CLUSTER_MASTER_IP}:8443 -u ${USERNAME} -p ${PASSWORD} -n mcm --skip-ssl-validation; # Clone the Reference Architecture Repository git clone git@github.com:ibm-cloud-architecture/kubernetes-multicloud-management.git # Go to the project's folder cd kubernetes-multicloud-management # Apply the Compliance kubectl apply -f demos/compliance Where: ${MCM_HUB_CLUSTER_MASTER_IP} is the IP Address of the master node in the MCM Hub Cluster. ${USERNAME} is admin user. ${PASSWORD} is the admin password. mcm is the namespace in which the Compliance object will be created. In the next section, we will use the MCM Compliance Manager dashboard to see the compliance results of the above Compliance object. 2. View Compliance Status on the Dashboard To access the Policies view for the Compliance Manager, open a new browser window and go to https://${MCM_HUB_CLUSTER_MASTER_IP}:8443/multicloud/policies , where ${MCM_HUB_CLUSTER_MASTER_IP} is the IP Address of the master node in the MCM Hub Cluster. If the Compliance object was created successfully, you will see a screen that looks like this: Now click on compliance-all-icp to see more details on the Compliance object. The first thing you will see are some high level details for the Compliance , which are similar to what we saw in the previous screenshot: If you scroll down, you will see the Compliance YAML Template section that shows the actual YAML file that was used to create the Compliance, which you can also edit directly: If you scroll further down you can see the Compliance Status and Compliance Policies sections: Here is a breakdown of the above 2 sections: Compliance Status : The overall compliance status of an individual cluster. Cluster Name : The name of the cluster. Policy Compliant : Ratio of compliant policies to total number of policies. Policy Valid : Ratio of policies with valid syntax to total number of policies. Compliance Policies : A breakdown of all the policies and their compliance status for each cluster . Compliant : Whether the given policy is compliant. Name : Name of the policy. Cluster Compliant : List of compliant clusters. Cluster Not Compliant : List of non-compliant clusters. If you scroll further down you can see the following Placement Policies and Placement Bindings sections: Here is a breakdown of the above 2 sections: Placement Policies : The overall compliance status of an individual cluster. Name : The name of the Placement Policy. Namespace : The namespace ( mcm ) in the MCM Hub cluster where this Placement Policy resource is stored. Replicas : This is the number of application instances to be installed in the selected clusters. This field is only relevant when deploying actual MCM Applications, which we explained in the MCM Applications chapter. Cluster Selector : The cluster labels that were used to select the clusters. Resource Selector : The resource selectors (CPU, RAM, etc) that were used to select the clusters, which is none in our case. Decisions : The clusters that MCM decided to apply the Compliance object to based on the selectors. Placement Bindings : A breakdown of all the policies and their compliance status for each cluster . Name : The name of the Placement Binding. Namespace : The namespace ( mcm ) in the MCM Hub cluster where this Placement Binding resource is stored. Placement Policy : The name of the Placement Policy to be bound. Subjects : The name of the objects (and their types) that the Placement Policy will be bound to, which is the Compliance object. The above should give you a high level idea of the compliance status for each policy in each cluster. 3. View Individual Policy The above section showed us how to get a high level compliance status for each policy on each cluster. If we want to investigate, for example, what exactly is causing a policy to show a non-compliant status, then we have to inspect the contents of the policy itself. To view the contents of any policy, click the name of the policy in the Compliance Policies section shown in the last screenshot. For this example, go ahead and click on the policy-all-icp policy to examine its contents. You should now be greeted with the screen above, which gives you the high level policy details that were explained in the The Basics section. If you scroll down, you will see the Policy YAML Template section that shows the actual YAML file that was used to create the Policy, which you can also edit directly. In here you will also see the Role Template , with the API Groups and Verbs that it should and shouldn't have: If you scroll further down, you will encounter the Role Templates section: Here is a breakdown of the above section: Template Details : Name : Name of the template, which is operator-role in this case. Compliance Type : This field determines whether the resource ( Role in this case) that matches the given rules must exist or not, which in this case must exist based on its musthave value. API version : The API version that was used to create the template. Conclusion The more you use the Compliance Manager and implement Compliance objects into more clusters, the easier it becomes to maintain increasingly complicated configurations across multiple clusters. At this point, it is up to you to use the skills you just learned and implement Compliance policies that best suit your needs.","title":"Create Multi-cluster Compliance Policies"},{"location":"mcm-compliance/#mcm-compliance-manager","text":"Author: Fabio Gomez (fabiogomez@us.ibm.com)","title":"MCM Compliance Manager"},{"location":"mcm-compliance/#introduction","text":"In the previous chapter we spoke of the complexities of managing applications across multiple clusters and how MCM takes that complexity away with its placement policies. We also covered a DevOps demo where you can get first hand experience on how easy it is to deploy an application to multiple clusters by leveraging MCM's placement policies. What was not mentioned in the above scenario was that, outside of setting up MCM Hub Cluster and Klusterlets properly, to get the Guestbook app to work we had to create a Cluster Image Policy in both clusters to allow them to download Docker images from Docker Hub and other Docker registries. This manual step may be trivial when you have just 2 clusters. However, the more granular your configuration requirements become and the more environments/clusters you have, it gets more difficult, time-consuming, and error-prone to manage cluster configuration. Configuration management tools, such as Chef Ansible , have existed for a while and have become paramount for managing infrastructure configuration at scale. However, at the time of writing, there is no major configuration management solution for Kubernetes native resources such as Deployments , Quotas , Roles , Role Bindings , etc. MCM attempts to solve this with its MCM Compliance Manager feature, which provides a desired state-based management approach for enforcing custom compliance policies (explained in later sections) on clusters that are managed by MCM. Such policies allow you to, for example, enforce the existence of any Kubernetes resource across all of your Kubernetes clusters. The Cluster Image Policy scenario mentioned above is a clear use case for MCM Compliance Manager. The MCM Compliance Manager policies also allow you to simply inform you of the existence or non-existence of a Kubernetes resource across your clusters without the need of enforcement, thus showing compliance status of individual clusters.","title":"Introduction"},{"location":"mcm-compliance/#the-basics","text":"There are 5 concepts that you need to understand to make sense of the Compliance Manager's features. These concepts are: Template Policy Compliance Placement Policy Placement Binding In the following sections we will be explaining the concepts above. However, for this to make sense, we will apply the concepts to a single use case with the following criteria: 2 IBM Cloud Private clusters that are managed by MCM. Each cluster is required to have a Role called operator-role . The operator-role MUST have the following access to Pod resources. get list watch The operator-role must NOT have the following access to Secrets resources: get list watch delete create update patch If the operator-role does not exist, MCM will enforce it into existence. The enforcement applies only to clusters with the following labels and values: vendor : ICP Now that we understand the criteria of the use case above, we can start explaining the custom resources that MCM requires to enforce it.","title":"The Basics"},{"location":"mcm-compliance/#template","text":"A Template defines a list of policyRules that determine whether a particular Kubernetes Resource is compliant or not. These rules are basically a list of attributes that the given Kubernetes Resource must and/or must not have in order to be compliant. To better understand a Template , let's look an actual YAML file for a Template of kind RoleTemplate : apiVersion: roletemplate.mcm.ibm.com/v1alpha1 kind: RoleTemplate metadata: namespace: # will be inferred name: operator-role complianceType: musthave # at this level, it means the role must exist with the rules that it musthave below rules: - complianceType: mustnothave # at this level, it means if the role exists the rule is a mustnothave policyRule: apiGroups: [ core ] resources: [ secrets ] verbs: [ get , list , watch , delete , create , update , patch ] - complianceType: musthave # at this level, it means if the role exists the rule is a musthave policyRule: apiGroups: [ core ] resources: [ pods ] verbs: [ get , list , watch ] Here is a breakdown of the fields above: apiVersion : The path where the custom API is defined. kind : Defines the template kind, which is RoleTemplate in this case. metadata : namespace : The Kubernetes namespace for the template. name : The name of the template. complianceType : This field determines whether the resource ( Role in this case) that matches the given rules must exist or not. musthave : Means that the resource with the matching rules must exist. mustnothave : Means that the resource with the matching rules must NOT exist. Note that the above fields are specific to Template s in general. The following fields are specific to RoleTemplate : rules : List of rules that will determine the compliance of a resource ( Role in this case). complianceType : musthave or mustnothave . Similar to a Template's higher level complianceType , this field determines whether the rule inside the template must exist or not. policyRule : This is the actual compliance rule, which is defined by the fields below: apiGroups : Array of API Groups to apply this rule against. resources : Array of Resources from API Groups mentioned above to apply this rule against. verbs : Array of verbs that apply to the API Groups and Resources mentioned above. The contents of the templates will vary based on the target Kubernetes resource. But just know that the contents of a template is what defines the rules that Compliance Manager will be checking for in order to meet compliance. A Template is the most important concept to grasp correctly as everything that follows revolves around it. But don't worry, it's all downhill from here.","title":"Template"},{"location":"mcm-compliance/#policy","text":"Now that we understand what a Template is, it will be much easier to explain what a Policy is. A Policy is responsible for the following: Take a list of Template s and enforce their rules in a cluster. Determine which namespaces to enforce the rules into. Determine what remediation action to take when compliance is not met. To better understand a Policy , let's look an actual YAML file for a Policy object: apiVersion: policy.mcm.ibm.com/v1alpha1 kind: Policy metadata: name: policy-all-icp description: operator-role policy spec: remediationAction: enforce # enforce or inform complianceType: musthave # used as default, when missing in a particular sub-template namespaces: include: [ default ] exclude: [ kube* ] role-templates: - apiVersion: roletemplate.mcm.ibm.com/v1alpha1 kind: RoleTemplate metadata: namespace: # will be inferred name: operator-role complianceType: musthave # at this level, it means the role must exist with the rules that it musthave below rules: - complianceType: mustnothave # at this level, it means if the role exists the rule is a mustnothave policyRule: apiGroups: [ core ] resources: [ secrets ] verbs: [ get , list , watch , delete , create , update , patch ] - complianceType: musthave # at this level, it means if the role exists the rule is a musthave policyRule: apiGroups: [ core ] resources: [ pods ] verbs: [ get , list , watch ] Here is a breakdown of the fields above: apiVersion : The path where the custom API is defined. kind : Policy in this case. spec : remediationAction : this is where you specify what you would like the Compliance Manager to do when there is a non-compliance for the given Policy. Here are the options. enforce : If non-compliance is found, enforce policy by creating/deleting the resources in role-templates . inform : If non-compliance is found, don't enforce policy but inform the MCM Admin of the non-compliance via the Dashboard. namespaces : These are the namespaces where the policies will be enforced in. include : Array or expression of namespaces to INCLUDE in the policy checks. exclude : Array or expression of namespaces to EXCLUDE in the policy checks. role-templates : List of Templates ( RoleTemplates in this case), which contain the rules to enforce. NOTE : You could also use object-templates field (not shown above) to create templates for Kubernetes resources of any kind. Note that, we skipped role-templates as we already broke down and explained the contents of a Template. The only comment to add is that the templates' namespace will be inherited from the Policy's namespaces field.","title":"Policy"},{"location":"mcm-compliance/#compliance","text":"We now understand that a Template defines the rules to enforce and a Policy defines how and where (namespaces) the rules are enforced in a cluster. However, a Policy must still be a applied to each cluster. Applying the policies manually may be fine for 1 or 2 clusters. But once again, the more abundant and more granular the policies become and the more clusters you have to manage, the more cumbersome managing the policies become. Luckily, the Compliance Manager can manage this for you with a Compliance object. A Compliance object takes a list of Policy objects (and their respective Template s) and determines what clusters to apply them to based on cluster selectors. A Compliance , though not apparent by its name alone, is the final piece that ties the policies into the actual infrastructure (clusters) and, therefore, fully implement Compliance. To better understand a Compliance , let's look an actual YAML file for a Compliance object, which contains 2 Policy objects: apiVersion: compliance.mcm.ibm.com/v1alpha1 kind: Compliance metadata: name: compliance-all-icp namespace: mcm description: operator-role compliance spec: runtime-rules: - apiVersion: policy.mcm.ibm.com/v1alpha1 kind: Policy metadata: name: policy-all-icp description: operator-role policy spec: remediationAction: enforce # enforce or inform complianceType: musthave # used as default, when missing in a particular sub-template namespaces: include: [ default ] exclude: [ kube* ] role-templates: - apiVersion: roletemplate.mcm.ibm.com/v1alpha1 kind: RoleTemplate metadata: namespace: # will be inferred name: operator-role complianceType: musthave # at this level, it means the role must exist with the rules that it musthave below rules: - complianceType: mustnothave # at this level, it means if the role exists the rule is a mustnothave policyRule: apiGroups: [ core ] resources: [ secrets ] verbs: [ get , list , watch , delete , create , update , patch ] - complianceType: musthave # at this level, it means if the role exists the rule is a musthave policyRule: apiGroups: [ core ] resources: [ pods ] verbs: [ get , list , watch ] The above may look overwhelming, but outside a few fields specific to the Compliance object, there is nothing we haven't already seen above. Here is a breakdown of the fields above: apiVersion : The path where the custom API is defined. kind : Compliance in this case. metadata : The common Kubernetes metadata object. There are a couple of fields that are of interest for Compliance Manager. namespace : This is the name of the namespace that was created specifically for MCM to put Compliance objects during MCM installation. description : Description of the Compliance object, which will be displayed in the MCM Dashboard. spec : runtime-rules : This is a list of Policy objects that will be applied to the clusters based on the PlacementPolicy (explained in the next section). Once again, the Compliance object is responsible for applying the specified Policy objects to the matching clusters and reporting status back to the Compliance Manager dashboard in MCM.","title":"Compliance"},{"location":"mcm-compliance/#placement-policy","text":"Now that you understand the functionality of a Compliance object, it's time to decide what clusters to apply this Compliance to. For that, we need to create the following Custom Resources: A Placement Policy resource, where, via cluster labels, you define the rules for selecting the clusters to apply the Compliance object to. A Placement Binding resource, where you bind a specific Placement Policy object to a specific Compliance object, which is the final step that tells MCM to start applying the Compliance object to the selected clusters. We will go over Placement Binding in more detail in the next section. To better understand a PlacementPolicy , let's look an actual YAML file for a Placement Policy object: apiVersion: mcm.ibm.com/v1alpha1 kind: PlacementPolicy metadata: name: placement-policy-all-icp namespace: mcm spec: clusterLabels: matchLabels: vendor: ICP Here is a breakdown of the fields above: apiVersion : The path where the custom API is defined. kind : PlacementPolicy in this case. spec : clusterLabels : matchLabels : Here is where we define the labels to select the clusters where the Compliance object will be applied to. In this case we used the label vendor with a value of ICP , which means to apply the Compliance object to all ICP clusters registered with MCM Controller. You can use a combination of the following filters to select clusters: matchNames : List of cluster names to match against. matchLabels : List of cluster labels to match against. These are the labels which fields you for a cluster during Klusterlet installation. matchExpressions : List of expressions to match against. matchConditions : List of cluster conditions to match against, usually involving cluster health status.","title":"Placement Policy"},{"location":"mcm-compliance/#placement-binding","text":"Now that we have both a Compliance object (with all its rules and templates) and a Placement Policy object, a Placement Binding resource is required to bind them together and start the process of applying the Compliance policies to the clusters based on the Placement Policy criteria. To better understand a PlacementBinding , let's look an actual YAML file for a Placement Binding object: apiVersion: mcm.ibm.com/v1alpha1 kind: PlacementBinding metadata: name: placement-binding-all-icp namespace: mcm placementRef: name: placement-policy-all-icp subjects: - name: compliance-all-icp kind: Compliance Here is a breakdown of the fields above: apiVersion : The path where the custom API is defined. kind : PlacementBinding in this case. placementRef : name : The name of the Placement Policy to bind. subjects : The list of objects to bind the Placement Policy with. name : The name of the object to bind the Placement Policy with. kind : The type of object (Compliance in our case) that it's going to be bound with the Placement Policy. We have now introduced and explained all of the resources that are required to create and enforce Compliance Policies on Kubernetes Clusters. Now let's move on to a quick tutorial.","title":"Placement Binding"},{"location":"mcm-compliance/#compliance-manager-tutorial","text":"Now that we know the basic concepts and understand on a high level the components that power the Compliance Manager, let's go ahead and apply the above Compliance, Placement Policy, and Placement Binding objects to our clusters and see what that looks like on the Compliance Manager dashboard.","title":"Compliance Manager Tutorial"},{"location":"mcm-compliance/#pre-requisites","text":"This tutorial will assume you have the following pre-requisites: 2 IBM Cloud Private 3.1 Clusters loaded with the MCM 3.1.2 PPA loaded Cluster 1 Setup : Install MCM Controller Chart Create a separate namespace called mcm , which will be used to store the Compliance objects. Install MCM Klusterlet Chart and enter the following values: Name : se-stg-31 Namespace : mcm-se-stg Labels : cloud : IBM datacenter : dallas environment : Staging owner : case region : US vendor : ICP Cluster 2 Setup : Install MCM Klusterlet Chart and enter the following values: Name : se-dev-31 Namespace : mcm-se-dev Labels : cloud : IBM datacenter : austin environment : Dev owner : case region : US vendor : ICP","title":"Pre-requisites"},{"location":"mcm-compliance/#1-apply-the-compliance","text":"Now that we have installed MCM in both clusters, let's create the Compliance , Placement Policy , and Placement Binding objects located at demos/compliance . Here is a high level breakdown of the contents of the Compliance object: A Policy that enforces the creation of the policy's RoleTemplate if the cluster is found non-compliant. This means that even if the cluster was found compliant and, for some reason, a cluster admin deletes the role, the Compliance Manager will recreate it. Let's proceed with the Compliance object creation by running the following commands: # Login against MCM Hub Cluster cloudctl login -a https://${MCM_HUB_CLUSTER_MASTER_IP}:8443 -u ${USERNAME} -p ${PASSWORD} -n mcm --skip-ssl-validation; # Clone the Reference Architecture Repository git clone git@github.com:ibm-cloud-architecture/kubernetes-multicloud-management.git # Go to the project's folder cd kubernetes-multicloud-management # Apply the Compliance kubectl apply -f demos/compliance Where: ${MCM_HUB_CLUSTER_MASTER_IP} is the IP Address of the master node in the MCM Hub Cluster. ${USERNAME} is admin user. ${PASSWORD} is the admin password. mcm is the namespace in which the Compliance object will be created. In the next section, we will use the MCM Compliance Manager dashboard to see the compliance results of the above Compliance object.","title":"1. Apply the Compliance"},{"location":"mcm-compliance/#2-view-compliance-status-on-the-dashboard","text":"To access the Policies view for the Compliance Manager, open a new browser window and go to https://${MCM_HUB_CLUSTER_MASTER_IP}:8443/multicloud/policies , where ${MCM_HUB_CLUSTER_MASTER_IP} is the IP Address of the master node in the MCM Hub Cluster. If the Compliance object was created successfully, you will see a screen that looks like this: Now click on compliance-all-icp to see more details on the Compliance object. The first thing you will see are some high level details for the Compliance , which are similar to what we saw in the previous screenshot: If you scroll down, you will see the Compliance YAML Template section that shows the actual YAML file that was used to create the Compliance, which you can also edit directly: If you scroll further down you can see the Compliance Status and Compliance Policies sections: Here is a breakdown of the above 2 sections: Compliance Status : The overall compliance status of an individual cluster. Cluster Name : The name of the cluster. Policy Compliant : Ratio of compliant policies to total number of policies. Policy Valid : Ratio of policies with valid syntax to total number of policies. Compliance Policies : A breakdown of all the policies and their compliance status for each cluster . Compliant : Whether the given policy is compliant. Name : Name of the policy. Cluster Compliant : List of compliant clusters. Cluster Not Compliant : List of non-compliant clusters. If you scroll further down you can see the following Placement Policies and Placement Bindings sections: Here is a breakdown of the above 2 sections: Placement Policies : The overall compliance status of an individual cluster. Name : The name of the Placement Policy. Namespace : The namespace ( mcm ) in the MCM Hub cluster where this Placement Policy resource is stored. Replicas : This is the number of application instances to be installed in the selected clusters. This field is only relevant when deploying actual MCM Applications, which we explained in the MCM Applications chapter. Cluster Selector : The cluster labels that were used to select the clusters. Resource Selector : The resource selectors (CPU, RAM, etc) that were used to select the clusters, which is none in our case. Decisions : The clusters that MCM decided to apply the Compliance object to based on the selectors. Placement Bindings : A breakdown of all the policies and their compliance status for each cluster . Name : The name of the Placement Binding. Namespace : The namespace ( mcm ) in the MCM Hub cluster where this Placement Binding resource is stored. Placement Policy : The name of the Placement Policy to be bound. Subjects : The name of the objects (and their types) that the Placement Policy will be bound to, which is the Compliance object. The above should give you a high level idea of the compliance status for each policy in each cluster.","title":"2. View Compliance Status on the Dashboard"},{"location":"mcm-compliance/#3-view-individual-policy","text":"The above section showed us how to get a high level compliance status for each policy on each cluster. If we want to investigate, for example, what exactly is causing a policy to show a non-compliant status, then we have to inspect the contents of the policy itself. To view the contents of any policy, click the name of the policy in the Compliance Policies section shown in the last screenshot. For this example, go ahead and click on the policy-all-icp policy to examine its contents. You should now be greeted with the screen above, which gives you the high level policy details that were explained in the The Basics section. If you scroll down, you will see the Policy YAML Template section that shows the actual YAML file that was used to create the Policy, which you can also edit directly. In here you will also see the Role Template , with the API Groups and Verbs that it should and shouldn't have: If you scroll further down, you will encounter the Role Templates section: Here is a breakdown of the above section: Template Details : Name : Name of the template, which is operator-role in this case. Compliance Type : This field determines whether the resource ( Role in this case) that matches the given rules must exist or not, which in this case must exist based on its musthave value. API version : The API version that was used to create the template.","title":"3. View Individual Policy"},{"location":"mcm-compliance/#conclusion","text":"The more you use the Compliance Manager and implement Compliance objects into more clusters, the easier it becomes to maintain increasingly complicated configurations across multiple clusters. At this point, it is up to you to use the skills you just learned and implement Compliance policies that best suit your needs.","title":"Conclusion"},{"location":"mcm-devops/","text":"DevOps in Multi-cluster Environment Author: Fabio Gomez (fabiogomez@us.ibm.com) This section focuses on using Jenkins and MCM command line tool mcmctl to run CI/CD Pipelines that deploy applications on multiple clusters. Introduction Provisioning and maintaining multiple ICP clusters can be a challenge on its own. On top of that, deploying applications to multiple environments with a CICD pipeline requires highly detailed coordination. The more environments you have, managing Kubernetes service account credentials and their expiration date becomes cumbersome and time-consuming. Now imagine that an application is made of 50 microservices and they all have to be deployed on each cluster. If you have 3 clusters, that's 150 pipelines that have to have the right environment credentials. What happens if the certificate for one environment expires? That means you have to update the certificate in at least 50 pipelines! Even though it is a simple change, in a world of CICD where there is constantly new application updates, you just cannot afford to waste time with such trivial tasks. It would be great to have a tool that can manage certificates for you. Luckily, MCM can do just that for you. In MCM, you can securely provide and store your clusters endpoints and certificates. If the certificates expire, MCM will renew them for you automatically. MCM takes advantage of its certificate management in another feature, which is the ability to deploy a helm chart to multiple clusters with a single helm install command. MCM accomplishes this, primarily, by allowing the developer to specify in the chart itself what clusters to deploy the chart to by using selector labels. When a user/pipeline performs a helm install against the MCM cluster, MCM uses these labels to find the clusters that it will then deploy the chart to. Using labels allows you to implement, for example, a code promotion pipeline that does the following: Deploy a helm chart to all clusters with a label of type environment with a value of dev. Run smoke tests against the dev clusters. If tests succeed, deploy the helm chart to all clusters with a label of type environment with a value of staging. Note that nowhere in the above pipeline example there is a mention of cluster certificates. Granted, you still need to provide cluster certificates for the MCM cluster that does the deployments, but that's a much easier job on the developer compared to the scenarios above. Pre-Requisites For this tutorial, your main pre-requisite is to have 2 IBM Cloud Private clusters setup. The master nodes for each clusters should be routable for MCM to be able to manage them. In the following sections, you will learn how to setup MCM to manage these 2 clusters. 1 x NFS server. Use these instructions to setup a NFS server. Make sure to create and expose an NFS Shared Directory, which will be used by Jenkins to persist build data. Kubectl (Kubernetes CLI) Follow the instructions here to install it on your platform. ICP Helm (Kubernetes package manager) Follow the instructions here to install it on your platform. IBM Cloud Private CLI Follow the instructions here to install it on your platform. 1. MCM Controller and Klusterlet Cluster Preparation The first cluster will be the MCM Controller , which means that this cluster will be able to manage other clusters, itself included. In order for this cluster to become the MCM Controller and manage itself, we will need to install both the MCM Controller and the MCM Klusterlet Helm Charts. The MCM Controller is in charge of monitoring and send commands to all clusters. The MCM Klusterlet is responsible for reporting status back to the MCM Controller and implementing its instructions. Follow these instructions to install the MCM Controller and the MCM Klusterlet. In the Klusterlet section, for Cluster Name field enter se-dev-31 . Make sure to use these labels and values for this cluster: cloud : IBM datacenter : austin environment : Dev owner : case region : US vendor : ICP If the above was done correctly, you have successfully setup the first cluster and can now manage it through MCM Controller . 2. MCM Klusterlet Cluster Preparation The second cluster will only contain the MCM Klusterlet that reports information back to the MCM Controller cluster. Follow these instructions to install the MCM Klusterlet. For Cluster Name field, enter se-stg-31 . Make sure to use these labels and values for this cluster: cloud : IBM datacenter : dallas environment : Staging owner : case region : US vendor : ICP If the above was done correctly, you have successfully setup the first cluster and can now manage it through the MCM Controller in the first cluster. 3. Create Image Policies on Both Clusters Since ICP version 3.1, you are required to create Image Policies that allow you to pull Docker images from specific Docker registries ( gcr.io in our case). To do so, let's start by cloning the project's git repository: # Clone the Reference Architecture Repository git clone git@github.com:ibm-cloud-architecture/kubernetes-multicloud-management.git # Go to the project's folder cd kubernetes-multicloud-management Now, on EACH ICP CLUSTER , let's run the following commands: # Login to the ICP Cluster cloudctl login -a https://ICP_MASTER_IP:8443 -n default --skip-ssl-validation # Create the Image Policy in the ICP Cluster kubectl apply -f demos/guestbook/guestbook-cluster-image-policy.yaml Don't forget to run the above commands on EACH ICP Cluster so that there are no issues when deploying the guestbook application. Jenkins MCM CI/CD Tutorial Here is the High Level Jenkins-MCM DevOps Architecture Diagram and CI/CD Workflow: The main things to notice are the following: A Jenkins instance will be deployed on a IBM Cloud Private (Kubernets) cluster. Jenkins will use mcmctl to deploy applications through the MCM Controller. MCM Controller will use Placement Policies to decide what clusters to deploy the application to. Not all available clusters will be selected to deploy the application to, as shown by the missing arrow to the Prod Cluster This guide will help install the following resources: 1 x 10Gi Persistent Volume Claim (PVC) to Store Jenkins data and builds' information. Be sure that your Kubernetes Cluster can support PVCs size of at least 10Gi 1 x Jenkins Master Kubernetes Pod with Kubernetes Plugin Installed. 1 x Kubernetes Service for above Jenkins Master Pod with port 8080 exposed to a LoadBalancer. Ingress to expose Jenkins service (HTTP/8080), externally (HTTPS/443) All using Kubernetes Resources. 1. Install Jenkins Chart We are now going to start the process of installing the Community Jenkins Helm Chart . Feel free to pick any of the ICP clusters for the remaining of this guide to setup Jenkins, but make sure you run all the following commands against ONLY ONE CLUSTER . Let's start by going to the Jenkins folder, followed by a cloudctl login : # Go to the Jenkins folder cd demos/jenkins/deploy # Login against the ICP Cluster that will contain the Jenkins instance cloudctl login -a https://ICP_MASTER_IP:8443 -n default --skip-ssl-validation Now we are ready to start setting up the Jenkins instance. a. Create a new namespace Create a new namespace mcm-devops-demo . This namespace will be used for CI/CD setup purposes. User may require 'cluster admin' access to create a new namespace. if you encounter any issues creating namespace, reach out to ICP admin for the same. # Create mcm-devops-demo namespace kubectl create namespace mcm-devops-demo b. Create Jenkins Image Policy Since we will be deploying the Jenkins Helm chart, we will need to create a Cluster Image Policy that allows the cluster to pull the Jenkins and the custom Jenkins Slave images from Docker Hub. To do so, run the following command: # Create the Image Policy in the Jenkins ICP Cluster kubectl apply -f jenkins-cluster-image-policy.yaml c. Create a Persistence Volume Claim Create Persistence Volume (PV) and a Persistence Volume Claim (PVC) using the NFS Shared Directory you created earlier. To create the PV and the PVC, open the jenkins-pvc.yaml and change the values of Lines 16 and 17 to the NFS server's IP Address and the Share Directory's absolute path, respectively. Then save the file and create the PV and PVC with the following command: # Create the PV and PVC kubectl apply -f jenkins-pvc.yaml Wait a few seconds/minutes for the PVC to be bound and ready. To verify that the PVC has been successfully bound, run the following command: # Check that PVC status kubectl -n mcm-devops-demo get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE jenkins-master-claim Bound jenkins-master 10Gi RWO 1d If you see the above PVC with a Status of Bound , that means that your PVC is ready to be used! d. Install the Jenkins Helm Chart Let's finally install the Community Jenkins Helm Chart . We will be using our own values.yaml file to setup things like admin password, a /jenkins prefix (which will be used by ingress), and to use the PVC we created in the above section. To install the chart, run the following command: # Install the Jenkins Helm Chart helm upgrade --namespace mcm-devops-demo --install jenkins -f jenkins-values.yaml stable/jenkins --tls The Jenkins container will take a few minutes to start and be fully initialized. To access the Jenkins instance once it's ready, you will need to create an Ingress entry as shown in the next section. e. Create Jenkins Ingress To be able to access the Jenkins instance, you will need an Ingress record that forwards traffic from the Ingress IP address to the Jenkins pod. To create the Ingress, run the following command: kubectl apply -f jenkins-ingress.yaml To get the Ingress IP address, run the following command: kubectl get ingress -n mcm-devops-demo NAME HOSTS ADDRESS PORTS AGE jenkins-ingress * 172.16.50.228 80 1d If you see the jenkins-ingress listed, the IP Address will be listed under the ADDRESS column. f. Login to Jenkins Finally, to let's test that the Ingress was created successfully by trying to access Jenkins from your browser. Open a new browser window and go to http://ICP_INGRESS_IP/jenkins . You should be greeted with a login window Now enter admin and admin_0000 (as defined in the jenkins-values.yaml file) for username and password, respectively and click the Sign In button. If the login worked, then that means that you have successfully installed Jenkins on your ICP cluster! NOTE : Depending on what version of Jenkins gets installed from the Helm Chart, you may be asked to upgrade some of the Jenkins plugins that were installed with the chart, which is beyond the scope of this document, to ensure that the pipelines work flawlessly. Just know that if you get an error that doesn't make sense while running your pipelines, it may be because your Jenkins plugins need an upgrade. 2. Configure Jenkins The Jenkins configuration will be default. However, we need to create a Credentials entry for the username and password for the MCM Controller ICP cluster. To create the credentials, open a browser window and go to http://ICP_INGRESS_IP/jenkins/credentials/store/system/domain/_/newCredentials and select Username with password for Kind . Enter the admin username and password for the ICP cluster. Please make that you enter id-sedev-cluster-credentials for the ID field, as this is the Jenkins credentials id that will be used by the pipeline. Depending on whether you are accessing this repository from your own fork or from our repository directly, you may be asked to setup a private ssh key for your GitHub username. To do so, open a browser window and go to http://ICP_INGRESS_IP/jenkins/credentials/store/system/domain/_/newCredentials and select SSH Username with private key for Kind . Now enter your GitHub username and your Private SSH key in the Username and Private Key fields, respectively. Now we are ready to start setting up the CI/CD Pipeline. 3. Create 2 CI/CD Pipelines Now we are going to create 2 pipeline projects. The first pipeline project will be called mcm-dev and, as the name implies, will be used to deploy the guestbook app to the se-dev-31 cluster. The second pipeline will be called mcm-promotion and, as the name implies, it will be used to promote the guestbook deployment from the se-dev-31 to the se-stg-31 cluster. a. Pipeline Code Overview The pipeline code (which you can checkout here ) consists of the following 4 stages: Initialize CLIs In this stage, the pipeline will log against the MCM Controller cluster and initialize contexts for kubectl , helm , and mcmctl CLIs. Deploy to X As the name implies, this is the stage in which the guestbook application will be deployed (via helm ) to the specified environment. Because we are logged against the MCM Controller cluster, the MCM Controller will look at the gbapp chart and figure out, based on the cluster selector labels and the specified number of replicas, what clusters and how many clusters to deploy the application to. Validation Test In this stage, as the name implies, we will be performing a validation test to confirm that the guestbook application was deployed to the correct cluster. Also, a simple functional test will be done to make sure that we get the web application's HTML code. The validation test itself is done as follows: Use mcmctl with cluster specific selectors to verify that the guestbook deployments exist in the cluster. Wait until the guestbook deployments show up in the cluster. Use mcmctl to obtain the available replicas for each deployment. Wait until there is at least 1 available replica. Use mcmctl to get the IP Address (or domain name) of the cluster's Ingress for the geustbook frontend service. Finally, use curl to query the guestbook frontend service using the Ingress from above and check that we get the guestbook frontend HTML code. Trigger Promotion Job Assuming that we provided the name of the promotion job and that the pipeline passes validation tests, this step will trigger the mcm-promotion pipeline, which runs the same stages minus the Trigger Promotion Job stage. The mcm-promotion pipeline uses the same pipeline code (with different cluster selector labels and a replica count of 2) to deploy the guestbook app to both se-dev-31 and se-stg-31 clusters. In this case, since the guestbook application was already deployed to the se-dev-31 cluster, MCM will simply keep that deployment as is and deploy the guestbook application to the se-stg-31 cluster, thus satyisfying the requirements. b. Creating the mcm-dev pipeline Now that we understand the pipeline code at a high level, we can proceed with creating the pipelines, which is the easier part. Let's start by creating the mcm-dev pipeline with the following steps: From the Jenkins Home page, click on Jenkins- New Item . Enter mcm-dev for the Item Name . Select Pipeline as project type. Click the OK Button. This will bring you to the Pipeline Configuration view. Now that we have created the pipeline, in the following sections you will setup the pipeline environment variables and the git repository details. i. Setup the Environment Variables Let's setup the pipeline environment variables, which contain the cluster specific information needed to deploy the guestbook application into the se-dev-31 cluster. We will explain all of the variables that you need to enter, which result in a configuration similar to the following On the General section of the Pipeline Configuration view, check the This project is parameterized box, which will expose the Add Parameter dropdown. Then click Add Parameter - String Parameter and create the following parameters with their respective Name and Default Value : CLUSTER_NAMESPACE : default This is the cluster namespace on which the guestbook application will be deployed to. CLUSTER_ACCOUNT_ID : id-se-dev-31-cluster-account This is the IBM Cloud Private account ID that cloudctl will log in against. Assuming that you named this cluster se-dev-31 then the above ID will apply. If you created your cluster under a different name, then it might show up as id-CLUSTER-NAME-cluster-account . To find out the exact id name of the account, perform a cloudctl login and when you get to the account selection you will see the account ID between parenthesis. CLUSTER_CREDENTIAL_ID : id-sedev-cluster-credentials This is the Jenkins credential id for the ICP cluster's username and password that we created in a previous step. CLUSTER_URL : https://MCM_CONTROLLER_CLUSTER_MASTER_IP:8443 The URL for the MCM Controller cluster, which uses the IP Address of the master node. HELM_RELEASE_NAME : guestbook The helm release name of the guestbook application. REPLICA_COUNT : 1 The replica count is the number of matching clusters that the guestbook application will be deployed into. Since this is the mcm-dev pipeline, we will set this to just 1 . PIPELINE_IMAGE : ibmcase/kube-helm-cloudctl-mcmctl:3.1.2 The docker image that contains all of the CLIs needed by the pipeline, which are cloudctl , mcmctl , kubectl , and helm . This image is publicly available on our Docker Hub and the source can be found here . NOTE: We built this image for demo purposes, which means that it should NOT be used for PRODUCTION. For that, you will have to create your own Docker image with the specific versions of the CLIs that you wish to use. LABEL_ENVIRONMENT : Dev This is the Environment cluster selector label that we will use to select the se-dev-31 cluster to deploy the guestbook application. Notice that we ser the value to Dev to match the cluster selector that was assigned to se-dev-31 when we installed the Klusterlet earlier. TEST_ENVIRONMENT : Dev This variable is used to determine what cluster/environment to perform validation tests against. It may look as unnecessary to have this label since we have LABEL_ENVIRONMENT set to Dev , but you will notice in the next section when we deploy the guestbook application against multiple clusters that we will have to choose one environment to perform the validation test against. PROMOTION_JOB : mcm-promotion When this variable is set and the pipeline successfully passed the validation tests, the pipeline will automatically trigger then mcm-promotion pipeline, which will deploy the guestbook application to the se-stg-31 cluster along with se-dev-31 . You are done setting up all the environment variables. ii. Setup the git repository Now we need to setup the Pipeline git repository details, which will tell Jenkins the git repository and the location where the Jenkinsfile is located. We will explain all of the settings that you need to enter, which result in a configuration similar to the following: On the Pipeline section of the Pipeline Configuration view do the following: Select Pipeline script from SCM on the Definition field. Select Git on the SCM field Enter https://github.com/ibm-cloud-architecture/kubernetes-multicloud-management in the Repositories - Repository URL field. If using SSH to clone the repository, enter git@github.com:ibm-cloud-architecture/kubernetes-multicloud-management.git instead. Also, if using SSH , in the Credentials field select the SSH Credentials that you created earlier. Enter */master in the Branches to build - Branch Specifier field. Enter demos/jenkins/Jenkinsfile in the Script Path field. iii. Save the pipeline configuration Now that you have configured pipeline, go ahead and click the Save button to save your the pipeline configuration. c. Creating the mcm-promotion pipeline Now let's move on to creating the mcm-promotion pipeline. Luckily for you, creating this pipeline will be much easier since we can just copy the mcm-dev pipeline and change some environment variables, which saves us time. To create the mcm-promotion pipeline, do the following: From the Jenkins Home page, click on Jenkins- New Item . Enter mcm-promotion for the Item Name . Enter mcm-dev in the Copy from towards the bottom. Click the OK Button. This will bring you to the Pipeline Configuration view with pre-filled Environment Variables and Repository details from the mcm-dev pipeline. Now go to the General section and enter the following values for the environment variables below: CLUSTER_NAMESPACE : default CLUSTER_ACCOUNT_ID : id-se-dev-31-cluster-account CLUSTER_CREDENTIAL_ID : id-sedev-cluster-credentials CLUSTER_URL : https://MCM_CONTROLLER_CLUSTER_MASTER_IP:8443 HELM_RELEASE_NAME : guestbook REPLICA_COUNT : 2 Since we want to deploy the application to both se-dev-31 and se-stg-31 clusters, we set the replica count to 2 . PIPELINE_IMAGE : ibmcase/kube-helm-cloudctl-mcmctl:3.1.2 Rename LABEL_ENVIRONMENT to LABEL_OWNER and set the value to case Since both se-dev-31 and se-stg-31 share the Owner cluster selector label with a value of case , we can use this label to select both cluster as deployment candidates for the guestbook application. TEST_ENVIRONMENT : Staging Since the goal to this pipeline is to promote/deploy the guestbook application to se-stg-31 cluster, we will set the test environment as Staging . Remove the PROMOTION_JOB variable or set its value to an empty string. Since this pipeline is the promotion pipeline, there is no need to automatically trigger another promotion pipeline. To promote an application to a production environment, it is best practice for a human to manually trigger the pipeline, watch its progress, and correct any issues that it may come accross. Now that you have configured pipeline, go ahead and click the Save button to save your the pipeline configuration. 4. Running the Pipelines Whew, setting up those pipelines took a lot of work but now comes the rewarding part. We are finally going to run the pipelines! To run the mcm-dev pipeline, do the following: From the Jenkins home page select the mcm-dev item, then click Build with parameters , which will take you to a view similar to the following, where you can edit the Environment Variables, if needed: Click Build to start the pipeline, which will take you back to the project view. From the mcm-dev project view, click on the latest build number (in the above picture it would be #7) to view build details. To access the build logs, click on the Console Output button. I'll let you look into the logs to figure out what's going on. The best way to read the logs is by matching the output you see with the pipeline stages we went through earlier in the Pipeline Code Overview section. An important detail to know is that towards the end of the pipeline output, if the Validation Test stage completed successfully, you will notice that the pipeline triggers the mcm-promotion job and starts a new build for it, as shown below: If you would like to see output of the mcm-promotion job build, just click the mcm-promotion build number (#2 in the above picture). The way Jenkins treats jobs that trigger builds for other jobs is that it will wait until the triggered jobs finish before finishing the current job. In our case, the mcm-dev job build will wait until mcm-promotion job build finishes before it can finish itself. Finally, you will know that the pipeline finished successfully if you see Finished: SUCCESS as the last line of console output. If that's the case, then CONGRATULATIONS!!! You have successfully run an MCM CI/CD pipeline that automatically deploys the guestbook application to the se-dev-31 cluster but also, upon passing the validation tests, triggers the mcm-promotion pipeline to deploy the application to the se-stg-31 cluster. Now let's move on to manually testing the guestbook application on each cluster to verify that the pipelines indeed deployed the application successfully. 5. Test the Guestbook Application There are 2 ways in which we can access the guestbook application's frontend on each cluster: Getting the application URL from the Jenkins build output. From the Deployments page in the IBM Cloud Private console of each cluster. a. Getting the application URL from the Jenkins build output On the Validation Test stage you will see a link to the guestbook application's frontend URL in the logs as shown below: To access the guestbook application just click on the URL and you will be taken to the web application in your browser. This step applies to both mcm-dev and mcm-promotion jobs. b. Launching the application from the IBM Cloud Private Console You can also launch the guestbook frontend from the IBM Cloud Private console. To do so, open a new browser window and go to https://ICP_MASTER_IP:8443/console/workloads/deployments and enter guestbook in the search bar. If done correctly, you will see a view like the following: If you are able to see the md-guestbook-gbapp , md-guestbook-gbapp-redismaster , and md-guestbook-gbapp-redisslave deployments and all have an Available value of 1 , then this means that all of the deployments have succesfully been deployed and started. At this point, all it takes to launch the application is to click the Launch button in the md-guestbook-gbapp row. This step applies to both se-dev-31 and se-stg-31 clusters. c. Testing the Application The guestbook application itself is very simple. It consists of a web application that saves guest names (or any text) to a Redis deployment and persists them there even if the web application dies or restarts for some reason. To test its functionality, enter any text in the textbox shown below and click the Submit button. If everything worked successfully, you will see that the text you entered has now moved below the Submit button, which indicates that the text has been saved to and successfully read from the Redis deployment. To make sure that the text persists in the Redis deployment, feel free to refresh the page and make sure that the text you entered is bein shown again below the Submit button. If all the above was done successfully, that means that you have successfully verified the guestbook deployment and tested its functionality! Conclusion An automated CI/CD pipeline is the holy grail of a true DevOps setup. It enforces code quality by doing things like Validation Tests. It also saves many headaches when deploying new code to new environments because everything is automated. The problem with the traditional CI/CD approach is that the more environments you have (which is common in big enterprises), the more difficult and error prone it becomes to set up multiple CI/CD pipelines because you have to take many configuration settings and credentials into account for each environment. Thankfully, MCM takes care of saving all of the environment settings and credentials for you and lets you deploy applications to these environments by supplying values for easy-to-read cluster selector labels, as you saw in the examples above. The only thing that the developer has to remember is the login information for the MCM Controller cluster and the values for the cluster selector labels. Now that you have the knowledge of using MCM in your CI/CD pipelines, I encourage you to try this out with your own workloads!","title":"DevOps in Multi-cluster Environment"},{"location":"mcm-devops/#devops-in-multi-cluster-environment","text":"Author: Fabio Gomez (fabiogomez@us.ibm.com) This section focuses on using Jenkins and MCM command line tool mcmctl to run CI/CD Pipelines that deploy applications on multiple clusters.","title":"DevOps in Multi-cluster Environment"},{"location":"mcm-devops/#introduction","text":"Provisioning and maintaining multiple ICP clusters can be a challenge on its own. On top of that, deploying applications to multiple environments with a CICD pipeline requires highly detailed coordination. The more environments you have, managing Kubernetes service account credentials and their expiration date becomes cumbersome and time-consuming. Now imagine that an application is made of 50 microservices and they all have to be deployed on each cluster. If you have 3 clusters, that's 150 pipelines that have to have the right environment credentials. What happens if the certificate for one environment expires? That means you have to update the certificate in at least 50 pipelines! Even though it is a simple change, in a world of CICD where there is constantly new application updates, you just cannot afford to waste time with such trivial tasks. It would be great to have a tool that can manage certificates for you. Luckily, MCM can do just that for you. In MCM, you can securely provide and store your clusters endpoints and certificates. If the certificates expire, MCM will renew them for you automatically. MCM takes advantage of its certificate management in another feature, which is the ability to deploy a helm chart to multiple clusters with a single helm install command. MCM accomplishes this, primarily, by allowing the developer to specify in the chart itself what clusters to deploy the chart to by using selector labels. When a user/pipeline performs a helm install against the MCM cluster, MCM uses these labels to find the clusters that it will then deploy the chart to. Using labels allows you to implement, for example, a code promotion pipeline that does the following: Deploy a helm chart to all clusters with a label of type environment with a value of dev. Run smoke tests against the dev clusters. If tests succeed, deploy the helm chart to all clusters with a label of type environment with a value of staging. Note that nowhere in the above pipeline example there is a mention of cluster certificates. Granted, you still need to provide cluster certificates for the MCM cluster that does the deployments, but that's a much easier job on the developer compared to the scenarios above.","title":"Introduction"},{"location":"mcm-devops/#pre-requisites","text":"For this tutorial, your main pre-requisite is to have 2 IBM Cloud Private clusters setup. The master nodes for each clusters should be routable for MCM to be able to manage them. In the following sections, you will learn how to setup MCM to manage these 2 clusters. 1 x NFS server. Use these instructions to setup a NFS server. Make sure to create and expose an NFS Shared Directory, which will be used by Jenkins to persist build data. Kubectl (Kubernetes CLI) Follow the instructions here to install it on your platform. ICP Helm (Kubernetes package manager) Follow the instructions here to install it on your platform. IBM Cloud Private CLI Follow the instructions here to install it on your platform.","title":"Pre-Requisites"},{"location":"mcm-devops/#1-mcm-controller-and-klusterlet-cluster-preparation","text":"The first cluster will be the MCM Controller , which means that this cluster will be able to manage other clusters, itself included. In order for this cluster to become the MCM Controller and manage itself, we will need to install both the MCM Controller and the MCM Klusterlet Helm Charts. The MCM Controller is in charge of monitoring and send commands to all clusters. The MCM Klusterlet is responsible for reporting status back to the MCM Controller and implementing its instructions. Follow these instructions to install the MCM Controller and the MCM Klusterlet. In the Klusterlet section, for Cluster Name field enter se-dev-31 . Make sure to use these labels and values for this cluster: cloud : IBM datacenter : austin environment : Dev owner : case region : US vendor : ICP If the above was done correctly, you have successfully setup the first cluster and can now manage it through MCM Controller .","title":"1. MCM Controller and Klusterlet Cluster Preparation"},{"location":"mcm-devops/#2-mcm-klusterlet-cluster-preparation","text":"The second cluster will only contain the MCM Klusterlet that reports information back to the MCM Controller cluster. Follow these instructions to install the MCM Klusterlet. For Cluster Name field, enter se-stg-31 . Make sure to use these labels and values for this cluster: cloud : IBM datacenter : dallas environment : Staging owner : case region : US vendor : ICP If the above was done correctly, you have successfully setup the first cluster and can now manage it through the MCM Controller in the first cluster.","title":"2. MCM Klusterlet Cluster Preparation"},{"location":"mcm-devops/#3-create-image-policies-on-both-clusters","text":"Since ICP version 3.1, you are required to create Image Policies that allow you to pull Docker images from specific Docker registries ( gcr.io in our case). To do so, let's start by cloning the project's git repository: # Clone the Reference Architecture Repository git clone git@github.com:ibm-cloud-architecture/kubernetes-multicloud-management.git # Go to the project's folder cd kubernetes-multicloud-management Now, on EACH ICP CLUSTER , let's run the following commands: # Login to the ICP Cluster cloudctl login -a https://ICP_MASTER_IP:8443 -n default --skip-ssl-validation # Create the Image Policy in the ICP Cluster kubectl apply -f demos/guestbook/guestbook-cluster-image-policy.yaml Don't forget to run the above commands on EACH ICP Cluster so that there are no issues when deploying the guestbook application.","title":"3. Create Image Policies on Both Clusters"},{"location":"mcm-devops/#jenkins-mcm-cicd-tutorial","text":"Here is the High Level Jenkins-MCM DevOps Architecture Diagram and CI/CD Workflow: The main things to notice are the following: A Jenkins instance will be deployed on a IBM Cloud Private (Kubernets) cluster. Jenkins will use mcmctl to deploy applications through the MCM Controller. MCM Controller will use Placement Policies to decide what clusters to deploy the application to. Not all available clusters will be selected to deploy the application to, as shown by the missing arrow to the Prod Cluster","title":"Jenkins MCM CI/CD Tutorial"},{"location":"mcm-devops/#this-guide-will-help-install-the-following-resources","text":"1 x 10Gi Persistent Volume Claim (PVC) to Store Jenkins data and builds' information. Be sure that your Kubernetes Cluster can support PVCs size of at least 10Gi 1 x Jenkins Master Kubernetes Pod with Kubernetes Plugin Installed. 1 x Kubernetes Service for above Jenkins Master Pod with port 8080 exposed to a LoadBalancer. Ingress to expose Jenkins service (HTTP/8080), externally (HTTPS/443) All using Kubernetes Resources.","title":"This guide will help install the following resources:"},{"location":"mcm-devops/#1-install-jenkins-chart","text":"We are now going to start the process of installing the Community Jenkins Helm Chart . Feel free to pick any of the ICP clusters for the remaining of this guide to setup Jenkins, but make sure you run all the following commands against ONLY ONE CLUSTER . Let's start by going to the Jenkins folder, followed by a cloudctl login : # Go to the Jenkins folder cd demos/jenkins/deploy # Login against the ICP Cluster that will contain the Jenkins instance cloudctl login -a https://ICP_MASTER_IP:8443 -n default --skip-ssl-validation Now we are ready to start setting up the Jenkins instance.","title":"1. Install Jenkins Chart"},{"location":"mcm-devops/#a-create-a-new-namespace","text":"Create a new namespace mcm-devops-demo . This namespace will be used for CI/CD setup purposes. User may require 'cluster admin' access to create a new namespace. if you encounter any issues creating namespace, reach out to ICP admin for the same. # Create mcm-devops-demo namespace kubectl create namespace mcm-devops-demo","title":"a. Create a new namespace"},{"location":"mcm-devops/#b-create-jenkins-image-policy","text":"Since we will be deploying the Jenkins Helm chart, we will need to create a Cluster Image Policy that allows the cluster to pull the Jenkins and the custom Jenkins Slave images from Docker Hub. To do so, run the following command: # Create the Image Policy in the Jenkins ICP Cluster kubectl apply -f jenkins-cluster-image-policy.yaml","title":"b. Create Jenkins Image Policy"},{"location":"mcm-devops/#c-create-a-persistence-volume-claim","text":"Create Persistence Volume (PV) and a Persistence Volume Claim (PVC) using the NFS Shared Directory you created earlier. To create the PV and the PVC, open the jenkins-pvc.yaml and change the values of Lines 16 and 17 to the NFS server's IP Address and the Share Directory's absolute path, respectively. Then save the file and create the PV and PVC with the following command: # Create the PV and PVC kubectl apply -f jenkins-pvc.yaml Wait a few seconds/minutes for the PVC to be bound and ready. To verify that the PVC has been successfully bound, run the following command: # Check that PVC status kubectl -n mcm-devops-demo get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE jenkins-master-claim Bound jenkins-master 10Gi RWO 1d If you see the above PVC with a Status of Bound , that means that your PVC is ready to be used!","title":"c. Create a Persistence Volume Claim"},{"location":"mcm-devops/#d-install-the-jenkins-helm-chart","text":"Let's finally install the Community Jenkins Helm Chart . We will be using our own values.yaml file to setup things like admin password, a /jenkins prefix (which will be used by ingress), and to use the PVC we created in the above section. To install the chart, run the following command: # Install the Jenkins Helm Chart helm upgrade --namespace mcm-devops-demo --install jenkins -f jenkins-values.yaml stable/jenkins --tls The Jenkins container will take a few minutes to start and be fully initialized. To access the Jenkins instance once it's ready, you will need to create an Ingress entry as shown in the next section.","title":"d. Install the Jenkins Helm Chart"},{"location":"mcm-devops/#e-create-jenkins-ingress","text":"To be able to access the Jenkins instance, you will need an Ingress record that forwards traffic from the Ingress IP address to the Jenkins pod. To create the Ingress, run the following command: kubectl apply -f jenkins-ingress.yaml To get the Ingress IP address, run the following command: kubectl get ingress -n mcm-devops-demo NAME HOSTS ADDRESS PORTS AGE jenkins-ingress * 172.16.50.228 80 1d If you see the jenkins-ingress listed, the IP Address will be listed under the ADDRESS column.","title":"e. Create Jenkins Ingress"},{"location":"mcm-devops/#f-login-to-jenkins","text":"Finally, to let's test that the Ingress was created successfully by trying to access Jenkins from your browser. Open a new browser window and go to http://ICP_INGRESS_IP/jenkins . You should be greeted with a login window Now enter admin and admin_0000 (as defined in the jenkins-values.yaml file) for username and password, respectively and click the Sign In button. If the login worked, then that means that you have successfully installed Jenkins on your ICP cluster! NOTE : Depending on what version of Jenkins gets installed from the Helm Chart, you may be asked to upgrade some of the Jenkins plugins that were installed with the chart, which is beyond the scope of this document, to ensure that the pipelines work flawlessly. Just know that if you get an error that doesn't make sense while running your pipelines, it may be because your Jenkins plugins need an upgrade.","title":"f. Login to Jenkins"},{"location":"mcm-devops/#2-configure-jenkins","text":"The Jenkins configuration will be default. However, we need to create a Credentials entry for the username and password for the MCM Controller ICP cluster. To create the credentials, open a browser window and go to http://ICP_INGRESS_IP/jenkins/credentials/store/system/domain/_/newCredentials and select Username with password for Kind . Enter the admin username and password for the ICP cluster. Please make that you enter id-sedev-cluster-credentials for the ID field, as this is the Jenkins credentials id that will be used by the pipeline. Depending on whether you are accessing this repository from your own fork or from our repository directly, you may be asked to setup a private ssh key for your GitHub username. To do so, open a browser window and go to http://ICP_INGRESS_IP/jenkins/credentials/store/system/domain/_/newCredentials and select SSH Username with private key for Kind . Now enter your GitHub username and your Private SSH key in the Username and Private Key fields, respectively. Now we are ready to start setting up the CI/CD Pipeline.","title":"2. Configure Jenkins"},{"location":"mcm-devops/#3-create-2-cicd-pipelines","text":"Now we are going to create 2 pipeline projects. The first pipeline project will be called mcm-dev and, as the name implies, will be used to deploy the guestbook app to the se-dev-31 cluster. The second pipeline will be called mcm-promotion and, as the name implies, it will be used to promote the guestbook deployment from the se-dev-31 to the se-stg-31 cluster.","title":"3. Create 2 CI/CD Pipelines"},{"location":"mcm-devops/#a-pipeline-code-overview","text":"The pipeline code (which you can checkout here ) consists of the following 4 stages: Initialize CLIs In this stage, the pipeline will log against the MCM Controller cluster and initialize contexts for kubectl , helm , and mcmctl CLIs. Deploy to X As the name implies, this is the stage in which the guestbook application will be deployed (via helm ) to the specified environment. Because we are logged against the MCM Controller cluster, the MCM Controller will look at the gbapp chart and figure out, based on the cluster selector labels and the specified number of replicas, what clusters and how many clusters to deploy the application to. Validation Test In this stage, as the name implies, we will be performing a validation test to confirm that the guestbook application was deployed to the correct cluster. Also, a simple functional test will be done to make sure that we get the web application's HTML code. The validation test itself is done as follows: Use mcmctl with cluster specific selectors to verify that the guestbook deployments exist in the cluster. Wait until the guestbook deployments show up in the cluster. Use mcmctl to obtain the available replicas for each deployment. Wait until there is at least 1 available replica. Use mcmctl to get the IP Address (or domain name) of the cluster's Ingress for the geustbook frontend service. Finally, use curl to query the guestbook frontend service using the Ingress from above and check that we get the guestbook frontend HTML code. Trigger Promotion Job Assuming that we provided the name of the promotion job and that the pipeline passes validation tests, this step will trigger the mcm-promotion pipeline, which runs the same stages minus the Trigger Promotion Job stage. The mcm-promotion pipeline uses the same pipeline code (with different cluster selector labels and a replica count of 2) to deploy the guestbook app to both se-dev-31 and se-stg-31 clusters. In this case, since the guestbook application was already deployed to the se-dev-31 cluster, MCM will simply keep that deployment as is and deploy the guestbook application to the se-stg-31 cluster, thus satyisfying the requirements.","title":"a. Pipeline Code Overview"},{"location":"mcm-devops/#b-creating-the-mcm-dev-pipeline","text":"Now that we understand the pipeline code at a high level, we can proceed with creating the pipelines, which is the easier part. Let's start by creating the mcm-dev pipeline with the following steps: From the Jenkins Home page, click on Jenkins- New Item . Enter mcm-dev for the Item Name . Select Pipeline as project type. Click the OK Button. This will bring you to the Pipeline Configuration view. Now that we have created the pipeline, in the following sections you will setup the pipeline environment variables and the git repository details.","title":"b. Creating the mcm-dev pipeline"},{"location":"mcm-devops/#i-setup-the-environment-variables","text":"Let's setup the pipeline environment variables, which contain the cluster specific information needed to deploy the guestbook application into the se-dev-31 cluster. We will explain all of the variables that you need to enter, which result in a configuration similar to the following On the General section of the Pipeline Configuration view, check the This project is parameterized box, which will expose the Add Parameter dropdown. Then click Add Parameter - String Parameter and create the following parameters with their respective Name and Default Value : CLUSTER_NAMESPACE : default This is the cluster namespace on which the guestbook application will be deployed to. CLUSTER_ACCOUNT_ID : id-se-dev-31-cluster-account This is the IBM Cloud Private account ID that cloudctl will log in against. Assuming that you named this cluster se-dev-31 then the above ID will apply. If you created your cluster under a different name, then it might show up as id-CLUSTER-NAME-cluster-account . To find out the exact id name of the account, perform a cloudctl login and when you get to the account selection you will see the account ID between parenthesis. CLUSTER_CREDENTIAL_ID : id-sedev-cluster-credentials This is the Jenkins credential id for the ICP cluster's username and password that we created in a previous step. CLUSTER_URL : https://MCM_CONTROLLER_CLUSTER_MASTER_IP:8443 The URL for the MCM Controller cluster, which uses the IP Address of the master node. HELM_RELEASE_NAME : guestbook The helm release name of the guestbook application. REPLICA_COUNT : 1 The replica count is the number of matching clusters that the guestbook application will be deployed into. Since this is the mcm-dev pipeline, we will set this to just 1 . PIPELINE_IMAGE : ibmcase/kube-helm-cloudctl-mcmctl:3.1.2 The docker image that contains all of the CLIs needed by the pipeline, which are cloudctl , mcmctl , kubectl , and helm . This image is publicly available on our Docker Hub and the source can be found here . NOTE: We built this image for demo purposes, which means that it should NOT be used for PRODUCTION. For that, you will have to create your own Docker image with the specific versions of the CLIs that you wish to use. LABEL_ENVIRONMENT : Dev This is the Environment cluster selector label that we will use to select the se-dev-31 cluster to deploy the guestbook application. Notice that we ser the value to Dev to match the cluster selector that was assigned to se-dev-31 when we installed the Klusterlet earlier. TEST_ENVIRONMENT : Dev This variable is used to determine what cluster/environment to perform validation tests against. It may look as unnecessary to have this label since we have LABEL_ENVIRONMENT set to Dev , but you will notice in the next section when we deploy the guestbook application against multiple clusters that we will have to choose one environment to perform the validation test against. PROMOTION_JOB : mcm-promotion When this variable is set and the pipeline successfully passed the validation tests, the pipeline will automatically trigger then mcm-promotion pipeline, which will deploy the guestbook application to the se-stg-31 cluster along with se-dev-31 . You are done setting up all the environment variables.","title":"i. Setup the Environment Variables"},{"location":"mcm-devops/#ii-setup-the-git-repository","text":"Now we need to setup the Pipeline git repository details, which will tell Jenkins the git repository and the location where the Jenkinsfile is located. We will explain all of the settings that you need to enter, which result in a configuration similar to the following: On the Pipeline section of the Pipeline Configuration view do the following: Select Pipeline script from SCM on the Definition field. Select Git on the SCM field Enter https://github.com/ibm-cloud-architecture/kubernetes-multicloud-management in the Repositories - Repository URL field. If using SSH to clone the repository, enter git@github.com:ibm-cloud-architecture/kubernetes-multicloud-management.git instead. Also, if using SSH , in the Credentials field select the SSH Credentials that you created earlier. Enter */master in the Branches to build - Branch Specifier field. Enter demos/jenkins/Jenkinsfile in the Script Path field.","title":"ii. Setup the git repository"},{"location":"mcm-devops/#iii-save-the-pipeline-configuration","text":"Now that you have configured pipeline, go ahead and click the Save button to save your the pipeline configuration.","title":"iii. Save the pipeline configuration"},{"location":"mcm-devops/#c-creating-the-mcm-promotion-pipeline","text":"Now let's move on to creating the mcm-promotion pipeline. Luckily for you, creating this pipeline will be much easier since we can just copy the mcm-dev pipeline and change some environment variables, which saves us time. To create the mcm-promotion pipeline, do the following: From the Jenkins Home page, click on Jenkins- New Item . Enter mcm-promotion for the Item Name . Enter mcm-dev in the Copy from towards the bottom. Click the OK Button. This will bring you to the Pipeline Configuration view with pre-filled Environment Variables and Repository details from the mcm-dev pipeline. Now go to the General section and enter the following values for the environment variables below: CLUSTER_NAMESPACE : default CLUSTER_ACCOUNT_ID : id-se-dev-31-cluster-account CLUSTER_CREDENTIAL_ID : id-sedev-cluster-credentials CLUSTER_URL : https://MCM_CONTROLLER_CLUSTER_MASTER_IP:8443 HELM_RELEASE_NAME : guestbook REPLICA_COUNT : 2 Since we want to deploy the application to both se-dev-31 and se-stg-31 clusters, we set the replica count to 2 . PIPELINE_IMAGE : ibmcase/kube-helm-cloudctl-mcmctl:3.1.2 Rename LABEL_ENVIRONMENT to LABEL_OWNER and set the value to case Since both se-dev-31 and se-stg-31 share the Owner cluster selector label with a value of case , we can use this label to select both cluster as deployment candidates for the guestbook application. TEST_ENVIRONMENT : Staging Since the goal to this pipeline is to promote/deploy the guestbook application to se-stg-31 cluster, we will set the test environment as Staging . Remove the PROMOTION_JOB variable or set its value to an empty string. Since this pipeline is the promotion pipeline, there is no need to automatically trigger another promotion pipeline. To promote an application to a production environment, it is best practice for a human to manually trigger the pipeline, watch its progress, and correct any issues that it may come accross. Now that you have configured pipeline, go ahead and click the Save button to save your the pipeline configuration.","title":"c. Creating the mcm-promotion pipeline"},{"location":"mcm-devops/#4-running-the-pipelines","text":"Whew, setting up those pipelines took a lot of work but now comes the rewarding part. We are finally going to run the pipelines! To run the mcm-dev pipeline, do the following: From the Jenkins home page select the mcm-dev item, then click Build with parameters , which will take you to a view similar to the following, where you can edit the Environment Variables, if needed: Click Build to start the pipeline, which will take you back to the project view. From the mcm-dev project view, click on the latest build number (in the above picture it would be #7) to view build details. To access the build logs, click on the Console Output button. I'll let you look into the logs to figure out what's going on. The best way to read the logs is by matching the output you see with the pipeline stages we went through earlier in the Pipeline Code Overview section. An important detail to know is that towards the end of the pipeline output, if the Validation Test stage completed successfully, you will notice that the pipeline triggers the mcm-promotion job and starts a new build for it, as shown below: If you would like to see output of the mcm-promotion job build, just click the mcm-promotion build number (#2 in the above picture). The way Jenkins treats jobs that trigger builds for other jobs is that it will wait until the triggered jobs finish before finishing the current job. In our case, the mcm-dev job build will wait until mcm-promotion job build finishes before it can finish itself. Finally, you will know that the pipeline finished successfully if you see Finished: SUCCESS as the last line of console output. If that's the case, then CONGRATULATIONS!!! You have successfully run an MCM CI/CD pipeline that automatically deploys the guestbook application to the se-dev-31 cluster but also, upon passing the validation tests, triggers the mcm-promotion pipeline to deploy the application to the se-stg-31 cluster. Now let's move on to manually testing the guestbook application on each cluster to verify that the pipelines indeed deployed the application successfully.","title":"4. Running the Pipelines"},{"location":"mcm-devops/#5-test-the-guestbook-application","text":"There are 2 ways in which we can access the guestbook application's frontend on each cluster: Getting the application URL from the Jenkins build output. From the Deployments page in the IBM Cloud Private console of each cluster.","title":"5. Test the Guestbook Application"},{"location":"mcm-devops/#a-getting-the-application-url-from-the-jenkins-build-output","text":"On the Validation Test stage you will see a link to the guestbook application's frontend URL in the logs as shown below: To access the guestbook application just click on the URL and you will be taken to the web application in your browser. This step applies to both mcm-dev and mcm-promotion jobs.","title":"a. Getting the application URL from the Jenkins build output"},{"location":"mcm-devops/#b-launching-the-application-from-the-ibm-cloud-private-console","text":"You can also launch the guestbook frontend from the IBM Cloud Private console. To do so, open a new browser window and go to https://ICP_MASTER_IP:8443/console/workloads/deployments and enter guestbook in the search bar. If done correctly, you will see a view like the following: If you are able to see the md-guestbook-gbapp , md-guestbook-gbapp-redismaster , and md-guestbook-gbapp-redisslave deployments and all have an Available value of 1 , then this means that all of the deployments have succesfully been deployed and started. At this point, all it takes to launch the application is to click the Launch button in the md-guestbook-gbapp row. This step applies to both se-dev-31 and se-stg-31 clusters.","title":"b. Launching the application from the IBM Cloud Private Console"},{"location":"mcm-devops/#c-testing-the-application","text":"The guestbook application itself is very simple. It consists of a web application that saves guest names (or any text) to a Redis deployment and persists them there even if the web application dies or restarts for some reason. To test its functionality, enter any text in the textbox shown below and click the Submit button. If everything worked successfully, you will see that the text you entered has now moved below the Submit button, which indicates that the text has been saved to and successfully read from the Redis deployment. To make sure that the text persists in the Redis deployment, feel free to refresh the page and make sure that the text you entered is bein shown again below the Submit button. If all the above was done successfully, that means that you have successfully verified the guestbook deployment and tested its functionality!","title":"c. Testing the Application"},{"location":"mcm-devops/#conclusion","text":"An automated CI/CD pipeline is the holy grail of a true DevOps setup. It enforces code quality by doing things like Validation Tests. It also saves many headaches when deploying new code to new environments because everything is automated. The problem with the traditional CI/CD approach is that the more environments you have (which is common in big enterprises), the more difficult and error prone it becomes to set up multiple CI/CD pipelines because you have to take many configuration settings and credentials into account for each environment. Thankfully, MCM takes care of saving all of the environment settings and credentials for you and lets you deploy applications to these environments by supplying values for easy-to-read cluster selector labels, as you saw in the examples above. The only thing that the developer has to remember is the login information for the MCM Controller cluster and the values for the cluster selector labels. Now that you have the knowledge of using MCM in your CI/CD pipelines, I encourage you to try this out with your own workloads!","title":"Conclusion"},{"location":"mcm-eks/","text":"Manage AWS EKS Clusters Author: Gang Chen (gangchen@us.ibm.com) This section focuses on how to manage an AWS hosted Kubernetes as a Service EKS cluster through MCM. Architecture To manage an Amazon Elastic Container Service for Kubernetes (EKS) cluster, you need to install the IBM Multicloud Manager Klusterlet in an EKS cluster. This guide will walk through the detail setup and configuration under this architecture. Pre-Requisites In order to go through this document, you are going to need the following: 1 x IBM Cloud Private cluster. IBM Multicloud Manager on ICP (hub) Kubectl (Kubernetes CLI) Follow the instructions here to install it on your platform. MCM CLI Follow the instructions here to install it on your platform. Essentially, you need to have the MCM hub controller already installed and configured before managing EKS clusters. 1. Prepare EKS cluster You need to have an EKS cluster ready. We'll cover the high level steps here rather than going in detail on how to create an EKS cluster. Getting Started with Amazon EKS is pretty easy to follow to get an EKS cluster. a. Create your Amazon EKS Service Role EKS kubernetes components uses this role to get Permission in AWS environment. For the sample configuration, I defined a role as: eksCaseServiceRole b. Create Cluster VPC and SecurityGroup It is recommended that you create a unique VPC and SecurityGroup for each EKS cluster. The getting started guide uses AWS CloudFormation to automate the VPC and SecurityGroup creation. You'll need the VPC, Subnet and SecurityGroup information for later steps. c. Configure kubectl and AWS CLI to Work with EKS In order for kubectl to interact with EKS, you'll need to configure the command line utilities to manage the authentication and authorization properly. Please follow these instruction to Configure kubectl for EKS , which you can run from your workstation. d. Create EKS cluster You can either create a cluster using AWS console or CLI. I used the console to create a cluster. You will be using the VPC and Security Group information created before when going through the cluster creation wizard. As this point, you are just getting an empty EKS cluster with control plane provisioned. But you will be charged for $0.20 per hour going forward. Once you are done with the cluster, go ahead and delete it to avoid further charges. You can validate that your cluster is running by running the following command\" kubectl get svc You should see output similar to the following, which means that kubectl was configured properly: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kubernetes ClusterIP 10.100.0.1 none 443/TCP 1m e. Launch and Configure EKS Worker Nodes In order to install MCM Klusterlet or do anything with your EKS cluster, you need to provision Worker nodes (essentially a collection of EC2 instances) and join them to the EKS cluster. Again, Amazon suggest to use CloudFormation to provision and join EKS worker nodes. To do so, you will need to provide the VPC, Subnet, SecurityGroup and EKS cluster name that you created earlier in the Stack creation wizard. Upon finishing the CloudFormation creation, you should see a stack similar to the following: You can validate that your worker nodes joined the EKS cluster by running the following command: $ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-1xx-22.ec2.internal Ready none 7h v1.11.5 ip-192-168-1xx-117.ec2.internal Ready none 7h v1.11.5 You can deploy sample apps to validate your cluster, but it is now ready to be managed by IBM MCM. 2. Install MCM Klusterlet on EKS Here is the official documentation for Installing the IBM Multicloud Manager Klusterlet Amazon Elastic Container Service for Kubernetes . It is a pretty straight forward process to get the klusterlet installed with the new IBM Multicloud Manager inception container. To learn more about the inception image, checkout its Docker Hub page at: https://hub.docker.com/r/ibmcom/mcm-inception-amd64 . a. Configure Installation Before installing the klusterlet, you need to get the configuration file and add the EKS cluster information. To create the configuration file, run the following commands: $ docker run -v $(pwd):/data -e LICENSE=accept \\ ibmcom/mcm-inception-amd64:3.1.2-ce \\ cp -r /installer/cluster.eks /data/cluster $ cd cluster At this point, you just need to update the config.yaml file and fill in the EKS cluster information: aws_access_key_id aws_secret_access_key aws_region eks-cluster cluster-name cluster-namespace cluster-tags hub-k8s-endpoint This is the MCM HUB Cluster endpoint. hub-k8s-token This is the MCM HUB Cluster access token. Here is the sample file I used: ## Kubernete Service Provider (DO NOT MODIFY) kubernete_service_provider: eks ## Service Provider Specific Paramaters aws: aws_access_key_id: your_access_id aws_secret_access_key: your_secret_key aws_region: us-east-1 eks-cluster: ibmcase-eks-cluster ## Multicloud Manager Klusterlet Settings klusterlet: cluster-name: ibmcase-eks-cluster cluster-namespace: eks-cluster cluster-tags: environment: 'Dev' owner: 'ibmcase' datacenter: 'auto-detect' region: 'auto-detect' hub-k8s-endpoint: https://mcm-mastr-xxx-xxxx-tor01.lb.bluemix.net:8001 hub-k8s-token: xxxx b. Install Klusterlet - Run the Inception Container To start the installation using the configuration file you provided, run the following command: $ docker run --net=host -t -e LICENSE=accept \\ -v $(pwd) :/installer/cluster \\ ibmcom/mcm-inception-amd64:3.1.2-ce \\ install-mcm-klusterlet -v In couple of minutes, you should see the installation completion message similar to this: ... ... ... NOTES: Thank you for installing ibm-mcmk-dev. Your release is named ibm-mcm-klusterlet. To learn more about the release, try: $ helm status ibm-mcm-klusterlet $ helm get ibm-mcm-klusterlet stdout_lines: omitted TASK [addon : include_tasks] ************************************************************************************************************************ skipping: [localhost] = (item={'key': u'ibm-mcm-klusterlet', 'value': {u'path': u'/addon/ibm-mcmk-dev-3.1.2-ce.tgz', u'namespace': u'mcm-klusterlet', u'use_custom_template': True}}) = changed=false item: key: ibm-mcm-klusterlet value: namespace: mcm-klusterlet path: /addon/ibm-mcmk-dev-3.1.2-ce.tgz use_custom_template: true skip_reason: Conditional result was False PLAY RECAP ****************************************************************************************************************************************** localhost : ok=47 changed=28 unreachable=0 failed=0 Playbook run took 0 days, 0 hours, 0 minutes, 50 seconds If you get an output similar to above, that means your EKS cluster is now ready to be managed by MCM! You can validate the installation by running the following command, which should show the ibm-mcm-klusterlet-* pods: $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-dmwhw 1/1 Running 0 8h kube-system aws-node-lfjxk 1/1 Running 0 8h kube-system coredns-7bcbfc4774-fl48s 1/1 Running 0 22h kube-system coredns-7bcbfc4774-h22bw 1/1 Running 0 22h kube-system kube-proxy-2gsts 1/1 Running 0 8h kube-system kube-proxy-gb6zz 1/1 Running 0 8h kube-system tiller-deploy-b7f4768d6-vp9js 1/1 Running 0 6h mcm-klusterlet ibm-mcm-klusterlet-ibm-mcmk-dev-klusterlet-c49b87894-dgp7k 4/4 Running 0 6h mcm-klusterlet ibm-mcm-klusterlet-ibm-mcmk-dev-weave-scope-app-54dd79b6fblvfx6 2/2 Running 0 6h mcm-klusterlet ibm-mcm-klusterlet-ibm-mcmk-dev-weave-scope-pzw5c 1/1 Running 0 6h mcm-klusterlet ibm-mcm-klusterlet-ibm-mcmk-dev-weave-scope-rlrcl 1/1 Running 0 6h mcm-klusterlet ibm-mcm-monitoring-prometheus-6cb4d8dbdb-g4mbb 2/2 Running 0 6h If you log in to the MCM HUB console, you should see the EKS cluster show up in the Clusters page: 3. Deploy a Sample Application to the Managed EKS Cluster a. Deploy NodeJS Application In MCM HUB Cluster console, click the Catalog button to go to the ICP applications catalog. Search for node , then click on the ibm-node-sample application, as shown below: Click Configure to move to the Helm configuration page. Enter the following values: Helm release name : ibmnode-eks Target namespace : eks-cluster Click the checkbox under License . To install on the application on the EKS cluster, make sure to click Remote Install (shown above), then in Target Clusters field drop down (shown below), check your target EKS cluster as deployment target. Click Install to perform the installation on the EKS cluster. b. Validate NodeJS Application Installation on the MCM HUB Cluster Console Now go back to the MCM HUB cluster console and go to the Helm Releases page to see that the app has been deployed to the EKS cluster: c. Validate NodeJS Application Installation from kubectl on EKS To validate the NodeJS application installation, run the following command: # Get the application pod $ kubectl get pods -n eks-cluster # replace the namespace with the one you correlated NAME READY STATUS RESTARTS AGE ibmnode-eks-nodejssample-nodejs-54775f7845-ngf6f 1/1 Running 0 2m # Get the application service $ kubectl get svc -n eks-cluster NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ibmnode-eks-nodejssample-nodejs NodePort 10.100.220.242 none 3000:32337/TCP 3m You should see the NodeJS pod and service are running. To see it in browser, you need to expose the service to AWS load balancer or Ingress. I'll leave that to you as homework. Congratulations, you now have successfully integrated an EKS cluster with IBM Multicloud Manager and deployed an application on the EKS cluster through IBM Multicloud Manager. 4. Delete the EKS Cluster It is suggested to delete the EKS cluster and worker nodes after testing completes to avoid further charts. To do so, please follow this guide on how to Delete an EKS cluster . Conclusion Now that you know how to use MCM to manage, monitor, and deploy applications on EKS clusters, you should checkout the DevOps in Multi-cluster Environment chapter and attempt to deploy applications to the EKS cluster with MCM using an automated CI/CD pipeline.","title":"Manage AWS EKS Clusters"},{"location":"mcm-eks/#manage-aws-eks-clusters","text":"Author: Gang Chen (gangchen@us.ibm.com) This section focuses on how to manage an AWS hosted Kubernetes as a Service EKS cluster through MCM.","title":"Manage AWS EKS Clusters"},{"location":"mcm-eks/#architecture","text":"To manage an Amazon Elastic Container Service for Kubernetes (EKS) cluster, you need to install the IBM Multicloud Manager Klusterlet in an EKS cluster. This guide will walk through the detail setup and configuration under this architecture.","title":"Architecture"},{"location":"mcm-eks/#pre-requisites","text":"In order to go through this document, you are going to need the following: 1 x IBM Cloud Private cluster. IBM Multicloud Manager on ICP (hub) Kubectl (Kubernetes CLI) Follow the instructions here to install it on your platform. MCM CLI Follow the instructions here to install it on your platform. Essentially, you need to have the MCM hub controller already installed and configured before managing EKS clusters.","title":"Pre-Requisites"},{"location":"mcm-eks/#1-prepare-eks-cluster","text":"You need to have an EKS cluster ready. We'll cover the high level steps here rather than going in detail on how to create an EKS cluster. Getting Started with Amazon EKS is pretty easy to follow to get an EKS cluster.","title":"1. Prepare EKS cluster"},{"location":"mcm-eks/#a-create-your-amazon-eks-service-role","text":"EKS kubernetes components uses this role to get Permission in AWS environment. For the sample configuration, I defined a role as: eksCaseServiceRole","title":"a. Create your Amazon EKS Service Role"},{"location":"mcm-eks/#b-create-cluster-vpc-and-securitygroup","text":"It is recommended that you create a unique VPC and SecurityGroup for each EKS cluster. The getting started guide uses AWS CloudFormation to automate the VPC and SecurityGroup creation. You'll need the VPC, Subnet and SecurityGroup information for later steps.","title":"b. Create Cluster VPC and SecurityGroup"},{"location":"mcm-eks/#c-configure-kubectl-and-aws-cli-to-work-with-eks","text":"In order for kubectl to interact with EKS, you'll need to configure the command line utilities to manage the authentication and authorization properly. Please follow these instruction to Configure kubectl for EKS , which you can run from your workstation.","title":"c. Configure kubectl and AWS CLI to Work with EKS"},{"location":"mcm-eks/#d-create-eks-cluster","text":"You can either create a cluster using AWS console or CLI. I used the console to create a cluster. You will be using the VPC and Security Group information created before when going through the cluster creation wizard. As this point, you are just getting an empty EKS cluster with control plane provisioned. But you will be charged for $0.20 per hour going forward. Once you are done with the cluster, go ahead and delete it to avoid further charges. You can validate that your cluster is running by running the following command\" kubectl get svc You should see output similar to the following, which means that kubectl was configured properly: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kubernetes ClusterIP 10.100.0.1 none 443/TCP 1m","title":"d. Create EKS cluster"},{"location":"mcm-eks/#e-launch-and-configure-eks-worker-nodes","text":"In order to install MCM Klusterlet or do anything with your EKS cluster, you need to provision Worker nodes (essentially a collection of EC2 instances) and join them to the EKS cluster. Again, Amazon suggest to use CloudFormation to provision and join EKS worker nodes. To do so, you will need to provide the VPC, Subnet, SecurityGroup and EKS cluster name that you created earlier in the Stack creation wizard. Upon finishing the CloudFormation creation, you should see a stack similar to the following: You can validate that your worker nodes joined the EKS cluster by running the following command: $ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-1xx-22.ec2.internal Ready none 7h v1.11.5 ip-192-168-1xx-117.ec2.internal Ready none 7h v1.11.5 You can deploy sample apps to validate your cluster, but it is now ready to be managed by IBM MCM.","title":"e. Launch and Configure EKS Worker Nodes"},{"location":"mcm-eks/#2-install-mcm-klusterlet-on-eks","text":"Here is the official documentation for Installing the IBM Multicloud Manager Klusterlet Amazon Elastic Container Service for Kubernetes . It is a pretty straight forward process to get the klusterlet installed with the new IBM Multicloud Manager inception container. To learn more about the inception image, checkout its Docker Hub page at: https://hub.docker.com/r/ibmcom/mcm-inception-amd64 .","title":"2. Install MCM Klusterlet on EKS"},{"location":"mcm-eks/#a-configure-installation","text":"Before installing the klusterlet, you need to get the configuration file and add the EKS cluster information. To create the configuration file, run the following commands: $ docker run -v $(pwd):/data -e LICENSE=accept \\ ibmcom/mcm-inception-amd64:3.1.2-ce \\ cp -r /installer/cluster.eks /data/cluster $ cd cluster At this point, you just need to update the config.yaml file and fill in the EKS cluster information: aws_access_key_id aws_secret_access_key aws_region eks-cluster cluster-name cluster-namespace cluster-tags hub-k8s-endpoint This is the MCM HUB Cluster endpoint. hub-k8s-token This is the MCM HUB Cluster access token. Here is the sample file I used: ## Kubernete Service Provider (DO NOT MODIFY) kubernete_service_provider: eks ## Service Provider Specific Paramaters aws: aws_access_key_id: your_access_id aws_secret_access_key: your_secret_key aws_region: us-east-1 eks-cluster: ibmcase-eks-cluster ## Multicloud Manager Klusterlet Settings klusterlet: cluster-name: ibmcase-eks-cluster cluster-namespace: eks-cluster cluster-tags: environment: 'Dev' owner: 'ibmcase' datacenter: 'auto-detect' region: 'auto-detect' hub-k8s-endpoint: https://mcm-mastr-xxx-xxxx-tor01.lb.bluemix.net:8001 hub-k8s-token: xxxx","title":"a. Configure Installation"},{"location":"mcm-eks/#b-install-klusterlet-run-the-inception-container","text":"To start the installation using the configuration file you provided, run the following command: $ docker run --net=host -t -e LICENSE=accept \\ -v $(pwd) :/installer/cluster \\ ibmcom/mcm-inception-amd64:3.1.2-ce \\ install-mcm-klusterlet -v In couple of minutes, you should see the installation completion message similar to this: ... ... ... NOTES: Thank you for installing ibm-mcmk-dev. Your release is named ibm-mcm-klusterlet. To learn more about the release, try: $ helm status ibm-mcm-klusterlet $ helm get ibm-mcm-klusterlet stdout_lines: omitted TASK [addon : include_tasks] ************************************************************************************************************************ skipping: [localhost] = (item={'key': u'ibm-mcm-klusterlet', 'value': {u'path': u'/addon/ibm-mcmk-dev-3.1.2-ce.tgz', u'namespace': u'mcm-klusterlet', u'use_custom_template': True}}) = changed=false item: key: ibm-mcm-klusterlet value: namespace: mcm-klusterlet path: /addon/ibm-mcmk-dev-3.1.2-ce.tgz use_custom_template: true skip_reason: Conditional result was False PLAY RECAP ****************************************************************************************************************************************** localhost : ok=47 changed=28 unreachable=0 failed=0 Playbook run took 0 days, 0 hours, 0 minutes, 50 seconds If you get an output similar to above, that means your EKS cluster is now ready to be managed by MCM! You can validate the installation by running the following command, which should show the ibm-mcm-klusterlet-* pods: $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-dmwhw 1/1 Running 0 8h kube-system aws-node-lfjxk 1/1 Running 0 8h kube-system coredns-7bcbfc4774-fl48s 1/1 Running 0 22h kube-system coredns-7bcbfc4774-h22bw 1/1 Running 0 22h kube-system kube-proxy-2gsts 1/1 Running 0 8h kube-system kube-proxy-gb6zz 1/1 Running 0 8h kube-system tiller-deploy-b7f4768d6-vp9js 1/1 Running 0 6h mcm-klusterlet ibm-mcm-klusterlet-ibm-mcmk-dev-klusterlet-c49b87894-dgp7k 4/4 Running 0 6h mcm-klusterlet ibm-mcm-klusterlet-ibm-mcmk-dev-weave-scope-app-54dd79b6fblvfx6 2/2 Running 0 6h mcm-klusterlet ibm-mcm-klusterlet-ibm-mcmk-dev-weave-scope-pzw5c 1/1 Running 0 6h mcm-klusterlet ibm-mcm-klusterlet-ibm-mcmk-dev-weave-scope-rlrcl 1/1 Running 0 6h mcm-klusterlet ibm-mcm-monitoring-prometheus-6cb4d8dbdb-g4mbb 2/2 Running 0 6h If you log in to the MCM HUB console, you should see the EKS cluster show up in the Clusters page:","title":"b. Install Klusterlet - Run the Inception Container"},{"location":"mcm-eks/#3-deploy-a-sample-application-to-the-managed-eks-cluster","text":"","title":"3. Deploy a Sample Application to the Managed EKS Cluster"},{"location":"mcm-eks/#a-deploy-nodejs-application","text":"In MCM HUB Cluster console, click the Catalog button to go to the ICP applications catalog. Search for node , then click on the ibm-node-sample application, as shown below: Click Configure to move to the Helm configuration page. Enter the following values: Helm release name : ibmnode-eks Target namespace : eks-cluster Click the checkbox under License . To install on the application on the EKS cluster, make sure to click Remote Install (shown above), then in Target Clusters field drop down (shown below), check your target EKS cluster as deployment target. Click Install to perform the installation on the EKS cluster.","title":"a. Deploy NodeJS Application"},{"location":"mcm-eks/#b-validate-nodejs-application-installation-on-the-mcm-hub-cluster-console","text":"Now go back to the MCM HUB cluster console and go to the Helm Releases page to see that the app has been deployed to the EKS cluster:","title":"b. Validate NodeJS Application Installation on the MCM HUB Cluster Console"},{"location":"mcm-eks/#c-validate-nodejs-application-installation-from-kubectl-on-eks","text":"To validate the NodeJS application installation, run the following command: # Get the application pod $ kubectl get pods -n eks-cluster # replace the namespace with the one you correlated NAME READY STATUS RESTARTS AGE ibmnode-eks-nodejssample-nodejs-54775f7845-ngf6f 1/1 Running 0 2m # Get the application service $ kubectl get svc -n eks-cluster NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ibmnode-eks-nodejssample-nodejs NodePort 10.100.220.242 none 3000:32337/TCP 3m You should see the NodeJS pod and service are running. To see it in browser, you need to expose the service to AWS load balancer or Ingress. I'll leave that to you as homework. Congratulations, you now have successfully integrated an EKS cluster with IBM Multicloud Manager and deployed an application on the EKS cluster through IBM Multicloud Manager.","title":"c. Validate NodeJS Application Installation from kubectl on EKS"},{"location":"mcm-eks/#4-delete-the-eks-cluster","text":"It is suggested to delete the EKS cluster and worker nodes after testing completes to avoid further charts. To do so, please follow this guide on how to Delete an EKS cluster .","title":"4. Delete the EKS Cluster"},{"location":"mcm-eks/#conclusion","text":"Now that you know how to use MCM to manage, monitor, and deploy applications on EKS clusters, you should checkout the DevOps in Multi-cluster Environment chapter and attempt to deploy applications to the EKS cluster with MCM using an automated CI/CD pipeline.","title":"Conclusion"},{"location":"mcm-monitoring-event-management/","text":"IBM Multicloud Manager - Monitoring and Event Management Author: Rafal Szypulka (rafal.szypulka@pl.ibm.com) This chapter is focused on monitoring and event management features delivered by IBM Multicloud Manager 3.1.2. Overall MCM dashboard MCM Application monitoring Metrics collection and visualization IBM Cloud Event Management for IBM Multicloud Manager Installing the Cloud Event Management for IBM Multicloud Manager Prerequisites Installation of the Cloud Event Management controller Installation of the Cloud Event Management for IBM Multicloud Manager User management First logon to the CEM console Conclusion Overall MCM dashboard IBM Multicloud Manager Overview dashboard is available from the Overview section in the MCM menu. You can view details of your IBM Cloud Private clusters and other cloud service providers supported by IBM Cloud Private. You can also view details about your applications. The Overview dashboard is continuously refreshed in real time. The following information about clusters is provided: Name of the cloud service with the number of clusters Cluster compliance Pod details Cluster status Cluster resources (VCPU/Memory usage) Storage usage You can also view the information about each application and clusters where this application has been deployed: Number of clusters Number of Kubernetes types Number of regions Number of nodes Number of pods The Overview page can be further personalized with the filtering feature. Click Filter results menu to specify what information is displayed on your page. MCM Application monitoring Metrics collection and visualization To access an Application Health View Dashboard (shown below), you must first access the Applications page from the MCM Menu: Next to each application, under the DASHBOARD column, there is a Launch Health View button. If you click it, you will open the Health View Grafana dashboard for that application. A Grafana Dashboard for MCM applications is generated automatically for each deployed application and shows metrics related to resource utilization (CPU, memory, network) of the application containers and overall resource utilization of the clusters where an application has been deployed. MCM federated Prometheus is a data source for an application monitoring dashboard. MCM Controller installation deploys a federated Prometheus instance which will pull selected metric data from the Prometheus instances located on managed ICP clusters. The deployment name for the MCM Controller's Federated Prometheus is: mcm-controller-ibm-mcm-prod-prometheus Initially, just after MCM installation, the MCM federated Prometheus instance doesn't collect any data. Its configuration is generated dynamically during application deployment via MCM. The Target ICP clusters are added to the MCM federated Prometheus instance's ConfigMap during application deployment. The example below shows a dynamic update of the MCM federated Prometheus ConfigMap after deployment of the application to three ICP clusters: scrape_configs: - job_name: mcm-dynamic-se-prod-312-ubuntu honor_labels: true params: match[]: - '{job= kubernetes-cadvisor }' scrape_interval: 1m scrape_timeout: 30s metrics_path: /apis/mcm.ibm.com/v1alpha1/namespaces/se-prod-312-ubuntu/clusterstatuses/se-prod-312-ubuntu/monitor/federate scheme: https static_configs: - targets: - kubernetes.default:443 labels: cluster_name: se-prod-312-ubuntu bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: false - job_name: mcm-dynamic-se-stg-312-ubuntu honor_labels: true params: match[]: - '{job= kubernetes-cadvisor }' scrape_interval: 1m scrape_timeout: 30s metrics_path: /apis/mcm.ibm.com/v1alpha1/namespaces/se-stg-312-ubuntu/clusterstatuses/se-stg-312-ubuntu/monitor/federate scheme: https static_configs: - targets: - kubernetes.default:443 labels: cluster_name: se-stg-312-ubuntu bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: false - job_name: mcm-dynamic-se-dev-312-ubuntu honor_labels: true params: match[]: - '{job= kubernetes-cadvisor }' scrape_interval: 1m scrape_timeout: 30s metrics_path: /apis/mcm.ibm.com/v1alpha1/namespaces/se-dev-312-ubuntu/clusterstatuses/se-dev-312-ubuntu/monitor/federate scheme: https static_configs: - targets: - kubernetes.default:443 labels: cluster_name: se-dev-312-ubuntu bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: false In the example above, the generated configuration instructs MCM federated Prometheus instance to collect cAdvisor metrics from three child ICP Prometheus instances, located respectively on three ICP clusters: se-prod-312-ubuntu , se-stg-312-ubuntu and se-dev-312-ubuntu . More information about Prometheus federation mechanisms: https://prometheus.io/docs/prometheus/latest/federation/ . IBM Cloud Event Management for IBM Multicloud Manager IBM Cloud Event Management (CEM) allows to set up a real-time incident management for the applications and infrastructure managed by the Multi Cloud Manager. Incidents are generated from events/alerts which indicate that something has happened on an application, service, or another monitored object. Cloud Event Management can receive events from various monitoring sources, either on premise or in the cloud. In the MCM environment, the CEM collects alerts from Prometheus instances located at each managed cluster. The Cloud Event Management Controller for MCM (deployed using alerttargetcontroller helm chart) automatically configures managed Prometheus Alertmanager instances to send alert notifications to the central CEM instance installed on MCM Controller. Installing the Cloud Event Management for IBM Multicloud Manager The IBM Cloud Event Management for MCM is included inside IBM Multicloud Manager installation package. Unpack the MCM installation archive mcm-3.1.2.tgz and inside you will find two CEM PPA packages: Could Event Management Controller - alerttargetcontroller-ppa-0.0.2.tar.gz Cloud Event Management - cem-mcm-ppa-ibm-cem-2.2.0.tar.gz Load both PPA packages to the local container registry on MCM hub cluster and alerttargetcontroller on every managed cluster. docker login MCM-cluster-hostname :8500 cloudctl login -a https:// MCM-cluster-hostname :8443 --skip-ssl-validation -n kube-system cloudctl catalog load-archive -a ppa-archive --registry MCM-cluster-hostname :8500/kube-system The following procedure describes installation and configuration steps on example ICP 3.1.2 cluster running MCM controller. The sequence of the steps is important. Prerequisites MCM Controller and MCM Klusterlet deployed on MCM hub ICP 3.1.2 cluster and Klusterlet deployed on managed ICP clusters. CEM PPA packages imported as per instructions above. CEM users have an Administrator role within a Team which has a resource management assigned to a managed cluster namespaces. Installation of the Cloud Event Management controller Go to the ICP Catalog and deploy the alerttargetcontroller chart on both MCM hub cluster and managed clusters in the kube-system namespace. MCM Fullname Override option can be obtained using: kubectl get po -n kube-system | grep klusterlet Copy the pod name part before the klusterlet word ( klusterlet-ibm-mcmk-prod on the example below): # kubectl get po | grep klusterlet klusterlet-ibm-mcmk-prod-klusterlet-657958f69f-v7cw9 3/3 Running 0 30h klusterlet-ibm-mcmk-prod-weave-scope-49sxf 1/1 Running 0 30h ICP Cluster namespace is the cluster namespace created during klusterlet deployment. In our case the namespace name is mcm-se-dev-31 . It can be obtained using: # kubectl get clusters --all-namespaces NAMESPACE NAME ENDPOINTS STATUS AGE se-stg-31 se-stg-31 172.16.40.68:8001 Ready 1d se-dev-31 se-dev-31 172.16.40.98:8001 Ready 1d After chart deployment, make sure the alerttargetcontroller pod is running. # kubectl get pod -n kube-system|grep alerttarget atc-alerttargetcontroller-alerttargetcontroller-77f87fb77cx6fph 1/1 Running 1 10h Installation of the Cloud Event Management for IBM Multicloud Manager Deploy ibm-cem chart you loaded together with CEM image to local-charts repository in the kube-system namespace on the MCM hub cluster. NOTE : The CEM chart provided with ICP's built-in ibm-charts repository is a Community Edition version which is not designed to work with MCM. In our setup we used ICP UI console hostname for both ICP Master IP and Ingress Domain options. After deployment, wait a couple of minutes until all CEM pods are started and run the following command to configure OIDC registration with IBM Cloud Private: kubectl exec -n kube-system -t `kubectl get pods -l release=cem -n kube-system \\ | grep cem-ibm-cem-cem-users | grep Running | head -n 1 \\ | awk '{print $1}'` bash -- /etc/oidc/oidc_reg.sh `echo $(kubectl get secret platform-oidc-credentials -o yaml -n kube-system \\ | grep OAUTH2_CLIENT_REGISTRATION_SECRET: | awk '{print $2}')` Verify the alerttargets CRD has been created: kubectl get alerttargets --all-namespaces NAMESPACE NAME AGE se-stg-31 se-stg-31-se-stg-31 2h se-dev-31 se-dev-31-se-dev-31 2h At this point the Alertmanager ConfigMaps for Prometheus instances are located on managed clusters: monitoring-prometheus-alertmanager should be automatically updated by the alerttargetcontroller . To see the ConfigMap YAML for each managed cluster, run the following command on each managed cluster: kubectl get cm monitoring-prometheus-alertmanager -n kube-system -o yaml In the route: - routes: section you should see the following: - receiver: cemwebhook group_by: - alertname - instance - severity continue: true group_wait: 10s group_interval: 10s repeat_interval: 1m And in the receivers: section you should see: - name: cemwebhook webhook_configs: - send_resolved: true http_config: tls_config: insecure_skip_verify: true url: https://172.16.40.68:8443/norml/webhook/prometheus/cem-1/695c8db7-344c-4af7-84f3-10f99eab440a/Snfok7F3_0ndxxxxxx762ZWnsMmnPtnLG69ID_rzctg Note, the CEM url: will be different in your environment. The CEM Alert Target Controller adds a couple of sample alert definitions to the AlertRules CRD. This can be verified using by running the following command: kubectl get alertrules NAME ENABLED AGE CHART RELEASE ERRORS cem-alert-rules true 2h (...) These alert rules can be customized based on your requirements. We recommend considering our best practice alert definitions for ICP platform . Check the Prometheus Alertmanager logs to verify that there are no errors while sending the webhook notifications to CEM. kubectl logs alertmanager pod -n kube-system User management CEM console access is managed through ICP Teams page in the ICP Console. To create a team with Administrator role, run the following steps: On the MCM HUB Cluster, go to Manage - Identity Access . Click on the Teams menu. Create a Team and call it CEM . Click the Users tab and assign the Administrator role to the users in the Team. Finally, click the Resources tab and add a row of type Namespace with the namespace name for each managed cluster. First login to the CEM console Cloud Event Management console can be accessed from the Multicloud Manager console. Logon to MCM UI as one of the Team members (mentioned in the User Management section) and select Event Management . You may be asked to authenticate again with the ICP user and you should see one or more subscriptions. Example subscription cem-1 on the picture above is the name of the ICP Team authorized to manage a cluster namespace. Click Launch and then on the Incidents tab. If some defined Prometheus alerts are active (you can verify it via Prometheus Alertmanager UI available from the ICP console Platform - Alerting ), you should see those incidents in the CEM UI: Now click on one of the incidents to see the Prometheus alert details: Click on the Generator URL link to open the Prometheus UI on the managed cluster and see the current result of the PromQL query that generated this alert: Conclusion IBM Cloud Event Management for IBM Multicloud Manager enables you to access Prometheus alert information for each of your managed clusters from a centralized location. To learn more about using and operating Cloud Event Management, check out the Cloud Event Management documentation .","title":"Monitoring and Event Management with MCM"},{"location":"mcm-monitoring-event-management/#ibm-multicloud-manager-monitoring-and-event-management","text":"Author: Rafal Szypulka (rafal.szypulka@pl.ibm.com) This chapter is focused on monitoring and event management features delivered by IBM Multicloud Manager 3.1.2. Overall MCM dashboard MCM Application monitoring Metrics collection and visualization IBM Cloud Event Management for IBM Multicloud Manager Installing the Cloud Event Management for IBM Multicloud Manager Prerequisites Installation of the Cloud Event Management controller Installation of the Cloud Event Management for IBM Multicloud Manager User management First logon to the CEM console Conclusion","title":"IBM Multicloud Manager - Monitoring and Event Management"},{"location":"mcm-monitoring-event-management/#overall-mcm-dashboard","text":"IBM Multicloud Manager Overview dashboard is available from the Overview section in the MCM menu. You can view details of your IBM Cloud Private clusters and other cloud service providers supported by IBM Cloud Private. You can also view details about your applications. The Overview dashboard is continuously refreshed in real time. The following information about clusters is provided: Name of the cloud service with the number of clusters Cluster compliance Pod details Cluster status Cluster resources (VCPU/Memory usage) Storage usage You can also view the information about each application and clusters where this application has been deployed: Number of clusters Number of Kubernetes types Number of regions Number of nodes Number of pods The Overview page can be further personalized with the filtering feature. Click Filter results menu to specify what information is displayed on your page.","title":"Overall MCM dashboard"},{"location":"mcm-monitoring-event-management/#mcm-application-monitoring","text":"","title":"MCM Application monitoring"},{"location":"mcm-monitoring-event-management/#metrics-collection-and-visualization","text":"To access an Application Health View Dashboard (shown below), you must first access the Applications page from the MCM Menu: Next to each application, under the DASHBOARD column, there is a Launch Health View button. If you click it, you will open the Health View Grafana dashboard for that application. A Grafana Dashboard for MCM applications is generated automatically for each deployed application and shows metrics related to resource utilization (CPU, memory, network) of the application containers and overall resource utilization of the clusters where an application has been deployed. MCM federated Prometheus is a data source for an application monitoring dashboard. MCM Controller installation deploys a federated Prometheus instance which will pull selected metric data from the Prometheus instances located on managed ICP clusters. The deployment name for the MCM Controller's Federated Prometheus is: mcm-controller-ibm-mcm-prod-prometheus Initially, just after MCM installation, the MCM federated Prometheus instance doesn't collect any data. Its configuration is generated dynamically during application deployment via MCM. The Target ICP clusters are added to the MCM federated Prometheus instance's ConfigMap during application deployment. The example below shows a dynamic update of the MCM federated Prometheus ConfigMap after deployment of the application to three ICP clusters: scrape_configs: - job_name: mcm-dynamic-se-prod-312-ubuntu honor_labels: true params: match[]: - '{job= kubernetes-cadvisor }' scrape_interval: 1m scrape_timeout: 30s metrics_path: /apis/mcm.ibm.com/v1alpha1/namespaces/se-prod-312-ubuntu/clusterstatuses/se-prod-312-ubuntu/monitor/federate scheme: https static_configs: - targets: - kubernetes.default:443 labels: cluster_name: se-prod-312-ubuntu bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: false - job_name: mcm-dynamic-se-stg-312-ubuntu honor_labels: true params: match[]: - '{job= kubernetes-cadvisor }' scrape_interval: 1m scrape_timeout: 30s metrics_path: /apis/mcm.ibm.com/v1alpha1/namespaces/se-stg-312-ubuntu/clusterstatuses/se-stg-312-ubuntu/monitor/federate scheme: https static_configs: - targets: - kubernetes.default:443 labels: cluster_name: se-stg-312-ubuntu bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: false - job_name: mcm-dynamic-se-dev-312-ubuntu honor_labels: true params: match[]: - '{job= kubernetes-cadvisor }' scrape_interval: 1m scrape_timeout: 30s metrics_path: /apis/mcm.ibm.com/v1alpha1/namespaces/se-dev-312-ubuntu/clusterstatuses/se-dev-312-ubuntu/monitor/federate scheme: https static_configs: - targets: - kubernetes.default:443 labels: cluster_name: se-dev-312-ubuntu bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: false In the example above, the generated configuration instructs MCM federated Prometheus instance to collect cAdvisor metrics from three child ICP Prometheus instances, located respectively on three ICP clusters: se-prod-312-ubuntu , se-stg-312-ubuntu and se-dev-312-ubuntu . More information about Prometheus federation mechanisms: https://prometheus.io/docs/prometheus/latest/federation/ .","title":"Metrics collection and visualization"},{"location":"mcm-monitoring-event-management/#ibm-cloud-event-management-for-ibm-multicloud-manager","text":"IBM Cloud Event Management (CEM) allows to set up a real-time incident management for the applications and infrastructure managed by the Multi Cloud Manager. Incidents are generated from events/alerts which indicate that something has happened on an application, service, or another monitored object. Cloud Event Management can receive events from various monitoring sources, either on premise or in the cloud. In the MCM environment, the CEM collects alerts from Prometheus instances located at each managed cluster. The Cloud Event Management Controller for MCM (deployed using alerttargetcontroller helm chart) automatically configures managed Prometheus Alertmanager instances to send alert notifications to the central CEM instance installed on MCM Controller.","title":"IBM Cloud Event Management for IBM Multicloud Manager"},{"location":"mcm-monitoring-event-management/#installing-the-cloud-event-management-for-ibm-multicloud-manager","text":"The IBM Cloud Event Management for MCM is included inside IBM Multicloud Manager installation package. Unpack the MCM installation archive mcm-3.1.2.tgz and inside you will find two CEM PPA packages: Could Event Management Controller - alerttargetcontroller-ppa-0.0.2.tar.gz Cloud Event Management - cem-mcm-ppa-ibm-cem-2.2.0.tar.gz Load both PPA packages to the local container registry on MCM hub cluster and alerttargetcontroller on every managed cluster. docker login MCM-cluster-hostname :8500 cloudctl login -a https:// MCM-cluster-hostname :8443 --skip-ssl-validation -n kube-system cloudctl catalog load-archive -a ppa-archive --registry MCM-cluster-hostname :8500/kube-system The following procedure describes installation and configuration steps on example ICP 3.1.2 cluster running MCM controller. The sequence of the steps is important.","title":"Installing the Cloud Event Management for IBM Multicloud Manager"},{"location":"mcm-monitoring-event-management/#prerequisites","text":"MCM Controller and MCM Klusterlet deployed on MCM hub ICP 3.1.2 cluster and Klusterlet deployed on managed ICP clusters. CEM PPA packages imported as per instructions above. CEM users have an Administrator role within a Team which has a resource management assigned to a managed cluster namespaces.","title":"Prerequisites"},{"location":"mcm-monitoring-event-management/#installation-of-the-cloud-event-management-controller","text":"Go to the ICP Catalog and deploy the alerttargetcontroller chart on both MCM hub cluster and managed clusters in the kube-system namespace. MCM Fullname Override option can be obtained using: kubectl get po -n kube-system | grep klusterlet Copy the pod name part before the klusterlet word ( klusterlet-ibm-mcmk-prod on the example below): # kubectl get po | grep klusterlet klusterlet-ibm-mcmk-prod-klusterlet-657958f69f-v7cw9 3/3 Running 0 30h klusterlet-ibm-mcmk-prod-weave-scope-49sxf 1/1 Running 0 30h ICP Cluster namespace is the cluster namespace created during klusterlet deployment. In our case the namespace name is mcm-se-dev-31 . It can be obtained using: # kubectl get clusters --all-namespaces NAMESPACE NAME ENDPOINTS STATUS AGE se-stg-31 se-stg-31 172.16.40.68:8001 Ready 1d se-dev-31 se-dev-31 172.16.40.98:8001 Ready 1d After chart deployment, make sure the alerttargetcontroller pod is running. # kubectl get pod -n kube-system|grep alerttarget atc-alerttargetcontroller-alerttargetcontroller-77f87fb77cx6fph 1/1 Running 1 10h","title":"Installation of the Cloud Event Management controller"},{"location":"mcm-monitoring-event-management/#installation-of-the-cloud-event-management-for-ibm-multicloud-manager","text":"Deploy ibm-cem chart you loaded together with CEM image to local-charts repository in the kube-system namespace on the MCM hub cluster. NOTE : The CEM chart provided with ICP's built-in ibm-charts repository is a Community Edition version which is not designed to work with MCM. In our setup we used ICP UI console hostname for both ICP Master IP and Ingress Domain options. After deployment, wait a couple of minutes until all CEM pods are started and run the following command to configure OIDC registration with IBM Cloud Private: kubectl exec -n kube-system -t `kubectl get pods -l release=cem -n kube-system \\ | grep cem-ibm-cem-cem-users | grep Running | head -n 1 \\ | awk '{print $1}'` bash -- /etc/oidc/oidc_reg.sh `echo $(kubectl get secret platform-oidc-credentials -o yaml -n kube-system \\ | grep OAUTH2_CLIENT_REGISTRATION_SECRET: | awk '{print $2}')` Verify the alerttargets CRD has been created: kubectl get alerttargets --all-namespaces NAMESPACE NAME AGE se-stg-31 se-stg-31-se-stg-31 2h se-dev-31 se-dev-31-se-dev-31 2h At this point the Alertmanager ConfigMaps for Prometheus instances are located on managed clusters: monitoring-prometheus-alertmanager should be automatically updated by the alerttargetcontroller . To see the ConfigMap YAML for each managed cluster, run the following command on each managed cluster: kubectl get cm monitoring-prometheus-alertmanager -n kube-system -o yaml In the route: - routes: section you should see the following: - receiver: cemwebhook group_by: - alertname - instance - severity continue: true group_wait: 10s group_interval: 10s repeat_interval: 1m And in the receivers: section you should see: - name: cemwebhook webhook_configs: - send_resolved: true http_config: tls_config: insecure_skip_verify: true url: https://172.16.40.68:8443/norml/webhook/prometheus/cem-1/695c8db7-344c-4af7-84f3-10f99eab440a/Snfok7F3_0ndxxxxxx762ZWnsMmnPtnLG69ID_rzctg Note, the CEM url: will be different in your environment. The CEM Alert Target Controller adds a couple of sample alert definitions to the AlertRules CRD. This can be verified using by running the following command: kubectl get alertrules NAME ENABLED AGE CHART RELEASE ERRORS cem-alert-rules true 2h (...) These alert rules can be customized based on your requirements. We recommend considering our best practice alert definitions for ICP platform . Check the Prometheus Alertmanager logs to verify that there are no errors while sending the webhook notifications to CEM. kubectl logs alertmanager pod -n kube-system","title":"Installation of the Cloud Event Management for IBM Multicloud Manager"},{"location":"mcm-monitoring-event-management/#user-management","text":"CEM console access is managed through ICP Teams page in the ICP Console. To create a team with Administrator role, run the following steps: On the MCM HUB Cluster, go to Manage - Identity Access . Click on the Teams menu. Create a Team and call it CEM . Click the Users tab and assign the Administrator role to the users in the Team. Finally, click the Resources tab and add a row of type Namespace with the namespace name for each managed cluster.","title":"User management"},{"location":"mcm-monitoring-event-management/#first-login-to-the-cem-console","text":"Cloud Event Management console can be accessed from the Multicloud Manager console. Logon to MCM UI as one of the Team members (mentioned in the User Management section) and select Event Management . You may be asked to authenticate again with the ICP user and you should see one or more subscriptions. Example subscription cem-1 on the picture above is the name of the ICP Team authorized to manage a cluster namespace. Click Launch and then on the Incidents tab. If some defined Prometheus alerts are active (you can verify it via Prometheus Alertmanager UI available from the ICP console Platform - Alerting ), you should see those incidents in the CEM UI: Now click on one of the incidents to see the Prometheus alert details: Click on the Generator URL link to open the Prometheus UI on the managed cluster and see the current result of the PromQL query that generated this alert:","title":"First login to the CEM console"},{"location":"mcm-monitoring-event-management/#conclusion","text":"IBM Cloud Event Management for IBM Multicloud Manager enables you to access Prometheus alert information for each of your managed clusters from a centralized location. To learn more about using and operating Cloud Event Management, check out the Cloud Event Management documentation .","title":"Conclusion"},{"location":"mcm-multicloud-scenarios/","text":"Scenarios for Multi-cluster Management Author: Gang Chen (gangchen@us.ibm.com) The Era of Multicloud We are in a Multicloud arena with 8 out of 10 enterprises committing to multicloud and 71% use three or more cloud providers. Embracing Multicloud will be the end game. IBM Multicloud Manager is the enterprise-grade multicloud management solution for Kubernetes. Many enterprises deploy multiple Kubernetes clusters on premises and in the public cloud, such as IBM Cloud\u2122, Amazon Web Services (AWS), and Azure. You might find yourself using different vendor-supported Kubernetes releases, including IBM Cloud Private, Red Hat\u00ae OpenShift\u00ae, IBM Cloud Kubernetes Service, Amazon EKS, and Azure AKS. As each cloud provider or Kubernetes solution comes with its own tools and operations, managing a Multicloud Kubernetes environment can be overwhelming. We summarized the top multicloud multi-cluster Kubernetes management challenges into the following 4 categories: Cross-cluster visibility Workload deployment and placement Compliance and Security Day 2 operation The Client Story Let's start by walking through a client story. The acmeAir company (a fictitious airline company combining multiple real client scenarios and requirements) is adopting cloud with following key principles: Multi-cloud (On-prem, IBM Cloud, AWS, Azure) Kubernetes based container runtime Automated DevOps Security is the top priority of the adoption acmeAir has many isolated Kubernetes clusters dedicated for different development teams and business unit (LoB). For example, they are running web and mobile consumer facing sites for the acmeair.com on IBM Cloud Private (ICP) on on-prem datacenter and ICP on AWS. While the airline booking API service is running on IBM Cloud Kubernetes Service (IKS) and ICP on AWS. Even within the Booking API project, there are multiple Kubernetes (IKS and ICP) clusters that separate the Dev/QA from Staging/Production environment. And for High availability and Disaster Recovery consideration, acmeAir plans to deploy the production clusters in multiple cloud providers (regions). In summary, they end up having more than 12 clusters to support the various project. By adding other LoBs, the number grows. What are Ops/SRE/Devops engineers doing with these clusters now (As-Is with or without MCM): Single consolidated view of all the clusters for corporate Ops team. Patch and upgrade many clusters and components. A single DevOps pipeline will build Microservices app and deployment them as builds and promote through the environment (Dev - QA - Staging - Prod). Token Rotation. Every 30 days, access tokens stored in secrets will need to be rotated in every environment and updated in CICD build tools. Ops/DevOps engineer has to enforce Secrets and ConfigMaps synchronization in every cluster. The acmeair.com site clusters contains customer PCI data that needs to ensure compliance in staging/prod clusters. The booking API engine needs to handle the workload bursting situation to scale up a new auto-provisioned ICP cluster on public Cloud IaaS. Ops team is looking for a tool to manage multiple clusters from a single location including dashboards, command control, deployments. Ops team needs to keep things in sync across clusters, so need to create/update objects only once - teams, helm catalogs used by teams etc. Ops/DevOps wants single action (anything from hamburger menu in the console) to execute for a set of clusters. This list gives you a big picture of the issues when dealing with multiple Kubernetes clusters. Cross-cluster Visibility User Story (Client needs): More and more clients adopt multiple-cloud strategy where they will host and operate multiple Kubernetes clusters across multiple cloud IaaS environment (including on-prem). Operators need a single management interface and utility to manage all these clusters with secure access. Operators can easily get health status of all the clusters managed including overall health condition, the key resource usage information. This function should be delivered through both a UI and CLI utility. Operators can easily add, remove and update the labels of a managed cluster. MCM Capability The current release of MCM provides the following features in cluster inventory: View all the managed clusters from a central location. Query/search clusters and Kubernetes resources based on cluster labels. View the health of all the clusters in a given region. Know how many nodes are down in all the clusters. View the pods in a given namespace in all development clusters. View (read only) the health status of pods, nodes, persistent volumes and applications running in those clusters. Workload Deployment and Placement User Story (Client needs): Developers and Operators may want to deploy workloads (Cloud-Native apps, App modernization packages) to multiple clusters. For example, Developers or DevOps engineer would like to deploy the application Helm Chart to Dev cluster environment, and later promote it to QA environment, both through a single Catalog UI or CLI utility. The Operators often deploy workload to multiple clusters as well. In our acmeAir example, production workload needs to be deployed to 2 cloud provider clusters and ensure the consistency of the application. Users would like to embed the multi-cluster support capability in their existing CI/CD DevOps pipeline. That way, they don't have to manage and update the different endpoint, access token and artifact repo (Helm repo) for each cluster. Sometimes, developers and operators may need to deploy and manage the workload in a higher abstract format where several modules/Helm charts are grouped as deployment unit with dependency and other relationship built-in. Particularly, when some of the dependent components may end up in different clusters, the capability to have an abstract Application component spanning cross multiple cluster becomes very helpful. MCM Capability Abstract Application component. This defines the representation of the common resource across clusters. This part is similar to Kubernetes Federation V2 feature. MCM has more exposure in dependency management capability. As well as reflect the relationship in a visual topology format. Component placement. More information will be addressed in below section. In general, MCM can place a workload on different clusters/namespaces based on a set of selectors and criteria. This can be powerful in a multiple-clusters scenario. For example, solving the request from Air Canada, Telus. MCM CLI integration with CI/CD pipeline. Managed cross-cluster Helm repository. Local or Remote Helm chart deployment through multi-cluster catalog or CLI. Features/Gaps Package and creating MCM Application is a manual process requiring knowledge of the MCM application schema. A scaffolding tool could be very helpful to package components into MCM application. Integrating with existing CI/CD toolchain seems rudimentary at CLI level. Perhaps a dedicated MCM plugin for Jenkins can prove useful. Compliance and Security This feature is meant to address the following requests: How do I set consistent security policies across environments? Which clusters are compliant? How can I place workloads based on capacity and/or policy? How can I ensure that all the clusters are configured properly based on their desired state. User Story Operators need to ensure that the clusters are operating properly by comparing the current configuration of various resources against a desired state. And operator would like to enforce role or pod object placement within the clusters through a set of policy templates. Let's use some examples to walk through the user story. The acmeair.com site will be deployed to both IBM Cloud and AWS. However, only IBM Cloud is certified by the company as PCI compliant given customer payment information will be stored in the system. Thus, the policy needs to enforce the payment services can only run on Kuberentes clusters hosted in IBM Cloud. Another example is associated with Workload deployment explained in above section. acmeair.com is planning to roll out a new feature, but they would like it to be first deployed in Dev and QA environments to run various tests. Instead of manually configuring the CI/CD pipelines with the Dev and QA environment credentials to do the deployment, DevOps engineers can simply update the MCM Application Placement Policy (explained in MCM Applications chapter ) with the cluster selector labels of the Dev and QA environments and MCM will take of finding the Dev and QA environments (based on the provided labels) and deploy the feature on those clusters. MCM Capability Set and enforce polices for Security, Applications and Infrastructure (Auto enforcement at cluster level). Check compliance against deployment parameters, configuration and policies. Conclusion Multicloud is the way the industry is going right now. However, the management of multiple Kubernetes clusters across multiple cloud providers is no easy feat, as shown in the user stories above. IBM Multicloud Manager takes care of the needs in these user stories and places itself as the central location to manage Kubernetes clusters across multiple cloud providers and their workloads. To get started learning and using MCM, checkout the Quick Start and MCM Applications chapters.","title":"Scenarios for Multi-cluster Management"},{"location":"mcm-multicloud-scenarios/#scenarios-for-multi-cluster-management","text":"Author: Gang Chen (gangchen@us.ibm.com)","title":"Scenarios for Multi-cluster Management"},{"location":"mcm-multicloud-scenarios/#the-era-of-multicloud","text":"We are in a Multicloud arena with 8 out of 10 enterprises committing to multicloud and 71% use three or more cloud providers. Embracing Multicloud will be the end game. IBM Multicloud Manager is the enterprise-grade multicloud management solution for Kubernetes. Many enterprises deploy multiple Kubernetes clusters on premises and in the public cloud, such as IBM Cloud\u2122, Amazon Web Services (AWS), and Azure. You might find yourself using different vendor-supported Kubernetes releases, including IBM Cloud Private, Red Hat\u00ae OpenShift\u00ae, IBM Cloud Kubernetes Service, Amazon EKS, and Azure AKS. As each cloud provider or Kubernetes solution comes with its own tools and operations, managing a Multicloud Kubernetes environment can be overwhelming. We summarized the top multicloud multi-cluster Kubernetes management challenges into the following 4 categories: Cross-cluster visibility Workload deployment and placement Compliance and Security Day 2 operation","title":"The Era of Multicloud"},{"location":"mcm-multicloud-scenarios/#the-client-story","text":"Let's start by walking through a client story. The acmeAir company (a fictitious airline company combining multiple real client scenarios and requirements) is adopting cloud with following key principles: Multi-cloud (On-prem, IBM Cloud, AWS, Azure) Kubernetes based container runtime Automated DevOps Security is the top priority of the adoption acmeAir has many isolated Kubernetes clusters dedicated for different development teams and business unit (LoB). For example, they are running web and mobile consumer facing sites for the acmeair.com on IBM Cloud Private (ICP) on on-prem datacenter and ICP on AWS. While the airline booking API service is running on IBM Cloud Kubernetes Service (IKS) and ICP on AWS. Even within the Booking API project, there are multiple Kubernetes (IKS and ICP) clusters that separate the Dev/QA from Staging/Production environment. And for High availability and Disaster Recovery consideration, acmeAir plans to deploy the production clusters in multiple cloud providers (regions). In summary, they end up having more than 12 clusters to support the various project. By adding other LoBs, the number grows. What are Ops/SRE/Devops engineers doing with these clusters now (As-Is with or without MCM): Single consolidated view of all the clusters for corporate Ops team. Patch and upgrade many clusters and components. A single DevOps pipeline will build Microservices app and deployment them as builds and promote through the environment (Dev - QA - Staging - Prod). Token Rotation. Every 30 days, access tokens stored in secrets will need to be rotated in every environment and updated in CICD build tools. Ops/DevOps engineer has to enforce Secrets and ConfigMaps synchronization in every cluster. The acmeair.com site clusters contains customer PCI data that needs to ensure compliance in staging/prod clusters. The booking API engine needs to handle the workload bursting situation to scale up a new auto-provisioned ICP cluster on public Cloud IaaS. Ops team is looking for a tool to manage multiple clusters from a single location including dashboards, command control, deployments. Ops team needs to keep things in sync across clusters, so need to create/update objects only once - teams, helm catalogs used by teams etc. Ops/DevOps wants single action (anything from hamburger menu in the console) to execute for a set of clusters. This list gives you a big picture of the issues when dealing with multiple Kubernetes clusters.","title":"The Client Story"},{"location":"mcm-multicloud-scenarios/#cross-cluster-visibility","text":"User Story (Client needs): More and more clients adopt multiple-cloud strategy where they will host and operate multiple Kubernetes clusters across multiple cloud IaaS environment (including on-prem). Operators need a single management interface and utility to manage all these clusters with secure access. Operators can easily get health status of all the clusters managed including overall health condition, the key resource usage information. This function should be delivered through both a UI and CLI utility. Operators can easily add, remove and update the labels of a managed cluster. MCM Capability The current release of MCM provides the following features in cluster inventory: View all the managed clusters from a central location. Query/search clusters and Kubernetes resources based on cluster labels. View the health of all the clusters in a given region. Know how many nodes are down in all the clusters. View the pods in a given namespace in all development clusters. View (read only) the health status of pods, nodes, persistent volumes and applications running in those clusters.","title":"Cross-cluster Visibility"},{"location":"mcm-multicloud-scenarios/#workload-deployment-and-placement","text":"User Story (Client needs): Developers and Operators may want to deploy workloads (Cloud-Native apps, App modernization packages) to multiple clusters. For example, Developers or DevOps engineer would like to deploy the application Helm Chart to Dev cluster environment, and later promote it to QA environment, both through a single Catalog UI or CLI utility. The Operators often deploy workload to multiple clusters as well. In our acmeAir example, production workload needs to be deployed to 2 cloud provider clusters and ensure the consistency of the application. Users would like to embed the multi-cluster support capability in their existing CI/CD DevOps pipeline. That way, they don't have to manage and update the different endpoint, access token and artifact repo (Helm repo) for each cluster. Sometimes, developers and operators may need to deploy and manage the workload in a higher abstract format where several modules/Helm charts are grouped as deployment unit with dependency and other relationship built-in. Particularly, when some of the dependent components may end up in different clusters, the capability to have an abstract Application component spanning cross multiple cluster becomes very helpful. MCM Capability Abstract Application component. This defines the representation of the common resource across clusters. This part is similar to Kubernetes Federation V2 feature. MCM has more exposure in dependency management capability. As well as reflect the relationship in a visual topology format. Component placement. More information will be addressed in below section. In general, MCM can place a workload on different clusters/namespaces based on a set of selectors and criteria. This can be powerful in a multiple-clusters scenario. For example, solving the request from Air Canada, Telus. MCM CLI integration with CI/CD pipeline. Managed cross-cluster Helm repository. Local or Remote Helm chart deployment through multi-cluster catalog or CLI. Features/Gaps Package and creating MCM Application is a manual process requiring knowledge of the MCM application schema. A scaffolding tool could be very helpful to package components into MCM application. Integrating with existing CI/CD toolchain seems rudimentary at CLI level. Perhaps a dedicated MCM plugin for Jenkins can prove useful.","title":"Workload Deployment and Placement"},{"location":"mcm-multicloud-scenarios/#compliance-and-security","text":"This feature is meant to address the following requests: How do I set consistent security policies across environments? Which clusters are compliant? How can I place workloads based on capacity and/or policy? How can I ensure that all the clusters are configured properly based on their desired state. User Story Operators need to ensure that the clusters are operating properly by comparing the current configuration of various resources against a desired state. And operator would like to enforce role or pod object placement within the clusters through a set of policy templates. Let's use some examples to walk through the user story. The acmeair.com site will be deployed to both IBM Cloud and AWS. However, only IBM Cloud is certified by the company as PCI compliant given customer payment information will be stored in the system. Thus, the policy needs to enforce the payment services can only run on Kuberentes clusters hosted in IBM Cloud. Another example is associated with Workload deployment explained in above section. acmeair.com is planning to roll out a new feature, but they would like it to be first deployed in Dev and QA environments to run various tests. Instead of manually configuring the CI/CD pipelines with the Dev and QA environment credentials to do the deployment, DevOps engineers can simply update the MCM Application Placement Policy (explained in MCM Applications chapter ) with the cluster selector labels of the Dev and QA environments and MCM will take of finding the Dev and QA environments (based on the provided labels) and deploy the feature on those clusters. MCM Capability Set and enforce polices for Security, Applications and Infrastructure (Auto enforcement at cluster level). Check compliance against deployment parameters, configuration and policies.","title":"Compliance and Security"},{"location":"mcm-multicloud-scenarios/#conclusion","text":"Multicloud is the way the industry is going right now. However, the management of multiple Kubernetes clusters across multiple cloud providers is no easy feat, as shown in the user stories above. IBM Multicloud Manager takes care of the needs in these user stories and places itself as the central location to manage Kubernetes clusters across multiple cloud providers and their workloads. To get started learning and using MCM, checkout the Quick Start and MCM Applications chapters.","title":"Conclusion"},{"location":"mcm-openshift/","text":"Manage Red Hat OpenShift Clusters Author: Fabio Gomez (fabiogomez@us.ibm.com) This section focuses on demonstrating how to manage an IBM Cloud Private on OpenShift cluster through MCM. Architecture Here is a breakdown of the architecture we will attempt to replicate and use in this document: 1 x IBM Cloud Private cluster, which will serve as the MCM Controller to manage itself and another cluster with the help of the MCM Klusterlet . 1 x IBM Cloud Private on OpenShift cluster, which will be managed by the cluster above via the MCM Klusterlet . 1 x Jenkins instance running from inside the first cluster, which will run a CI/CD pipeline defined in a Git repository. Pre-Requisites In order to go through this document, you are going to need the following: 1 x IBM Cloud Private cluster. 1 x IBM Cloud Private on OpenShift cluster. Kubectl (Kubernetes CLI) Follow the instructions here to install it on your platform. ICP Helm (Kubernetes package manager) Follow the instructions here to install it on your platform. Note : It is important to use ICP provided Helm for Chart Management on ICP 3.1. IBM Cloud Private CLI Follow the instructions here to install it on your platform. MCM CLI Follow the instructions here to install it on your platform. In the following sections, you will learn how to setup MCM to manage these 2 clusters. 1. MCM Controller and Klusterlet Cluster Preparation The first cluster will be the MCM Controller , which means that this cluster will be able to manage other clusters, itself included. In order for this cluster to become the MCM Controller and manage itself, we will need to install both the MCM Controller and the MCM Klusterlet Helm Charts. The MCM Controller is in charge of monitoring and send commands to all clusters. The MCM Klusterlet is responsible for reporting status back to the MCM Controller and implementing its instructions. Follow these instructions to install the MCM Controller and the MCM Klusterlet. In the Klusterlet section, for Cluster Name field enter se-dev-31 . Make sure to use these labels and values for this cluster: cloud : IBM datacenter : austin environment : Dev owner : case region : US vendor : ICP If the above was done correctly, you have successfully setup the first cluster and can now manage it through MCM Controller . 2. MCM Klusterlet Cluster Preparation The second cluster will only contain the MCM Klusterlet that reports information back to the MCM Controller cluster. Follow these instructions to install the MCM Klusterlet. For Cluster Name field, enter osedev-31 . Make sure to use these labels and values for this cluster: cloud : IBM datacenter : san-antonio environment : Staging owner : case region : US vendor : ICP If the above was done correctly, you have successfully setup the first cluster and can now manage it through the MCM Controller in the first cluster. 3. Verifying ICP on OpenShift Cluster on MCM Dashboard To verify that the ICP on OpenShift cluster shows up on the MCM Dashboard, open a new browser window and enter https://MCM_CONTROLLER_MASTER_IP:8443/multicloud/clusters . You can also open this view from ICP Web Console by clicking the Hamburger button - Multicloud Manager to go to MCM console, then follow that with hamburger button - Clusters . If you see the osedev-31 cluster above with a green checkmark icon under the Status column, then that means that the MCM Klusterlet was successfully installed on the ICP on OpenShift cluster. This means that the Klusterlet is reporting information back to the MCM Controller. Deploying an App through MCM Now that the clusters have been setup properly with MCM, let's deploy a sample application on the ICP on OpenShift cluster through the mcmctl command line tool. # Login against the MCM Controller Cluster cloudctl login -a https://ICP_MASTER_IP:8443 -n default --skip-ssl-validation; # Clone project repo git clone https://github.com/ibm-cloud-architecture/kubernetes-multicloud-management.git # Go to application directory cd kubernetes-multicloud-management/demos/guestbook # # Create the Image Policy in the ICP Cluster kubectl apply -f guestbook-cluster-image-policy.yaml # Install the Guestbook application on both clusters using the owner selector # with a value of case This value was used when installing the MCM Klusterlets. # Also notice that we set the replicaCount field to 2 , which means it will # install the application on up to 2 clusters that match the label selector. helm upgrade --install guestbook --set replicaCount=2 --set targetCluster.labelSelector.matchLabels.owner=case gbapp --tls If the above was done correctly, then the application was deployed to both clusters usin only one helm upgrade command using the owner cluster selector that was passed above. This is possible because both clusters share a cluster selector label value of case for the owner field. The MCM Controller looks up clusters that match those labels and then deploys the application to those clusters. For more details on the guestbook application, check out the helm chart here . To verify that the application was created in MCM and deployed to both clusters, let's run the following command: # Get the application mcmctl get applications NAME LABELSELECTOR KINDS AGE DASHBAORD guestbook-gbapp Expression:app In [gbapp,gbf,gbrm,gbrs] Pods.core 1d https://172.16.50.227:8443/grafana/dashboard/db/guestbook-gbapp-dashboard-via-federated-prometheus If you see the guestbook-gbapp above that means that the application was successfully registered in MCM Controller. To verify that the application deployments were created in the ICP on OpenShift cluster, run the following command: mcmctl --cluster-selector environment=Staging get deployments CLUSTER NAME AGE osedev-31 md-guestbook-gbapp-osedev-31-gbf 11h osedev-31 md-guestbook-gbapp-redismaster-osedev-31-gbrm 11h osedev-31 md-guestbook-gbapp-redisslave-osedev-31-gbrs 11h If you see the md-guestbook-gbapp , md-guestbook-gbapp-redismaster , and md-guestbook-gbapp-redisslave deployments above, this means that the MCM Controller successfully deployed the application on the ICP on OpenShift cluster! Verifying the Application on MCM Console Now that we verified that the application was successfully deployed on the ICP on OpenShift cluster through MCM, let's see what that the appplication looks like from the MCM Controller Console. To see the applications view, open a browser and go to https://MCM_CONTROLLER_MASTER_IP:8443/multicloud/applications You should be able to see the guestbook-gbapp application above, which means that the application was successfully registered in the MCM Controller. Now click on the application name to go to a more in-details view of the application. Feel free to explore all of the menus in this view, but if you scroll all the way down you will see the most important part of this view below: The above shows the Placement Policies , which show the criteria used to deploy the application on multiple clusters. Pay attention to the fields and values for matchLabels under Cluster Selector . Do you recognize the owner field with a value of case ? This is the same field that we passed along in the helm upgrade command. Also notice under the Deployable Objects the guestbook-gbapp , guestbook-gbapp-redismaster , and the guestbook-gbapp-redisslave objects. Those are the guestbook helm charts that were deployed with the gbapp application chart. That's all you need to verify through MCM Console that the application was deployed to both clusters. Verifying the Application on ICP on OpenShift Console Now that have deployed the application on the ICP on OpenShift cluster, let's verify the installation from the ICP on OpenShift console and test its functinality to make sure that everything works as expected. To access the ICP on OpenShift dashboard, open a new browser and go to https://ICP_OPENSHIFT_MASTER_IP:5443/console/workloads/deployments to see the existing deployments. If you are able to see the md-guestbook-gbapp-frontend , md-guestbook-gbapp-redismaster , and md-guestbook-gbapp-redisslave deployments and all have an Available value of 1 , then this means that all of the deployments have succesfully been deployed and started. You might need to search for guestbook and make sure the view is displaying deployments for all the namespaces to be able to see them. To test the application itself, go ahead and click the Launch button at the end of the md-guestbook-gbapp row, which will open a new browser window to display the guestbook web application, as shown below. The guestbook application itself is very simple. It consists of a web application that saves guest names to a Redis deployment and persists them there even if the web application dies or restarts for some reason. To test its functionality, enter any text in the textbox shown above and click the Submit button. If everything worked successfully, you will see that the text you entered has now moved below the Submit button, which indicates that the text has been saved to and successfully read from the Redis deployment. To make sure that the text persists in the Redis deployment, feel free to refresh the page and make sure that the text you entered is bein shown again below the Submit button. If all the above was done successfully, that means that you have successfully verified the guestbook deployment and tested its functionality! Verifying the Application on OpenShift Console The last step that remains is to verify that the guestbook deployments are shown on the OpenShift console itself. To do that, open a new browser and go to https://ICP_OPENSHIFT_MASTER_IP:8443/console/project/default/overview , where default is the name of the OpenShift project (known as a namespace in ICP/Kubernetes terms). If you are able to see the md-guestbook-gbapp , md-guestbook-gbapp-redismaster , and md-guestbook-gbapp-redisslave deployments and all have at least 1 pod available, then this means that all of the deployments have succesfully been deployed and started. You might need to search for guestbook and make sure the view is displaying deployments for all the namespaces to be able to see them. Conclusion Now that you know how to use MCM to manage, monitor, and deploy applications on IBM Cloud Private and IBM Cloud Private on OpenShift clusters, you should checkout the Manage AWS EKS clusters with MCM chapter to learn how to use MCM to manage, monitor, and deploy applications on AWS EKS clusters.","title":"Manage Red Hat OpenShift Clusters"},{"location":"mcm-openshift/#manage-red-hat-openshift-clusters","text":"Author: Fabio Gomez (fabiogomez@us.ibm.com) This section focuses on demonstrating how to manage an IBM Cloud Private on OpenShift cluster through MCM.","title":"Manage Red Hat OpenShift Clusters"},{"location":"mcm-openshift/#architecture","text":"Here is a breakdown of the architecture we will attempt to replicate and use in this document: 1 x IBM Cloud Private cluster, which will serve as the MCM Controller to manage itself and another cluster with the help of the MCM Klusterlet . 1 x IBM Cloud Private on OpenShift cluster, which will be managed by the cluster above via the MCM Klusterlet . 1 x Jenkins instance running from inside the first cluster, which will run a CI/CD pipeline defined in a Git repository.","title":"Architecture"},{"location":"mcm-openshift/#pre-requisites","text":"In order to go through this document, you are going to need the following: 1 x IBM Cloud Private cluster. 1 x IBM Cloud Private on OpenShift cluster. Kubectl (Kubernetes CLI) Follow the instructions here to install it on your platform. ICP Helm (Kubernetes package manager) Follow the instructions here to install it on your platform. Note : It is important to use ICP provided Helm for Chart Management on ICP 3.1. IBM Cloud Private CLI Follow the instructions here to install it on your platform. MCM CLI Follow the instructions here to install it on your platform. In the following sections, you will learn how to setup MCM to manage these 2 clusters.","title":"Pre-Requisites"},{"location":"mcm-openshift/#1-mcm-controller-and-klusterlet-cluster-preparation","text":"The first cluster will be the MCM Controller , which means that this cluster will be able to manage other clusters, itself included. In order for this cluster to become the MCM Controller and manage itself, we will need to install both the MCM Controller and the MCM Klusterlet Helm Charts. The MCM Controller is in charge of monitoring and send commands to all clusters. The MCM Klusterlet is responsible for reporting status back to the MCM Controller and implementing its instructions. Follow these instructions to install the MCM Controller and the MCM Klusterlet. In the Klusterlet section, for Cluster Name field enter se-dev-31 . Make sure to use these labels and values for this cluster: cloud : IBM datacenter : austin environment : Dev owner : case region : US vendor : ICP If the above was done correctly, you have successfully setup the first cluster and can now manage it through MCM Controller .","title":"1. MCM Controller and Klusterlet Cluster Preparation"},{"location":"mcm-openshift/#2-mcm-klusterlet-cluster-preparation","text":"The second cluster will only contain the MCM Klusterlet that reports information back to the MCM Controller cluster. Follow these instructions to install the MCM Klusterlet. For Cluster Name field, enter osedev-31 . Make sure to use these labels and values for this cluster: cloud : IBM datacenter : san-antonio environment : Staging owner : case region : US vendor : ICP If the above was done correctly, you have successfully setup the first cluster and can now manage it through the MCM Controller in the first cluster.","title":"2. MCM Klusterlet Cluster Preparation"},{"location":"mcm-openshift/#3-verifying-icp-on-openshift-cluster-on-mcm-dashboard","text":"To verify that the ICP on OpenShift cluster shows up on the MCM Dashboard, open a new browser window and enter https://MCM_CONTROLLER_MASTER_IP:8443/multicloud/clusters . You can also open this view from ICP Web Console by clicking the Hamburger button - Multicloud Manager to go to MCM console, then follow that with hamburger button - Clusters . If you see the osedev-31 cluster above with a green checkmark icon under the Status column, then that means that the MCM Klusterlet was successfully installed on the ICP on OpenShift cluster. This means that the Klusterlet is reporting information back to the MCM Controller.","title":"3. Verifying ICP on OpenShift Cluster on MCM Dashboard"},{"location":"mcm-openshift/#deploying-an-app-through-mcm","text":"Now that the clusters have been setup properly with MCM, let's deploy a sample application on the ICP on OpenShift cluster through the mcmctl command line tool. # Login against the MCM Controller Cluster cloudctl login -a https://ICP_MASTER_IP:8443 -n default --skip-ssl-validation; # Clone project repo git clone https://github.com/ibm-cloud-architecture/kubernetes-multicloud-management.git # Go to application directory cd kubernetes-multicloud-management/demos/guestbook # # Create the Image Policy in the ICP Cluster kubectl apply -f guestbook-cluster-image-policy.yaml # Install the Guestbook application on both clusters using the owner selector # with a value of case This value was used when installing the MCM Klusterlets. # Also notice that we set the replicaCount field to 2 , which means it will # install the application on up to 2 clusters that match the label selector. helm upgrade --install guestbook --set replicaCount=2 --set targetCluster.labelSelector.matchLabels.owner=case gbapp --tls If the above was done correctly, then the application was deployed to both clusters usin only one helm upgrade command using the owner cluster selector that was passed above. This is possible because both clusters share a cluster selector label value of case for the owner field. The MCM Controller looks up clusters that match those labels and then deploys the application to those clusters. For more details on the guestbook application, check out the helm chart here . To verify that the application was created in MCM and deployed to both clusters, let's run the following command: # Get the application mcmctl get applications NAME LABELSELECTOR KINDS AGE DASHBAORD guestbook-gbapp Expression:app In [gbapp,gbf,gbrm,gbrs] Pods.core 1d https://172.16.50.227:8443/grafana/dashboard/db/guestbook-gbapp-dashboard-via-federated-prometheus If you see the guestbook-gbapp above that means that the application was successfully registered in MCM Controller. To verify that the application deployments were created in the ICP on OpenShift cluster, run the following command: mcmctl --cluster-selector environment=Staging get deployments CLUSTER NAME AGE osedev-31 md-guestbook-gbapp-osedev-31-gbf 11h osedev-31 md-guestbook-gbapp-redismaster-osedev-31-gbrm 11h osedev-31 md-guestbook-gbapp-redisslave-osedev-31-gbrs 11h If you see the md-guestbook-gbapp , md-guestbook-gbapp-redismaster , and md-guestbook-gbapp-redisslave deployments above, this means that the MCM Controller successfully deployed the application on the ICP on OpenShift cluster!","title":"Deploying an App through MCM"},{"location":"mcm-openshift/#verifying-the-application-on-mcm-console","text":"Now that we verified that the application was successfully deployed on the ICP on OpenShift cluster through MCM, let's see what that the appplication looks like from the MCM Controller Console. To see the applications view, open a browser and go to https://MCM_CONTROLLER_MASTER_IP:8443/multicloud/applications You should be able to see the guestbook-gbapp application above, which means that the application was successfully registered in the MCM Controller. Now click on the application name to go to a more in-details view of the application. Feel free to explore all of the menus in this view, but if you scroll all the way down you will see the most important part of this view below: The above shows the Placement Policies , which show the criteria used to deploy the application on multiple clusters. Pay attention to the fields and values for matchLabels under Cluster Selector . Do you recognize the owner field with a value of case ? This is the same field that we passed along in the helm upgrade command. Also notice under the Deployable Objects the guestbook-gbapp , guestbook-gbapp-redismaster , and the guestbook-gbapp-redisslave objects. Those are the guestbook helm charts that were deployed with the gbapp application chart. That's all you need to verify through MCM Console that the application was deployed to both clusters.","title":"Verifying the Application on MCM Console"},{"location":"mcm-openshift/#verifying-the-application-on-icp-on-openshift-console","text":"Now that have deployed the application on the ICP on OpenShift cluster, let's verify the installation from the ICP on OpenShift console and test its functinality to make sure that everything works as expected. To access the ICP on OpenShift dashboard, open a new browser and go to https://ICP_OPENSHIFT_MASTER_IP:5443/console/workloads/deployments to see the existing deployments. If you are able to see the md-guestbook-gbapp-frontend , md-guestbook-gbapp-redismaster , and md-guestbook-gbapp-redisslave deployments and all have an Available value of 1 , then this means that all of the deployments have succesfully been deployed and started. You might need to search for guestbook and make sure the view is displaying deployments for all the namespaces to be able to see them. To test the application itself, go ahead and click the Launch button at the end of the md-guestbook-gbapp row, which will open a new browser window to display the guestbook web application, as shown below. The guestbook application itself is very simple. It consists of a web application that saves guest names to a Redis deployment and persists them there even if the web application dies or restarts for some reason. To test its functionality, enter any text in the textbox shown above and click the Submit button. If everything worked successfully, you will see that the text you entered has now moved below the Submit button, which indicates that the text has been saved to and successfully read from the Redis deployment. To make sure that the text persists in the Redis deployment, feel free to refresh the page and make sure that the text you entered is bein shown again below the Submit button. If all the above was done successfully, that means that you have successfully verified the guestbook deployment and tested its functionality!","title":"Verifying the Application on ICP on OpenShift Console"},{"location":"mcm-openshift/#verifying-the-application-on-openshift-console","text":"The last step that remains is to verify that the guestbook deployments are shown on the OpenShift console itself. To do that, open a new browser and go to https://ICP_OPENSHIFT_MASTER_IP:8443/console/project/default/overview , where default is the name of the OpenShift project (known as a namespace in ICP/Kubernetes terms). If you are able to see the md-guestbook-gbapp , md-guestbook-gbapp-redismaster , and md-guestbook-gbapp-redisslave deployments and all have at least 1 pod available, then this means that all of the deployments have succesfully been deployed and started. You might need to search for guestbook and make sure the view is displaying deployments for all the namespaces to be able to see them.","title":"Verifying the Application on OpenShift Console"},{"location":"mcm-openshift/#conclusion","text":"Now that you know how to use MCM to manage, monitor, and deploy applications on IBM Cloud Private and IBM Cloud Private on OpenShift clusters, you should checkout the Manage AWS EKS clusters with MCM chapter to learn how to use MCM to manage, monitor, and deploy applications on AWS EKS clusters.","title":"Conclusion"},{"location":"mcm-quickstart/","text":"Multicloud Manager Quick Start In this document we will point you to the right resources to install IBM Multicloud Manager (MCM). Also, we will mention the core concepts you will need to learn to start using MCM and point you to the right resources. Installing MCM The official MCM documentation has great instructions on how to install MCM Controller and MCM Klusterlet on an IBM Cloud Private cluster. You will follow those instructions with a few modifications to install MCM on two existing IBM Cloud Private clusters. Using two clusters instead of just one is better to show the benefits of MCM. Cluster 1: MCM Controller and Klusterlet Cluster Preparation The first cluster will be the MCM Controller , which means that this cluster will be able to manage other clusters, itself included. In order for this cluster to become the MCM Controller and manage itself, we will need to install both the MCM Controller and the MCM Klusterlet Helm Charts. The MCM Controller is in charge of monitoring and send commands to all clusters. The MCM Klusterlet is responsible for reporting status back to the MCM Controller and implementing its instructions. Follow these instructions to install the MCM Controller and the MCM Klusterlet. Follow these instructions to install the MCM Klusterlet. In the Klusterlet section, for Cluster Name field enter se-dev-31 . Make sure to use these labels and values for this cluster: cloud : IBM datacenter : austin environment : Dev owner : case region : US vendor : ICP If the above was done correctly, you have successfully setup the first cluster and can now manage it through MCM Controller . Cluster 2: MCM Klusterlet Cluster Preparation The second cluster will only contain the MCM Klusterlet that reports information back to the MCM Controller cluster. Follow these instructions to install the MCM Klusterlet. For Cluster Name field, enter se-stg-31 . Make sure to use these labels and values for this cluster: cloud : IBM datacenter : dallas environment : Staging owner : case region : US vendor : ICP If the above was done correctly, you have successfully setup the first cluster and can now manage it through the MCM Controller in the first cluster. Verify Clusters in the MCM Dashboard To open the MCM Dashboard, on your MCM HUB Cluster and click the top left Hamburger Icon - Multicloud Manager as shown below: You should then be greeted by the MCM Welcome page. To open the Clusters page, click the Hamburger Icon - Clusters as shown below: If MCM Klusterlets were installed successfully, you should be able to see your clusters listed as shown below: NOTE : The above is just for illustration purposes, but in your Cluster page, you should be able to see the se-dev-31 and se-stg-31 clusters. Under the Status column, you should see a green checkmark (\u2705), which indicates that the cluster is healthy and actively sending status from the Klusterlet to the MCM Controller in the HUB Cluster. Core Multicloud Manager Resources to Learn Now that you have installed MCM on your IBM Cloud Private (ICP) clusters, it's time to learn how to use it! With anything Kubernetes, you will be interacting with MCM via YAML files (though MCM comes with a nice web portal that we will examine in later chapters). As mentioned before, the official MCM documentation is a great resource to start learning about MCM in detail. However, here is a quick list with a high level summary of the concepts you will be learning throughout this entire document: Application MCM has the the concept of an Application , which is a way to relate multiple workloads (Helm Charts) as a single unit. A cluster application is defined with the Kubernetes SIG Application CRD community specification. An Application , and its workloads, can be deployed into multiple ICP clusters through MCM with the help of other YAML resources that make up the application. Here are some of the Application YAML resources you need to use in order to view and manage applications through MCM. Application resource is used to only view your resource on the Command Line or on the MCM Dashboard. Deployable resource deploys your Helm chart. If you have multiple Helm Charts, then you will have a Deployable resource for each. PlacementPolicy resource defines the criteria to find cluster(s) to place or deploy workloads. DeployableOverride is used to define different values from the original Deployable and override a deployment. This is useful if you would like to pass different configuration values for specific clusters, i.e. a different database URL. ApplicationRelationship deploys an application on another application based on the source and destination values. It is also a way to visually connect your application components and see the topology on the MCM dashboard. To read more about the above resources, check out the official documentation . Compliance and Policy Multicloud Manager has the ability to validate and/or enforce the existence and non-existence of Kubernetes resources with specific configurations in your clusters through Compliance YAML files. For example, MCM can prevent or inform you of the existence of any god-level or too powerful roles bound to a specific account in a specific namespace across multiple clusters. As of version 3.1.2, MCM can also enforce and/or inform you of the existence and non-existence of any Kubernetes resource. This level of compliance checks makes it super simple to enforce configuration parity across multiple clusters. Also, since it's a YAML file, the compliance checks are self-documenting and can be checked into source code! To learn more about Compliance resource and its features, checkout official documentation .","title":"Quick Start"},{"location":"mcm-quickstart/#multicloud-manager-quick-start","text":"In this document we will point you to the right resources to install IBM Multicloud Manager (MCM). Also, we will mention the core concepts you will need to learn to start using MCM and point you to the right resources.","title":"Multicloud Manager Quick Start"},{"location":"mcm-quickstart/#installing-mcm","text":"The official MCM documentation has great instructions on how to install MCM Controller and MCM Klusterlet on an IBM Cloud Private cluster. You will follow those instructions with a few modifications to install MCM on two existing IBM Cloud Private clusters. Using two clusters instead of just one is better to show the benefits of MCM.","title":"Installing MCM"},{"location":"mcm-quickstart/#cluster-1-mcm-controller-and-klusterlet-cluster-preparation","text":"The first cluster will be the MCM Controller , which means that this cluster will be able to manage other clusters, itself included. In order for this cluster to become the MCM Controller and manage itself, we will need to install both the MCM Controller and the MCM Klusterlet Helm Charts. The MCM Controller is in charge of monitoring and send commands to all clusters. The MCM Klusterlet is responsible for reporting status back to the MCM Controller and implementing its instructions. Follow these instructions to install the MCM Controller and the MCM Klusterlet. Follow these instructions to install the MCM Klusterlet. In the Klusterlet section, for Cluster Name field enter se-dev-31 . Make sure to use these labels and values for this cluster: cloud : IBM datacenter : austin environment : Dev owner : case region : US vendor : ICP If the above was done correctly, you have successfully setup the first cluster and can now manage it through MCM Controller .","title":"Cluster 1: MCM Controller and Klusterlet Cluster Preparation"},{"location":"mcm-quickstart/#cluster-2-mcm-klusterlet-cluster-preparation","text":"The second cluster will only contain the MCM Klusterlet that reports information back to the MCM Controller cluster. Follow these instructions to install the MCM Klusterlet. For Cluster Name field, enter se-stg-31 . Make sure to use these labels and values for this cluster: cloud : IBM datacenter : dallas environment : Staging owner : case region : US vendor : ICP If the above was done correctly, you have successfully setup the first cluster and can now manage it through the MCM Controller in the first cluster.","title":"Cluster 2: MCM Klusterlet Cluster Preparation"},{"location":"mcm-quickstart/#verify-clusters-in-the-mcm-dashboard","text":"To open the MCM Dashboard, on your MCM HUB Cluster and click the top left Hamburger Icon - Multicloud Manager as shown below: You should then be greeted by the MCM Welcome page. To open the Clusters page, click the Hamburger Icon - Clusters as shown below: If MCM Klusterlets were installed successfully, you should be able to see your clusters listed as shown below: NOTE : The above is just for illustration purposes, but in your Cluster page, you should be able to see the se-dev-31 and se-stg-31 clusters. Under the Status column, you should see a green checkmark (\u2705), which indicates that the cluster is healthy and actively sending status from the Klusterlet to the MCM Controller in the HUB Cluster.","title":"Verify Clusters in the MCM Dashboard"},{"location":"mcm-quickstart/#core-multicloud-manager-resources-to-learn","text":"Now that you have installed MCM on your IBM Cloud Private (ICP) clusters, it's time to learn how to use it! With anything Kubernetes, you will be interacting with MCM via YAML files (though MCM comes with a nice web portal that we will examine in later chapters). As mentioned before, the official MCM documentation is a great resource to start learning about MCM in detail. However, here is a quick list with a high level summary of the concepts you will be learning throughout this entire document:","title":"Core Multicloud Manager Resources to Learn"},{"location":"mcm-quickstart/#application","text":"MCM has the the concept of an Application , which is a way to relate multiple workloads (Helm Charts) as a single unit. A cluster application is defined with the Kubernetes SIG Application CRD community specification. An Application , and its workloads, can be deployed into multiple ICP clusters through MCM with the help of other YAML resources that make up the application. Here are some of the Application YAML resources you need to use in order to view and manage applications through MCM. Application resource is used to only view your resource on the Command Line or on the MCM Dashboard. Deployable resource deploys your Helm chart. If you have multiple Helm Charts, then you will have a Deployable resource for each. PlacementPolicy resource defines the criteria to find cluster(s) to place or deploy workloads. DeployableOverride is used to define different values from the original Deployable and override a deployment. This is useful if you would like to pass different configuration values for specific clusters, i.e. a different database URL. ApplicationRelationship deploys an application on another application based on the source and destination values. It is also a way to visually connect your application components and see the topology on the MCM dashboard. To read more about the above resources, check out the official documentation .","title":"Application"},{"location":"mcm-quickstart/#compliance-and-policy","text":"Multicloud Manager has the ability to validate and/or enforce the existence and non-existence of Kubernetes resources with specific configurations in your clusters through Compliance YAML files. For example, MCM can prevent or inform you of the existence of any god-level or too powerful roles bound to a specific account in a specific namespace across multiple clusters. As of version 3.1.2, MCM can also enforce and/or inform you of the existence and non-existence of any Kubernetes resource. This level of compliance checks makes it super simple to enforce configuration parity across multiple clusters. Also, since it's a YAML file, the compliance checks are self-documenting and can be checked into source code! To learn more about Compliance resource and its features, checkout official documentation .","title":"Compliance and Policy"}]}